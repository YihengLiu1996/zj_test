import os
import multiprocessing as mp
from pathlib import Path
import pyarrow.parquet as pq
import orjson
import gc
from tqdm import tqdm  # â† æ–°å¢ï¼

# ========== å¯è°ƒå‚æ•° ==========
BATCH_SIZE = 100_000  # æ¯æ‰¹å¤„ç†è¡Œæ•°
# =============================

def parquet_to_jsonl_task(args):
    """
    å•æ–‡ä»¶è½¬æ¢ä»»åŠ¡ï¼ˆåˆ†æ‰¹è¯»å– + åˆ†æ‰¹å†™å…¥ï¼Œå†…å­˜å®‰å…¨ï¼‰
    """
    parquet_path, output_root, input_root = args
    try:
        gc.disable()  # å…³é—­ GC æé€Ÿ

        # è®¡ç®—è¾“å‡ºè·¯å¾„
        rel_path = os.path.relpath(parquet_path, input_root)
        jsonl_rel_path = Path(rel_path).with_suffix('.jsonl')
        output_path = Path(output_root) / jsonl_rel_path
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ‰“å¼€ Parquet æ–‡ä»¶ï¼ˆä¸å…¨åŠ è½½ï¼‰
        parquet_file = pq.ParquetFile(parquet_path)

        # ä½¿ç”¨ iter_batches åˆ†æ‰¹è¯»å–
        with open(output_path, 'wb') as f:
            for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):
                lines = []
                for i in range(batch.num_rows):
                    row_dict = {}
                    for col in batch.schema.names:
                        val = batch[col][i].as_py()
                        row_dict[col] = val
                    line = orjson.dumps(row_dict, option=orjson.OPT_APPEND_NEWLINE)
                    lines.append(line)
                f.writelines(lines)
                del lines, batch  # æ˜¾å¼é‡Šæ”¾

        gc.enable()
        return f"âœ… {parquet_path}"

    except Exception as e:
        return f"âŒ {parquet_path} | {e}"


def find_parquet_files_fast(root_dir):
    """
    ä½¿ç”¨ os.scandir é€’å½’æŸ¥æ‰¾ .parquet æ–‡ä»¶ï¼ˆæ›´å¿«ï¼‰
    """
    parquet_files = []
    stack = [root_dir]
    while stack:
        current_dir = stack.pop()
        try:
            with os.scandir(current_dir) as it:
                for entry in it:
                    if entry.is_file() and entry.name.lower().endswith('.parquet'):
                        parquet_files.append(entry.path)
                    elif entry.is_dir():
                        stack.append(entry.path)
        except PermissionError:
            continue
    return parquet_files


def main():
    input_folder = input("ğŸ“¥ è¯·è¾“å…¥åŒ…å« parquet æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
    output_folder = input("ğŸ“¤ è¯·è¾“å…¥è¾“å‡º jsonl æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip()

    if not os.path.exists(input_folder):
        print("âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼")
        return

    Path(output_folder).mkdir(parents=True, exist_ok=True)

    print("ğŸ” æ­£åœ¨æ‰«æ parquet æ–‡ä»¶...")
    parquet_files = find_parquet_files_fast(input_folder)

    if not parquet_files:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½• .parquet æ–‡ä»¶ï¼")
        return

    total_files = len(parquet_files)
    print(f"ğŸš€ æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¯åŠ¨ {mp.cpu_count()} è¿›ç¨‹å¹¶è¡Œè½¬æ¢...")

    tasks = [(f, output_folder, input_folder) for f in parquet_files]

    # ğŸ‘‡ ä½¿ç”¨ imap_unordered + tqdm å®ç°è¿›åº¦æ¡
    with mp.Pool(processes=mp.cpu_count()) as pool:
        # tqdm åŒ…è£¹è¿­ä»£å™¨ï¼Œæ˜¾ç¤ºè¿›åº¦
        results = list(tqdm(
            pool.imap_unordered(parquet_to_jsonl_task, tasks),
            total=total_files,
            desc="ğŸ”„ è½¬æ¢è¿›åº¦",
            unit="æ–‡ä»¶",
            ncols=80,
            colour='green'
        ))

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r.startswith('âœ…'))
    print(f"\nğŸ‰ è½¬æ¢å®Œæˆï¼æˆåŠŸ: {success_count} / {total_files}")
    if success_count < total_files:
        print("â— ä»¥ä¸‹æ–‡ä»¶è½¬æ¢å¤±è´¥:")
        for r in results:
            if r.startswith('âŒ'):
                print(r)


if __name__ == "__main__":
    mp.freeze_support()
    main()

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4000, "0-4k"),
    (4000, 8000, "4k-8k"),
    (8000, 16000, "8k-16k"),
    (16000, 32000, "16k-32k"),
    (32000, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low < token_count <= high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

def ipf_solver(df, target_ratios, target_total, max_iter=50, tol=0.01):
    """
    IPFè¿­ä»£æ¯”ä¾‹æ‹Ÿåˆæ±‚è§£å™¨
    :param df: æ•°æ®DataFrame
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(1%)
    :return: é‡‡æ ·æƒé‡æ•°ç»„, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    # åˆå§‹åŒ–æƒé‡
    weights = np.ones(len(df))
    total_tokens = df['token_count'].sum()
    
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        dim_total = 0
        for cat, ratio in targets.items():
            # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
            if cat not in df[dim].values:
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            
            # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
            orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²ï¼ˆIPFå¯å¾®è°ƒï¼‰
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®æ»¡è¶³")
        
        # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False
    
    # å¼€å§‹IPFè¿­ä»£
    for iter in range(max_iter):
        prev_weights = weights.copy()
        max_error = 0
        
        # æŒ‰ç»´åº¦è¿­ä»£è°ƒæ•´
        for dim, targets in target_ratios.items():
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / np.sum(weights * df['token_count'])
                
                # è®¡ç®—è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5:
                    factor = target_ratio / current_ratio
                    weights[mask] *= factor
                
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                max_error = max(max_error, error)
        
        # æ£€æŸ¥æ”¶æ•›
        if max_error < tol:
            break
            
        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-4:
            break
    
    # ç¼©æ”¾è‡³ç›®æ ‡æ€»é‡
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        weights *= (target_total / current_total)
    
    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            actual_dist[dim][cat] = np.sum(weights[mask] * df.loc[mask, 'token_count']) / target_total
    
    return weights, actual_dist, (max_error < tol)

def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])
    
    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        remaining['prob'] = (additional * remaining['token_count'] / 
                            remaining['token_count'].sum() / 
                            remaining['token_count'])
        retained[~retained] = np.random.random(len(remaining)) < np.minimum(remaining['prob'], 1.0)
    
    return df[retained].copy()

def export_shards(df, output_path, shard_size_gb=1):
    """åˆ†ç‰‡å¯¼å‡ºJSONLæ–‡ä»¶"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    current_size = 0
    shard_idx = 1
    buffer = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, row in df.iterrows():
        # è®¡ç®—å½“å‰æ ·æœ¬å­—èŠ‚æ•°
        sample_bytes = len(row['text'].encode('utf-8')) + 1  # +1 for newline
        
        # å¦‚æœå½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œå†™å…¥æ–‡ä»¶
        if current_size + sample_bytes > shard_size_bytes and buffer:
            shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
            with open(shard_path, 'w', encoding='utf-8') as f:
                f.write("".join(buffer))
            buffer = []
            current_size = 0
            shard_idx += 1
        
        # æ·»åŠ æ ·æœ¬åˆ°ç¼“å†²åŒº
        buffer.append(json.dumps({
            'source': row['source'],
            'category': row['category'],
            'domain': row['domain'],
            'language': row['language'],
            'token_count': row['token_count'],
            'text': row['text']
        }, ensure_ascii=False) + '\n')
        current_size += sample_bytes
        
        # æ›´æ–°è¿›åº¦
        if idx % 1000 == 0:
            progress = (idx + 1) / len(df)
            progress_bar.progress(min(progress, 1.0))
            status_text.text(f"å¤„ç†æ ·æœ¬ {idx+1}/{len(df)} | å½“å‰åˆ†ç‰‡: {shard_idx}")
    
    # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
    if buffer:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        with open(shard_path, 'w', encoding='utf-8') as f:
            f.write("".join(buffer))
    
    progress_bar.empty()
    status_text.empty()
    st.success(f"å¯¼å‡ºå®Œæˆï¼å…± {shard_idx} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        st.sidebar.info(f"åŒ…å« {len(os.listdir(data_path))} ä¸ªé¡¹ç›®")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        data_path = os.path.normpath(data_path)
        
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                jsonl_files = []
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            jsonl_files.append(os.path.join(root, file))
                
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶æ ¼å¼")
                    st.stop()

                # å¹¶è¡Œè¯»å–å‡½æ•°
                def read_jsonl(file_path):
                    try:
                        df = pd.read_json(file_path, lines=True)
                        # ä¿ç•™å¿…è¦å­—æ®µå¹¶ç¡®ä¿ç±»å‹
                        required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                        if not all(f in df.columns for f in required_fields):
                            return pd.DataFrame()  # è¿”å›ç©ºè¡¨è¡¨ç¤ºæ— æ•ˆ
                        df = df[required_fields]
                        df['token_count'] = pd.to_numeric(df['token_count'], errors='coerce')
                        df.dropna(subset=['token_count'], inplace=True)
                        df['token_count'] = df['token_count'].astype(int)
                        return df
                    except Exception as e:
                        st.sidebar.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
                        return pd.DataFrame()

                # å¹¶è¡ŒåŠ è½½
                from concurrent.futures import ThreadPoolExecutor, as_completed
                progress_bar = st.sidebar.progress(0)
                status_text = st.sidebar.empty()
                all_chunks = []
                total_files = len(jsonl_files)

                with ThreadPoolExecutor(max_workers=8) as executor:
                    future_to_file = {executor.submit(read_jsonl, f): f for f in jsonl_files}
                    for i, future in enumerate(as_completed(future_to_file)):
                        result = future.result()
                        if not result.empty:
                            all_chunks.append(result)
                        progress = (i + 1) / total_files
                        progress_bar.progress(progress)
                        status_text.text(f"å·²å¤„ç† {i+1}/{total_files} ä¸ªæ–‡ä»¶")

                progress_bar.empty()
                status_text.empty()

                if not all_chunks:
                    st.sidebar.error("âŒ æœªè¯»å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                    st.stop()

                df = pd.concat(all_chunks, ignore_index=True)
                total_tokens = df['token_count'].sum()

                st.session_state.df = df
                st.session_state.total_tokens = total_tokens
                st.session_state.token_bins = [get_token_bin(tc) for tc in df['token_count']]

                st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(df):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{total_tokens/1e9:.2f}B tokens")

            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df' in st.session_state:
    df = st.session_state.df
    total_tokens = st.session_state.total_tokens
    
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    
    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    target_ratios = {}
    
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        
        # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼
        if dim == 'token_bin':
            values = pd.Series(st.session_state.token_bins).unique()
        else:
            values = df[dim].unique()
        
        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        if dim == 'token_bin':
            current_dist = df.groupby(pd.Series(st.session_state.token_bins))['token_count'].sum() / total_tokens
        else:
            current_dist = df.groupby(dim)['token_count'].sum() / total_tokens
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
        target_ratios[dim] = {}
        total_ratio = 0.0
        cols = st.sidebar.columns(len(values))
        
        for i, val in enumerate(values):
            current_ratio = current_dist.get(val, 0.0)
            with cols[i % len(cols)]:
                ratio = st.number_input(
                    f"{val}", 
                    min_value=0.0, 
                    max_value=1.0, 
                    value=float(current_ratio),
                    step=0.01,
                    key=f"{dim}_{val}"
                )
                target_ratios[dim][val] = ratio
                total_ratio += ratio
        
        # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            # è¿è¡ŒIPFæ±‚è§£å™¨
            weights, actual_dist, converged = ipf_solver(
                df, 
                target_ratios, 
                target_total,
                tol=0.01  # 1%è¯¯å·®
            )
            
            if weights is not None:
                # å­˜å‚¨é‡‡æ ·ç»“æœ
                sampled_df = sample_dataset(df, weights, target_total)
                st.session_state.sampled_df = sampled_df
                
                # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                st.sidebar.info(f"å®é™…æ€»é‡: {sampled_df['token_count'].sum()/1e9:.2f}B tokens")
                
                # æ˜¾ç¤ºå…³é”®ç»´åº¦è¯¯å·®
                for dim in ['language', 'domain']:
                    if dim in actual_dist:
                        max_error = 0
                        for cat in actual_dist[dim]:
                            target = target_ratios[dim].get(cat, 0)
                            actual = actual_dist[dim].get(cat, 0)
                            error = abs(target - actual)
                            max_error = max(max_error, error)
                        st.sidebar.caption(f"{dim}: æœ€å¤§è¯¯å·® {max_error:.1%}")
    
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                export_shards(st.session_state.sampled_df, output_path, shard_size)
    
    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    
    # åˆ›å»ºå›¾è¡¨å¸ƒå±€
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        source_dist = calculate_distribution(df, 'source')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        category_dist = calculate_distribution(df, 'category')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        domain_dist = calculate_distribution(df, 'domain')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        lang_dist = calculate_distribution(df, 'language')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Token length distribution")
        df['token_bin'] = st.session_state.token_bins
        token_dist = calculate_distribution(df, 'token_bin')
        
        # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨
        for _, _, label in TOKEN_BINS:
            if label not in token_dist:
                token_dist[label] = 0.0
        
        token_dist = token_dist.reindex([label for _, _, label in TOKEN_BINS])
        
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.bar(token_dist.index, token_dist.values)
        ax.set_ylabel('æ¯”ä¾‹')
        ax.set_title('Tokené•¿åº¦åˆ†å¸ƒ')
        for i, v in enumerate(token_dist.values):
            ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
        st.pyplot(fig)
    
    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 50)")
        # åˆ›å»ºå­ç±»ç»„åˆ
        df['subclass'] = df['source'] + "+" + df['category'] + "+" + df['domain'] + "+" + df['language']
        subclass_dist = calculate_distribution(df, 'subclass')
        
        # å–Top 50
        top50 = subclass_dist.head(50)
        
        fig, ax = plt.subplots(figsize=(50, 5))
        ax.barh(top50.index, top50.values)
        ax.set_xlabel('æ¯”ä¾‹')
        ax.set_title('Top 50 distribution of subclass combinations')
        
        # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
        for i, v in enumerate(top50.values):
            ax.text(v + 0.005, i, f'{v:.1%}', va='center')
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
    st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")
    
    # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
    if 'sampled_df' in st.session_state:
        st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
        sampled_df = st.session_state.sampled_df
        sampled_tokens = sampled_df['token_count'].sum()
        
        st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
        st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")
        
        # æ¯”è¾ƒå…³é”®ç»´åº¦
        col1, col2, col3 = st.columns(3)
        for i, dim in enumerate(['language', 'domain', 'source']):
            orig_dist = calculate_distribution(df, dim)
            sampled_dist = calculate_distribution(sampled_df, dim)
            
            # è®¡ç®—æœ€å¤§è¯¯å·®
            max_error = 0
            for cat in orig_dist.index:
                orig = orig_dist.get(cat, 0)
                sampled = sampled_dist.get(cat, 0)
                error = abs(orig - sampled)
                max_error = max(max_error, error)
            
            if i == 0:
                col1.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            elif i == 1:
                col2.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            else:
                col3.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png  ", width=300)

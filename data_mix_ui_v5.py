import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4096, "0-4k"),
    (4096, 8192, "4k-8k"),
    (8192, 16384, "8k-16k"),
    (16384, 32768, "16k-32k"),
    (32768, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low <= token_count < high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

@st.cache_data
def calculate_distribution_cached(df, column, weights=None):
    """ç¼“å­˜ç‰ˆæœ¬çš„åˆ†å¸ƒè®¡ç®—"""
    return calculate_distribution(df, column, weights)

# def advanced_ipf_solver(df, target_ratios, target_total, max_iter=100, tol=0.005):
#     """
#     æ”¹è¿›çš„IPFæ±‚è§£å™¨ - æ”¯æŒå¤šç»´åº¦åŒæ—¶ä¼˜åŒ–
#     :param df: æ•°æ®DataFrame
#     :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
#     :param target_total: ç›®æ ‡æ€»tokenæ•°
#     :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
#     :param tol: è¯¯å·®å®¹å¿åº¦(0.5%)
#     :return: é‡‡æ ·æƒé‡æ•°ç»„, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
#     """
#     # åˆå§‹åŒ–æƒé‡
#     weights = np.ones(len(df))
#     total_tokens = df['token_count'].sum()
    
#     # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
#     for dim, targets in target_ratios.items():
#         for cat, ratio in targets.items():
#             # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
#             if cat not in df[dim].values:
#                 st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
#                 return None, None, False
#             # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
#             orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
#             if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²
#                 st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®æ»¡è¶³")
#         # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
#         dim_sum = sum(targets.values())
#         if not (0.99 <= dim_sum <= 1.01):
#             st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
#             return None, None, False

#     # å¼€å§‹IPFè¿­ä»£
#     # converged_dims = set()  # ä¸å†å†»ç»“ç»´åº¦ï¼Œæ¯æ¬¡éƒ½æ£€æŸ¥æ‰€æœ‰ç»´åº¦
#     all_dims = set(target_ratios.keys())
    
#     for iter in range(max_iter):
#         prev_weights = weights.copy()
#         max_errors = {}
        
#         # æŒ‰ç»´åº¦è¿­ä»£è°ƒæ•´
#         for dim, targets in target_ratios.items():
#             # if dim in converged_dims: # ç§»é™¤ç»´åº¦å†»ç»“é€»è¾‘
#             #     continue
#             dim_max_error = 0
#             for cat, target_ratio in targets.items():
#                 # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
#                 mask = (df[dim] == cat)
#                 current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / np.sum(weights * df['token_count'])
#                 # è®¡ç®—è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
#                 if current_ratio > 1e-5 and target_ratio > 0:
#                     factor = target_ratio / current_ratio
#                     # é™åˆ¶è°ƒæ•´å¹…åº¦ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
#                     factor = max(0.5, min(2.0, factor))
#                     weights[mask] *= factor
#                 # è®°å½•æœ€å¤§è¯¯å·®
#                 error = abs(current_ratio - target_ratio)
#                 dim_max_error = max(dim_max_error, error)
#             max_errors[dim] = dim_max_error
#             # æ£€æŸ¥è¯¥ç»´åº¦æ˜¯å¦æ”¶æ•› (ä½†ä¸å†»ç»“)
#             # if dim_max_error < tol:
#             #     converged_dims.add(dim)
        
#         # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•›
#         # if len(converged_dims) == len(all_dims): # æ”¹ä¸ºæ£€æŸ¥å½“å‰è¯¯å·®
#         if all(error < tol for error in max_errors.values()):
#             st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
#             break
            
#         # æ£€æŸ¥æƒé‡å˜åŒ–
#         weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
#         if weight_change < 1e-5:
#             st.info(f"âš ï¸ æƒé‡å˜åŒ–è¿‡å°ï¼Œåœ¨ç¬¬ {iter+1} è½®è¿­ä»£ååœæ­¢")
#             break

#     # ç¼©æ”¾è‡³ç›®æ ‡æ€»é‡ (åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å°±è€ƒè™‘ç›®æ ‡æ€»é‡ï¼Œæé«˜åˆ©ç”¨å†—ä½™çš„æ•ˆç‡)
#     # å…ˆè®¡ç®—å½“å‰åŠ æƒæ€»å’Œ
#     current_total = np.sum(weights * df['token_count'])
#     if current_total > 0:
#         # è®¡ç®—ç¼©æ”¾å› å­
#         scale_factor = target_total / current_total
#         # åº”ç”¨ç¼©æ”¾å› å­
#         weights *= scale_factor
#         # æ›´æ–° current_total
#         current_total = target_total # np.sum(weights * df['token_count'])

#     # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
#     actual_dist = {}
#     final_errors = {}
#     for dim in target_ratios.keys():
#         actual_dist[dim] = {}
#         dim_max_error = 0
#         for cat in target_ratios[dim].keys():
#             mask = (df[dim] == cat)
#             # ä½¿ç”¨ç¼©æ”¾åçš„æƒé‡è®¡ç®—å®é™…æ¯”ä¾‹
#             actual_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / current_total
#             actual_dist[dim][cat] = actual_ratio
#             target_ratio = target_ratios[dim][cat]
#             error = abs(actual_ratio - target_ratio)
#             dim_max_error = max(dim_max_error, error)
#         final_errors[dim] = dim_max_error

#     # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·®
#     st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
#     for dim, error in final_errors.items():
#         if error <= tol:
#             st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
#         else:
#             st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
#     is_converged = all(error <= tol for error in final_errors.values())
#     return weights, actual_dist, is_converged

def advanced_ipf_solver(df, target_ratios, target_total, max_iter=100, tol=0.005):
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                # ä½¿ç”¨å½“å‰çš„ weights è®¡ç®— current_ratio
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / current_total if current_total > 1e-5 else 0.0
                
                # è®¡ç®—æ¯”ä¾‹è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5 and target_ratio > 0:
                    ratio_factor = target_ratio / current_ratio
                    # é™åˆ¶æ¯”ä¾‹è°ƒæ•´å¹…åº¦
                    ratio_factor = max(0.7, min(1.4, ratio_factor))
                    
                    # æ›´æ–°æƒé‡ï¼šç»“åˆæ¯”ä¾‹å› å­å’Œæ€»é‡å› å­
                    # è¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ï¼šæƒé‡æ›´æ–°åŒæ—¶è€ƒè™‘äº†æ¯”ä¾‹å’Œæ€»é‡
                    # ä¾‹å¦‚ï¼Œå¦‚æœå½“å‰æ€»é‡æ˜¯ç›®æ ‡çš„80%ï¼Œåˆ™ total_factor æ˜¯ 1.25
                    # å¦‚æœå½“å‰æ¯”ä¾‹æ˜¯ç›®æ ‡çš„90%ï¼Œåˆ™ ratio_factor æ˜¯ ~1.11
                    # ç»¼åˆå› å­çº¦ä¸º 1.25 * 1.11 ~= 1.39ï¼Œæƒé‡ä¼šå¢åŠ 
                    combined_factor = ratio_factor * total_factor
                    
                    weights[mask] *= combined_factor
                    
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                dim_max_error = max(dim_max_error, error)
            max_errors[dim] = dim_max_error
            # æ³¨æ„ï¼šä¸å†å°†ç»´åº¦åŠ å…¥ converged_dims é›†åˆ

        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•› (åœ¨æ¯æ¬¡è¿­ä»£åéƒ½æ£€æŸ¥)
        if all(error < tol for error in max_errors.values()):
            st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
            break
            
        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-5:
            st.info(f"âš ï¸ æƒé‡å˜åŒ–è¿‡å°ï¼Œåœ¨ç¬¬ {iter+1} è½®è¿­ä»£ååœæ­¢")
            break

    # è¿­ä»£ç»“æŸåï¼Œè¿›è¡Œä¸€æ¬¡æœ€ç»ˆçš„æ€»é‡æ ¡å‡† (å¯é€‰ï¼Œä½†é€šå¸¸æ˜¯ä¸ªå¥½ä¸»æ„)
    # å› ä¸ºè¿­ä»£ä¸­çš„ total_factor æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        final_scale_factor = target_total / current_total
        weights *= final_scale_factor
        # æ›´æ–° current_total ä»¥ç”¨äºåç»­è®¡ç®—
        current_total = target_total

    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    final_errors = {}
    # ä½¿ç”¨æœ€ç»ˆæ ¡å‡†åçš„ current_total (å³ target_total) æ¥è®¡ç®—å®é™…æ¯”ä¾‹
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        dim_max_error = 0
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            # ä½¿ç”¨æœ€ç»ˆçš„ weights å’Œ target_total è®¡ç®—å®é™…æ¯”ä¾‹
            actual_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / current_total
            actual_dist[dim][cat] = actual_ratio
            target_ratio = target_ratios[dim][cat]
            error = abs(actual_ratio - target_ratio)
            dim_max_error = max(dim_max_error, error)
        final_errors[dim] = dim_max_error

    # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·®
    st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
    for dim, error in final_errors.items():
        if error <= tol:
            st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
            
    is_converged = all(error <= tol for error in final_errors.values())
    return weights, actual_dist, is_converged


def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])
    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        if len(remaining) > 0:
            remaining_prob = (additional * remaining['token_count'] / 
                             remaining['token_count'].sum() if remaining['token_count'].sum() > 0 else 0)
            remaining['prob'] = remaining_prob
            retained[~retained] = np.random.random(len(remaining)) < np.minimum(remaining['prob'], 1.0)
    return df[retained].copy()

def write_shard_batch(rows, shard_path):
    """æ‰¹é‡å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
    try:
        with open(shard_path, 'w', encoding='utf-8') as f:
            for row in rows:
                f.write(json.dumps({
                    'source': row['source'],
                    'category': row['category'],
                    'domain': row['domain'],
                    'language': row['language'],
                    'token_count': row['token_count'],
                    'text': row['text']
                }, ensure_ascii=False) + '\n')
        return True, shard_path
    except Exception as e:
        return False, f"Error writing {shard_path}: {str(e)}"

def export_shards_parallel(df, output_path, shard_size_gb=1, max_workers=4):
    """å¹¶è¡Œåˆ†ç‰‡å¯¼å‡ºJSONLæ–‡ä»¶"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªåˆ†ç‰‡
    total_bytes = df['text'].apply(lambda x: len(x.encode('utf-8')) + 1).sum()
    num_shards = math.ceil(total_bytes / shard_size_bytes)
    st.info(f"éœ€è¦åˆ›å»º {num_shards} ä¸ªåˆ†ç‰‡æ–‡ä»¶")
    # å°†æ•°æ®åˆ†ç»„åˆ°åˆ†ç‰‡ä¸­
    shards_data = []
    current_shard = []
    current_size = 0
    shard_idx = 1
    progress_bar = st.progress(0)
    status_text = st.empty()
    for idx, row in df.iterrows():
        # è®¡ç®—å½“å‰æ ·æœ¬å­—èŠ‚æ•°
        sample_bytes = len(row['text'].encode('utf-8')) + 1  # +1 for newline
        # å¦‚æœå½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œä¿å­˜å½“å‰åˆ†ç‰‡
        if current_size + sample_bytes > shard_size_bytes and current_shard:
            shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
            shards_data.append((current_shard.copy(), shard_path))
            current_shard = []
            current_size = 0
            shard_idx += 1
        # æ·»åŠ æ ·æœ¬åˆ°å½“å‰åˆ†ç‰‡
        current_shard.append(row.to_dict())
        current_size += sample_bytes
        # æ›´æ–°è¿›åº¦
        if idx % 1000 == 0:
            progress = min(idx / len(df), 1.0)
            progress_bar.progress(progress)
            status_text.text(f"åˆ†ç‰‡å‡†å¤‡ä¸­: {idx+1}/{len(df)} | å½“å‰åˆ†ç‰‡: {shard_idx}")
    # ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç‰‡
    if current_shard:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        shards_data.append((current_shard, shard_path))
    progress_bar.empty()
    status_text.empty()
    # å¹¶è¡Œå†™å…¥åˆ†ç‰‡æ–‡ä»¶
    st.info(f"å¼€å§‹å¹¶è¡Œå†™å…¥ {len(shards_data)} ä¸ªåˆ†ç‰‡æ–‡ä»¶...")
    success_count = 0
    failed_files = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰å†™å…¥ä»»åŠ¡
        future_to_shard = {executor.submit(write_shard_batch, rows, path): (i, path) 
                          for i, (rows, path) in enumerate(shards_data)}
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(as_completed(future_to_shard)):
            success, result = future.result()
            if success:
                success_count += 1
            else:
                failed_files.append(result)
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(shards_data)
            progress_bar.progress(progress)
            status_text.text(f"å†™å…¥è¿›åº¦: {i+1}/{len(shards_data)} | æˆåŠŸ: {success_count}")
    progress_bar.empty()
    status_text.empty()
    # æŠ¥å‘Šç»“æœ
    if failed_files:
        st.warning(f"å¯¼å‡ºå®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_files)}")
        for error in failed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            st.error(error)
        if len(failed_files) > 5:
            st.error(f"... è¿˜æœ‰ {len(failed_files) - 5} ä¸ªé”™è¯¯")
    else:
        st.success(f"å¯¼å‡ºå®Œæˆï¼å…± {success_count} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

def parse_jsonl_file_pandas(file_path, chunksize=50000):
    """ä½¿ç”¨pandasé«˜æ•ˆè§£æJSONLæ–‡ä»¶ï¼ˆæ”¯æŒåˆ†å—è¯»å–ï¼‰"""
    records = []
    try:
        # åˆ†å—è¯»å–ä»¥å¤„ç†å¤§æ–‡ä»¶
        chunk_iter = pd.read_json(file_path, lines=True, chunksize=chunksize)
        # å¦‚æœä¸æ˜¯è¿­ä»£å™¨ï¼Œè¯´æ˜æ–‡ä»¶è¾ƒå°ï¼Œç›´æ¥è¯»å–
        if not hasattr(chunk_iter, '__iter__'):
            chunk_iter = [chunk_iter]
        for chunk in chunk_iter:
            # å¿…éœ€å­—æ®µ
            required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
            # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
            if all(col in chunk.columns for col in required_fields):
                # åªä¿ç•™å¿…éœ€å­—æ®µ
                chunk = chunk[required_fields]
                # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
                chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                chunk.dropna(subset=['token_count'], inplace=True)
                chunk['token_count'] = chunk['token_count'].astype(int)
                # ç¡®ä¿å…¶ä»–å­—æ®µä¸ºå­—ç¬¦ä¸²ç±»å‹
                string_fields = ['source', 'category', 'domain', 'language', 'text']
                for field in string_fields:
                    chunk[field] = chunk[field].astype(str)
                # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨
                records.extend(chunk.to_dict(orient='records'))
            else:
                print(f"Missing required fields in {file_path}")
                continue
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
    return records

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        st.sidebar.info(f"åŒ…å« {len(os.listdir(data_path))} ä¸ªé¡¹ç›®")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        # å…³é”®ä¿®å¤ï¼šè§„èŒƒåŒ–è·¯å¾„ï¼ˆè§£å†³Windowså¤§å°å†™é—®é¢˜ï¼‰
        data_path = os.path.normpath(data_path)
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                # ä¿®å¤1ï¼šå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼ˆè§£å†³.JSONLé—®é¢˜ï¼‰
                jsonl_files = []
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            jsonl_files.append(os.path.join(root, file))
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
                    st.sidebar.caption("- è·¯å¾„æ˜¯å¦æ­£ç¡®")
                    st.sidebar.caption("- æ–‡ä»¶åç¼€æ˜¯å¦ä¸º.jsonlï¼ˆé.JSONLï¼‰")
                    st.sidebar.caption("- æ˜¯å¦æœ‰æ–‡ä»¶è®¿é—®æƒé™")
                    st.stop()
                # ä¿®å¤2ï¼šæ·»åŠ æ–‡ä»¶å†…å®¹é¢„è§ˆï¼ˆè¯Šæ–­æ ¼å¼é—®é¢˜ï¼‰
                sample_file = jsonl_files[0]
                try:
                    with open(sample_file, 'r', encoding='utf-8') as f:
                        sample_lines = [next(f).strip() for _ in range(3)]
                    st.sidebar.caption(f"ğŸ“„ é¢„è§ˆ {os.path.basename(sample_file)}:")
                    for line in sample_lines:
                        st.sidebar.caption(f"`{line[:100]}...`")
                except Exception as e:
                    st.sidebar.warning(f"âš ï¸ æ— æ³•è¯»å–ç¤ºä¾‹æ–‡ä»¶: {str(e)}")
                # å¹¶è¡Œè¯»å–æ‰€æœ‰JSONLæ–‡ä»¶ï¼ˆä½¿ç”¨pandasä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                all_data = []
                progress_bar = st.sidebar.progress(0)
                status_text = st.sidebar.empty()
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶
                with ThreadPoolExecutor(max_workers=8) as executor:
                    future_to_file = {executor.submit(parse_jsonl_file_pandas, file): file for file in jsonl_files}
                    for i, future in enumerate(as_completed(future_to_file)):
                        result = future.result()
                        all_data.extend(result)
                        status_text.text(f"âœ… å·²å¤„ç† {i+1}/{len(jsonl_files)} ä¸ªæ–‡ä»¶")
                        progress_bar.progress((i + 1) / len(jsonl_files))
                progress_bar.empty()
                status_text.empty()
                if all_data:
                    # è½¬ä¸ºDataFrame
                    df = pd.DataFrame(all_data)
                    total_tokens = df['token_count'].sum()
                    # å­˜å‚¨åˆ°session state
                    st.session_state.df = df
                    st.session_state.total_tokens = total_tokens
                    # ä¸ºåŸå§‹æ•°æ®æ·»åŠ token_binåˆ—
                    st.session_state.token_bins = [get_token_bin(tc) for tc in df['token_count']]
                    df['token_bin'] = st.session_state.token_bins
                    st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(df):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{total_tokens/1e9:.2f}B tokens")
                else:
                    st.sidebar.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                    st.sidebar.info("æœ‰æ•ˆJSONLæ ·æœ¬ç¤ºä¾‹:")
                    st.sidebar.code('''{"source": "CCI4", "category": "book", "domain": "science", "language": "CN", "token_count": 1234, "text": "ç¤ºä¾‹æ–‡æœ¬..."}''')
                    st.stop()
            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df' in st.session_state:
    df = st.session_state.df
    total_tokens = st.session_state.total_tokens
    # ç¡®ä¿token_binåˆ—å­˜åœ¨
    if 'token_bin' not in df.columns:
        df['token_bin'] = [get_token_bin(tc) for tc in df['token_count']]
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    target_ratios = {}
    # åˆå§‹åŒ– session_state å­˜å‚¨ç›®æ ‡æ¯”ä¾‹
    if 'target_ratios' not in st.session_state:
        st.session_state.target_ratios = {}
    # è·å– token_bin é¡ºåº
    token_bin_order = [label for _, _, label in TOKEN_BINS]
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼ï¼ˆæŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼‰
        if dim == 'token_bin':
            values = sorted(df['token_bin'].unique(), key=lambda x: token_bin_order.index(x) if x in token_bin_order else len(token_bin_order))
        else:
            values = sorted(df[dim].unique())
        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        if dim == 'token_bin':
            current_dist = df.groupby('token_bin')['token_count'].sum() / total_tokens
        else:
            current_dist = df.groupby(dim)['token_count'].sum() / total_tokens
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
        if dim not in st.session_state.target_ratios:
            st.session_state.target_ratios[dim] = {}
        target_ratios[dim] = {}
        total_ratio = 0.0
        # æ¯è¡Œæœ€å¤šæ”¾ 3 ä¸ªè¾“å…¥æ¡†
        items_per_row = 3
        for i_start in range(0, len(values), items_per_row):
            cols = st.sidebar.columns(items_per_row)
            for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                current_ratio = current_dist.get(val, 0.0)
                with cols[i_offset]:
                    ratio = st.number_input(
                        label=f"{val}",
                        min_value=0.0,
                        max_value=1.0,
                        value=float(current_ratio),
                        step=0.01,
                        format="%.3f",  # æ˜¾ç¤º3ä½å°æ•°
                        key=f"{dim}_{val}"
                    )
                    st.session_state.target_ratios[dim][val] = ratio
                    target_ratios[dim][val] = ratio
                    total_ratio += ratio
        # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            # ä» session_state è¯»å–æœ€æ–°çš„ç›®æ ‡æ¯”ä¾‹
            target_ratios = st.session_state.target_ratios
            # è¿è¡Œæ”¹è¿›çš„IPFæ±‚è§£å™¨
            weights, actual_dist, converged = advanced_ipf_solver(
                df, 
                target_ratios, 
                target_total,
                max_iter=100,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                tol=0.005      # é™ä½è¯¯å·®å®¹å¿åº¦åˆ°0.5%
            )
            if weights is not None:
                # å­˜å‚¨é‡‡æ ·ç»“æœ
                sampled_df = sample_dataset(df, weights, target_total)
                st.session_state.sampled_df = sampled_df
                # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                st.sidebar.info(f"å®é™…æ€»é‡: {sampled_df['token_count'].sum()/1e9:.2f}B tokens")
                # æ˜¾ç¤ºæ”¶æ•›çŠ¶æ€
                if converged:
                    st.sidebar.success("âœ… æ‰€æœ‰ç»´åº¦é…æ¯”å‡å·²æ»¡è¶³ï¼")
                else:
                    st.sidebar.warning("âš ï¸ éƒ¨åˆ†ç»´åº¦é…æ¯”æœªå®Œå…¨æ»¡è¶³ï¼Œè¯·æ£€æŸ¥è¯¯å·®æŠ¥å‘Š")
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    max_export_workers = st.sidebar.slider("å¯¼å‡ºå¹¶è¡Œçº¿ç¨‹æ•°", min_value=1, max_value=16, value=4)
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                export_shards_parallel(st.session_state.sampled_df, output_path, shard_size, max_export_workers)
    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    # åˆ›å»ºå›¾è¡¨å¸ƒå±€
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        source_dist = calculate_distribution_cached(df, 'source')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        category_dist = calculate_distribution_cached(df, 'category')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        domain_dist = calculate_distribution_cached(df, 'domain')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        lang_dist = calculate_distribution_cached(df, 'language')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        # ç¡®ä¿token_binåˆ—å­˜åœ¨
        if 'token_bin' not in df.columns:
            df['token_bin'] = [get_token_bin(tc) for tc in df['token_count']]
        token_dist = calculate_distribution_cached(df, 'token_bin')
        # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨å¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—
        ordered_labels = [label for _, _, label in TOKEN_BINS]
        for label in ordered_labels:
            if label not in token_dist:
                token_dist[label] = 0.0
        token_dist = token_dist.reindex(ordered_labels)
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.bar(token_dist.index, token_dist.values)
        ax.set_ylabel('Ratio')
        ax.set_title('Token length distribution')
        for i, v in enumerate(token_dist.values):
            ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
        st.pyplot(fig)
    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
        # åˆ›å»ºå­ç±»ç»„åˆ
        df['subclass'] = df['source'] + "+" + df['category'] + "+" + df['domain'] + "+" + df['language']
        subclass_dist = calculate_distribution_cached(df, 'subclass')
        # å–Top 10
        top10 = subclass_dist.head(10)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.barh(top10.index, top10.values)
        ax.set_xlabel('æ¯”ä¾‹')
        ax.set_title('Top 10 distribution of subclass combinations')
        # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
        for i, v in enumerate(top10.values):
            ax.text(v + 0.005, i, f'{v:.1%}', va='center')
        plt.tight_layout()
        st.pyplot(fig)
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
    st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")
    # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
    if 'sampled_df' in st.session_state:
        st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
        sampled_df = st.session_state.sampled_df
        sampled_tokens = sampled_df['token_count'].sum()
        # ç¡®ä¿é‡‡æ ·æ•°æ®ä¹Ÿæœ‰token_binåˆ—
        if 'token_bin' not in sampled_df.columns:
            sampled_df['token_bin'] = [get_token_bin(tc) for tc in sampled_df['token_count']]
        st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
        st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")
        # æ¯”è¾ƒå…³é”®ç»´åº¦
        st.subheader("ğŸ“ˆ é…æ¯”å¯¹æ¯”åˆ†æ")
        comparison_cols = st.columns(len(['language', 'domain', 'category', 'token_bin']))
        for i, dim in enumerate(['language', 'domain', 'category', 'token_bin']):
            with comparison_cols[i]:
                orig_dist = calculate_distribution_cached(df, dim)
                sampled_dist = calculate_distribution_cached(sampled_df, dim)
                # è®¡ç®—æœ€å¤§è¯¯å·®
                max_error = 0
                for cat in orig_dist.index:
                    orig = orig_dist.get(cat, 0)
                    sampled = sampled_dist.get(cat, 0)
                    error = abs(orig - sampled)
                    max_error = max(max_error, error)
                st.metric(f"{dim.capitalize()}", f"{max_error:.1%}", "æœ€å¤§è¯¯å·®")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png", width=300)

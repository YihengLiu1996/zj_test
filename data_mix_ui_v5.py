import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import tempfile
import shutil

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4096, "0-4k"),
    (4096, 8192, "4k-8k"),
    (8192, 16384, "8k-16k"),
    (16384, 32768, "16k-32k"),
    (32768, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

class LargeDataSampler:
    """å¤„ç†å¤§å®¹é‡æ•°æ®çš„é‡‡æ ·å™¨"""
    def __init__(self, data_path, chunk_size=50000):
        self.data_path = data_path
        self.chunk_size = chunk_size
        self.jsonl_files = []
        self.stats = {}

    def scan_files(self):
        """æ‰«ææ‰€æœ‰JSONLæ–‡ä»¶"""
        self.jsonl_files = []
        for root, _, files in os.walk(self.data_path):
            for file in files:
                if file.lower().endswith('.jsonl'):
                    self.jsonl_files.append(os.path.join(root, file))
        return len(self.jsonl_files)

    def calculate_statistics(self):
        """è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†å—å¤„ç†ï¼‰"""
        st.info("æ­£åœ¨è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯...")
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': 0,
            'total_tokens': 0,
            'dimensions': defaultdict(lambda: defaultdict(int))  # {dim: {value: count}}
        }
        progress_bar = st.progress(0)
        status_text = st.empty()
        # åˆ†å—è¯»å–å¹¶ç»Ÿè®¡
        for i, file_path in enumerate(self.jsonl_files):
            try:
                # åˆ†å—è¯»å–æ–‡ä»¶
                chunk_iter = pd.read_json(file_path, lines=True, chunksize=self.chunk_size)
                if not hasattr(chunk_iter, '__iter__'):
                    chunk_iter = [chunk_iter]
                for chunk in chunk_iter:
                    required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                    if all(col in chunk.columns for col in required_fields):
                        # æ•°æ®æ¸…æ´—
                        chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                        chunk.dropna(subset=['token_count'], inplace=True)
                        chunk['token_count'] = chunk['token_count'].astype(int)
                        # ç»Ÿè®¡ä¿¡æ¯
                        stats['total_samples'] += len(chunk)
                        stats['total_tokens'] += chunk['token_count'].sum()
                        # ç»´åº¦ç»Ÿè®¡
                        for dim in ['source', 'category', 'domain', 'language']:
                            dim_counts = chunk[dim].value_counts()
                            for val, count in dim_counts.items():
                                stats['dimensions'][dim][str(val)] += count
                        # token_binç»Ÿè®¡
                        chunk['token_bin'] = chunk['token_count'].apply(self.get_token_bin)
                        bin_counts = chunk['token_bin'].value_counts()
                        for val, count in bin_counts.items():
                            stats['dimensions']['token_bin'][val] += count
                status_text.text(f"å·²å¤„ç† {i+1}/{len(self.jsonl_files)} ä¸ªæ–‡ä»¶")
                progress_bar.progress((i + 1) / len(self.jsonl_files))
            except Exception as e:
                st.warning(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                continue
        progress_bar.empty()
        status_text.empty()
        # è½¬æ¢ä¸ºæ¯”ä¾‹
        for dim in stats['dimensions']:
            total = sum(stats['dimensions'][dim].values())
            for val in stats['dimensions'][dim]:
                stats['dimensions'][dim][val] = stats['dimensions'][dim][val] / total
        self.stats = stats
        return stats

    def get_token_bin(self, token_count):
        """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
        for low, high, label in TOKEN_BINS:
            if low <= token_count < high:
                return label
        return ">32k"

    def sample_data_streaming(self, target_ratios, target_total_tokens, output_path, shard_size_gb=1):
        """æµå¼é‡‡æ ·å¤§æ•°æ®é›†"""
        st.info("å¼€å§‹æµå¼é‡‡æ ·...")
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„é‡‡æ ·æƒé‡
        weights = self.calculate_sampling_weights(target_ratios)
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        # æµå¼å¤„ç†å’Œé‡‡æ ·
        total_written_tokens = 0
        shard_idx = 1
        current_shard_data = []
        current_shard_size = 0
        shard_size_bytes = shard_size_gb * GB
        progress_bar = st.progress(0)
        status_text = st.empty()
        total_files = len(self.jsonl_files)
        for file_idx, file_path in enumerate(self.jsonl_files):
            try:
                # åˆ†å—è¯»å–æ–‡ä»¶
                chunk_iter = pd.read_json(file_path, lines=True, chunksize=self.chunk_size)
                if not hasattr(chunk_iter, '__iter__'):
                    chunk_iter = [chunk_iter]
                for chunk in chunk_iter:
                    required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                    if all(col in chunk.columns for col in required_fields):
                        # æ•°æ®æ¸…æ´—
                        chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                        chunk.dropna(subset=['token_count'], inplace=True)
                        chunk['token_count'] = chunk['token_count'].astype(int)
                        chunk['token_bin'] = chunk['token_count'].apply(self.get_token_bin)
                        # ä¸ºæ¯è¡Œè®¡ç®—é‡‡æ ·æ¦‚ç‡
                        chunk['sample_prob'] = chunk.apply(
                            lambda row: self.calculate_row_probability(row, weights), axis=1
                        )
                        # é‡‡æ ·
                        sampled_chunk = chunk[np.random.random(len(chunk)) < chunk['sample_prob']]
                        # å†™å…¥åˆ†ç‰‡
                        for _, row in sampled_chunk.iterrows():
                            sample_bytes = len(row['text'].encode('utf-8')) + 1
                            # å¦‚æœå½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œå†™å…¥æ–‡ä»¶
                            if current_shard_size + sample_bytes > shard_size_bytes and current_shard_data:
                                shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
                                self.write_shard(current_shard_data, shard_path)
                                current_shard_data = []
                                current_shard_size = 0
                                shard_idx += 1
                                # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥æ˜¯ç´¯åŠ å½“å‰åˆ†ç‰‡çš„æ•°æ®é‡ï¼Œè€Œä¸æ˜¯ç´¯åŠ ç©ºåˆ—è¡¨
                                # ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—å·²å†™å…¥çš„tokenæ•°
                                total_written_tokens += sum(item['token_count'] for item in current_shard_data)
                            # æ·»åŠ åˆ°å½“å‰åˆ†ç‰‡
                            current_shard_data.append({
                                'source': str(row['source']),
                                'category': str(row['category']),
                                'domain': str(row['domain']),
                                'language': str(row['language']),
                                'token_count': int(row['token_count']),
                                'text': str(row['text'])
                            })
                            current_shard_size += sample_bytes
                progress = (file_idx + 1) / total_files
                progress_bar.progress(progress)
                status_text.text(f"å¤„ç†è¿›åº¦: {file_idx+1}/{total_files} æ–‡ä»¶ | å·²å†™å…¥: {total_written_tokens/1e9:.2f}B tokens")
            except Exception as e:
                st.warning(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                continue
        # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
        if current_shard_data:
            shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
            self.write_shard(current_shard_data, shard_path)
            # ä¿®å¤ï¼šåœ¨å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡åï¼Œæ›´æ–°æ€»å†™å…¥tokenæ•°
            total_written_tokens += sum(item['token_count'] for item in current_shard_data)
        progress_bar.empty()
        status_text.empty()
        st.success(f"é‡‡æ ·å®Œæˆï¼å…±å†™å…¥ {total_written_tokens/1e9:.2f}B tokensï¼Œ{shard_idx} ä¸ªåˆ†ç‰‡")
        return total_written_tokens

    def calculate_sampling_weights(self, target_ratios):
        """è®¡ç®—é‡‡æ ·æƒé‡"""
        weights = {}
        for dim, targets in target_ratios.items():
            weights[dim] = {}
            # ä¿®å¤ï¼šæ­£ç¡®è®¿é—®ç»Ÿè®¡ä¿¡æ¯
            current_dist = self.stats.get('dimensions', {}).get(dim, {})
            for cat, target_ratio in targets.items():
                current_ratio = current_dist.get(cat, 0)
                if current_ratio > 0:
                    # é™åˆ¶è°ƒæ•´å› å­èŒƒå›´ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
                    factor = max(0.1, min(10.0, target_ratio / (current_ratio + 1e-8)))
                    weights[dim][cat] = factor
                else:
                    weights[dim][cat] = 0
        return weights

    def calculate_row_probability(self, row, weights):
        """è®¡ç®—å•è¡Œçš„é‡‡æ ·æ¦‚ç‡"""
        prob = 1.0
        dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
        for dim in dimensions:
            if dim in weights and row[dim] in weights[dim]:
                prob *= weights[dim][row[dim]]
        # é™åˆ¶æ¦‚ç‡åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(1.0, prob))

    def write_shard(self, data, shard_path):
        """å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
        try:
            with open(shard_path, 'w', encoding='utf-8') as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n') # ä¿®å¤ï¼šæ·»åŠ æ¢è¡Œç¬¦
        except Exception as e:
            st.error(f"å†™å…¥åˆ†ç‰‡ {shard_path} å¤±è´¥: {str(e)}")

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low <= token_count < high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

@st.cache_data
def calculate_distribution_cached(df, column, weights=None):
    """ç¼“å­˜ç‰ˆæœ¬çš„åˆ†å¸ƒè®¡ç®—"""
    return calculate_distribution(df, column, weights)

def advanced_ipf_solver(df, target_ratios, target_total, max_iter=100, tol=0.005):
    """
    æ”¹è¿›çš„IPFæ±‚è§£å™¨ - æ”¯æŒå¤šç»´åº¦åŒæ—¶ä¼˜åŒ–
    ä¿®å¤ï¼šä¿è¯æ‰€æœ‰ç»´åº¦éƒ½æ”¶æ•›å†åœæ­¢è¿­ä»£
    """
    # åˆå§‹åŒ–æƒé‡
    weights = np.ones(len(df))
    total_tokens = df['token_count'].sum()
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        for cat, ratio in targets.items():
            # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
            if cat not in df[dim].values:
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False, {}
            # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
            orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®æ»¡è¶³")
        # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False, {}
    # å¼€å§‹IPFè¿­ä»£ - ä¿®å¤ï¼šä¿è¯æ‰€æœ‰ç»´åº¦éƒ½æ”¶æ•›å†åœæ­¢
    all_dims = set(target_ratios.keys())
    converged_dims = set()  # è®°å½•å·²æ”¶æ•›çš„ç»´åº¦
    for iter in range(max_iter):
        prev_weights = weights.copy()
        max_errors = {}
        iteration_converged_dims = set()  # æœ¬è½®è¿­ä»£ä¸­æ”¶æ•›çš„ç»´åº¦
        # æŒ‰ç»´åº¦è¿­ä»£è°ƒæ•´
        for dim, targets in target_ratios.items():
            dim_max_error = 0
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / np.sum(weights * df['token_count'])
                # è®¡ç®—è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5 and target_ratio > 0:
                    factor = target_ratio / (current_ratio + 1e-8)
                    # é™åˆ¶è°ƒæ•´å¹…åº¦ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
                    factor = max(0.5, min(2.0, factor))
                    weights[mask] *= factor
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                dim_max_error = max(dim_max_error, error)
            max_errors[dim] = dim_max_error
            # æ£€æŸ¥è¯¥ç»´åº¦æ˜¯å¦æ”¶æ•›
            if dim_max_error < tol:
                iteration_converged_dims.add(dim)
        # æ›´æ–°æ”¶æ•›ç»´åº¦é›†åˆ
        converged_dims.update(iteration_converged_dims)
        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•›
        if len(converged_dims) == len(all_dims):
            st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
            break
        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-5:
            st.info(f"âš ï¸ æƒé‡å˜åŒ–è¿‡å°ï¼Œåœ¨ç¬¬ {iter+1} è½®è¿­ä»£ååœæ­¢")
            break
    # ç¼©æ”¾è‡³ç›®æ ‡æ€»é‡
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        weights *= (target_total / current_total)
    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    final_errors = {}
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        dim_max_error = 0
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            actual_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / target_total
            actual_dist[dim][cat] = actual_ratio
            target_ratio = target_ratios[dim][cat]
            error = abs(actual_ratio - target_ratio)
            dim_max_error = max(dim_max_error, error)
        final_errors[dim] = dim_max_error
    # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·®
    st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
    for dim, error in final_errors.items():
        if error <= tol:
            st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
    is_converged = all(error <= tol for error in final_errors.values())
    return weights, actual_dist, is_converged, final_errors

def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])
    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        if len(remaining) > 0:
            remaining_token_sum = remaining['token_count'].sum()
            if remaining_token_sum > 0:
                remaining_prob = (additional * remaining['token_count'] / remaining_token_sum)
                # ç¡®ä¿æ¦‚ç‡ä¸è¶…è¿‡1
                remaining['prob'] = np.minimum(remaining_prob, 1.0)
                retained[~retained] = np.random.random(len(remaining)) < remaining['prob']
    return df[retained].copy()

def write_shard_batch(rows, shard_path):
    """æ‰¹é‡å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
    try:
        with open(shard_path, 'w', encoding='utf-8') as f:
            for row in rows:
                f.write(json.dumps({
                    'source': row['source'],
                    'category': row['category'],
                    'domain': row['domain'],
                    'language': row['language'],
                    'token_count': row['token_count'],
                    'text': row['text']
                }, ensure_ascii=False) + '\n') # ä¿®å¤ï¼šæ·»åŠ æ¢è¡Œç¬¦
        return True, shard_path
    except Exception as e:
        return False, f"Error writing {shard_path}: {str(e)}"

def export_shards_parallel(df, output_path, shard_size_gb=1, max_workers=4):
    """å¹¶è¡Œåˆ†ç‰‡å¯¼å‡ºJSONLæ–‡ä»¶"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªåˆ†ç‰‡
    total_bytes = df['text'].apply(lambda x: len(x.encode('utf-8')) + 1).sum()
    num_shards = math.ceil(total_bytes / shard_size_bytes)
    st.info(f"éœ€è¦åˆ›å»º {num_shards} ä¸ªåˆ†ç‰‡æ–‡ä»¶")
    # å°†æ•°æ®åˆ†ç»„åˆ°åˆ†ç‰‡ä¸­
    shards_data = []
    current_shard = []
    current_size = 0
    shard_idx = 1
    progress_bar = st.progress(0)
    status_text = st.empty()
    for idx, row in df.iterrows():
        # è®¡ç®—å½“å‰æ ·æœ¬å­—èŠ‚æ•°
        sample_bytes = len(row['text'].encode('utf-8')) + 1  # +1 for newline
        # å¦‚æœå½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œä¿å­˜å½“å‰åˆ†ç‰‡
        if current_size + sample_bytes > shard_size_bytes and current_shard:
            shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
            shards_data.append((current_shard.copy(), shard_path))
            current_shard = []
            current_size = 0
            shard_idx += 1
        # æ·»åŠ æ ·æœ¬åˆ°å½“å‰åˆ†ç‰‡
        current_shard.append(row.to_dict())
        current_size += sample_bytes
        # æ›´æ–°è¿›åº¦
        if idx % 1000 == 0:
            progress = min(idx / len(df), 1.0)
            progress_bar.progress(progress)
            status_text.text(f"åˆ†ç‰‡å‡†å¤‡ä¸­: {idx+1}/{len(df)} | å½“å‰åˆ†ç‰‡: {shard_idx}")
    # ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç‰‡
    if current_shard:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        shards_data.append((current_shard, shard_path))
    progress_bar.empty()
    status_text.empty()
    # å¹¶è¡Œå†™å…¥åˆ†ç‰‡æ–‡ä»¶
    st.info(f"å¼€å§‹å¹¶è¡Œå†™å…¥ {len(shards_data)} ä¸ªåˆ†ç‰‡æ–‡ä»¶...")
    success_count = 0
    failed_files = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰å†™å…¥ä»»åŠ¡
        future_to_shard = {executor.submit(write_shard_batch, rows, path): (i, path) 
                          for i, (rows, path) in enumerate(shards_data)}
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(as_completed(future_to_shard)):
            success, result = future.result()
            if success:
                success_count += 1
            else:
                failed_files.append(result)
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(shards_data)
            progress_bar.progress(progress)
            status_text.text(f"å†™å…¥è¿›åº¦: {i+1}/{len(shards_data)} | æˆåŠŸ: {success_count}")
    progress_bar.empty()
    status_text.empty()
    # æŠ¥å‘Šç»“æœ
    if failed_files:
        st.warning(f"å¯¼å‡ºå®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_files)}")
        for error in failed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            st.error(error)
        if len(failed_files) > 5:
            st.error(f"... è¿˜æœ‰ {len(failed_files) - 5} ä¸ªé”™è¯¯")
    else:
        st.success(f"å¯¼å‡ºå®Œæˆï¼å…± {success_count} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

def parse_jsonl_file_pandas(file_path, chunksize=50000):
    """ä½¿ç”¨pandasé«˜æ•ˆè§£æJSONLæ–‡ä»¶ï¼ˆæ”¯æŒåˆ†å—è¯»å–ï¼‰"""
    records = []
    try:
        # åˆ†å—è¯»å–ä»¥å¤„ç†å¤§æ–‡ä»¶
        chunk_iter = pd.read_json(file_path, lines=True, chunksize=chunksize)
        # å¦‚æœä¸æ˜¯è¿­ä»£å™¨ï¼Œè¯´æ˜æ–‡ä»¶è¾ƒå°ï¼Œç›´æ¥è¯»å–
        if not hasattr(chunk_iter, '__iter__'):
            chunk_iter = [chunk_iter]
        for chunk in chunk_iter:
            # å¿…éœ€å­—æ®µ
            required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
            # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
            if all(col in chunk.columns for col in required_fields):
                # åªä¿ç•™å¿…éœ€å­—æ®µ
                chunk = chunk[required_fields]
                # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
                chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                chunk.dropna(subset=['token_count'], inplace=True)
                chunk['token_count'] = chunk['token_count'].astype(int)
                # ç¡®ä¿å…¶ä»–å­—æ®µä¸ºå­—ç¬¦ä¸²ç±»å‹
                string_fields = ['source', 'category', 'domain', 'language', 'text']
                for field in string_fields:
                    chunk[field] = chunk[field].astype(str)
                # æ·»åŠ token_binåˆ—
                chunk['token_bin'] = chunk['token_count'].apply(get_token_bin)
                # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨
                records.extend(chunk.to_dict(orient='records'))
            else:
                print(f"Missing required fields in {file_path}")
                continue
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
    return records

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="./test_data")

# æ•°æ®å¤„ç†æ¨¡å¼é€‰æ‹©
# ç¡®ä¿ session_state ä¸­æœ‰ processing_mode
if 'processing_mode' not in st.session_state:
    st.session_state.processing_mode = "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰"

# æ ¹æ® session_state ä¸­çš„å€¼ç¡®å®š radio çš„ index
current_index = 0 if st.session_state.processing_mode == "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰" else 1

# åˆ›å»º radio æŒ‰é’®
processing_mode_selection = st.sidebar.radio(
    "å¤„ç†æ¨¡å¼",
    ["å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰", "æµå¼æ¨¡å¼ï¼ˆå¤§æ•°æ®ï¼‰"],
    index=current_index, # ä½¿ç”¨è®¡ç®—å‡ºçš„ index
    help="å†…å­˜æ¨¡å¼é€‚ç”¨äº<100GBæ•°æ®ï¼Œæµå¼æ¨¡å¼é€‚ç”¨äº>100GBæ•°æ®"
)

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        try:
            items = os.listdir(data_path)
            st.sidebar.info(f"åŒ…å« {len(items)} ä¸ªé¡¹ç›®")
            jsonl_files = [f for f in items if f.lower().endswith('.jsonl')]
            st.sidebar.info(f"å…¶ä¸­ {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        except Exception as e:
            st.sidebar.warning(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        data_path = os.path.normpath(data_path)
        st.sidebar.info(f"æ­£åœ¨å¤„ç†è·¯å¾„: {data_path}")
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                if processing_mode == "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰":
                    # åŸæœ‰çš„å†…å­˜æ¨¡å¼å¤„ç†
                    jsonl_files = []
                    for root, _, files in os.walk(data_path):
                        for file in files:
                            if file.lower().endswith('.jsonl'):
                                jsonl_files.append(os.path.join(root, file))
                    st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                    if not jsonl_files:
                        st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
                        st.sidebar.caption("- è·¯å¾„æ˜¯å¦æ­£ç¡®")
                        st.sidebar.caption("- æ–‡ä»¶åç¼€æ˜¯å¦ä¸º.jsonlï¼ˆé.JSONLï¼‰")
                        st.sidebar.caption("- æ˜¯å¦æœ‰æ–‡ä»¶è®¿é—®æƒé™")
                        st.stop()
                    # å¹¶è¡Œè¯»å–æ‰€æœ‰JSONLæ–‡ä»¶ï¼ˆä½¿ç”¨pandasä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                    all_data = []
                    progress_bar = st.sidebar.progress(0)
                    status_text = st.sidebar.empty()
                    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶
                    with ThreadPoolExecutor(max_workers=8) as executor:
                        future_to_file = {executor.submit(parse_jsonl_file_pandas, file, 50000): file for file in jsonl_files}
                        for i, future in enumerate(as_completed(future_to_file)):
                            result = future.result()
                            all_data.extend(result)
                            status_text.text(f"âœ… å·²å¤„ç† {i+1}/{len(jsonl_files)} ä¸ªæ–‡ä»¶")
                            progress_bar.progress((i + 1) / len(jsonl_files))
                    progress_bar.empty()
                    status_text.empty()
                    if all_data:
                        # è½¬ä¸ºDataFrame
                        df = pd.DataFrame(all_data)
                        total_tokens = df['token_count'].sum()
                        # å­˜å‚¨åˆ°session state
                        st.session_state.df = df
                        st.session_state.total_tokens = total_tokens
                        st.session_state.processing_mode = "å†…å­˜"
                        st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(df):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{total_tokens/1e9:.2f}B tokens")
                    else:
                        st.sidebar.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                        st.sidebar.info("æœ‰æ•ˆJSONLæ ·æœ¬ç¤ºä¾‹:")
                        st.sidebar.code('''{"source": "CCI4", "category": "book", "domain": "science", "language": "CN", "token_count": 1234, "text": "ç¤ºä¾‹æ–‡æœ¬..."}''')
                        st.stop()
                else:  # æµå¼æ¨¡å¼
                    # å¤§æ•°æ®æµå¼å¤„ç†
                    sampler = LargeDataSampler(data_path)
                    file_count = sampler.scan_files()
                    st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {file_count} ä¸ªJSONLæ–‡ä»¶")
                    if file_count == 0:
                        st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
                        st.stop()
                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    stats = sampler.calculate_statistics()
                    # å­˜å‚¨åˆ°session state
                    st.session_state.sampler = sampler
                    st.session_state.stats = stats
                    st.session_state.processing_mode = "æµå¼"
                    st.sidebar.success(f"ğŸ‰ ç»Ÿè®¡å®Œæˆï¼å…± {stats['total_samples']:,} ä¸ªæ ·æœ¬ï¼Œ{stats['total_tokens']/1e9:.2f}B tokens")
            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'processing_mode' in st.session_state and st.session_state.processing_mode in ["å†…å­˜", "æµå¼"]:
    processing_mode_session = st.session_state.processing_mode
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    target_ratios = {}
    # åˆå§‹åŒ– session_state å­˜å‚¨ç›®æ ‡æ¯”ä¾‹
    if 'target_ratios' not in st.session_state:
        st.session_state.target_ratios = {}
    # è·å– token_bin é¡ºåº
    token_bin_order = [label for _, _, label in TOKEN_BINS]
    if processing_mode_session == "å†…å­˜":
        df = st.session_state.df
        total_tokens = st.session_state.total_tokens
        # ç¡®ä¿token_binåˆ—å­˜åœ¨
        if 'token_bin' not in df.columns:
            df['token_bin'] = df['token_count'].apply(get_token_bin)
        for dim in dimensions:
            st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
            # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼ï¼ˆæŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼‰
            if dim == 'token_bin':
                values = sorted(df['token_bin'].unique(), key=lambda x: token_bin_order.index(x) if x in token_bin_order else len(token_bin_order))
            else:
                values = sorted(df[dim].unique())
            # è®¡ç®—å½“å‰åˆ†å¸ƒ
            if dim == 'token_bin':
                current_dist = df.groupby('token_bin')['token_count'].sum() / total_tokens
            else:
                current_dist = df.groupby(dim)['token_count'].sum() / total_tokens
            # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
            if dim not in st.session_state.target_ratios:
                st.session_state.target_ratios[dim] = {}
            target_ratios[dim] = {}
            total_ratio = 0.0
            # æ¯è¡Œæœ€å¤šæ”¾ 3 ä¸ªè¾“å…¥æ¡†
            items_per_row = 3
            for i_start in range(0, len(values), items_per_row):
                cols = st.sidebar.columns(items_per_row)
                for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                    current_ratio = current_dist.get(val, 0.0)
                    with cols[i_offset]:
                        ratio = st.number_input(
                            label=f"{val}",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(current_ratio),
                            step=0.01,
                            format="%.3f",  # æ˜¾ç¤º3ä½å°æ•°
                            key=f"{dim}_{val}"
                        )
                        st.session_state.target_ratios[dim][val] = ratio
                        target_ratios[dim][val] = ratio
                        total_ratio += ratio
            # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
            st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
            if not (0.99 <= total_ratio <= 1.01):
                st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    else:  # streaming mode
        stats = st.session_state.stats
        for dim in dimensions:
            st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
            # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼
            if dim == 'token_bin':
                values = sorted(stats['dimensions'][dim].keys(), key=lambda x: token_bin_order.index(x) if x in token_bin_order else len(token_bin_order))
            else:
                values = sorted(stats['dimensions'][dim].keys())
            # è·å–å½“å‰åˆ†å¸ƒ
            current_dist = stats['dimensions'][dim]
            # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
            if dim not in st.session_state.target_ratios:
                st.session_state.target_ratios[dim] = {}
            target_ratios[dim] = {}
            total_ratio = 0.0
            # æ¯è¡Œæœ€å¤šæ”¾ 3 ä¸ªè¾“å…¥æ¡†
            items_per_row = 3
            for i_start in range(0, len(values), items_per_row):
                cols = st.sidebar.columns(items_per_row)
                for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                    current_ratio = current_dist.get(val, 0.0)
                    with cols[i_offset]:
                        ratio = st.number_input(
                            label=f"{val}",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(current_ratio),
                            step=0.01,
                            format="%.3f",
                            key=f"{dim}_{val}_stream"
                        )
                        st.session_state.target_ratios[dim][val] = ratio
                        target_ratios[dim][val] = ratio
                        total_ratio += ratio
            # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
            st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
            if not (0.99 <= total_ratio <= 1.01):
                st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        if processing_mode_session == "å†…å­˜":
            with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
                # ä» session_state è¯»å–æœ€æ–°çš„ç›®æ ‡æ¯”ä¾‹
                target_ratios = st.session_state.target_ratios
                # è¿è¡Œæ”¹è¿›çš„IPFæ±‚è§£å™¨
                weights, actual_dist, converged, final_errors = advanced_ipf_solver(
                    df, 
                    target_ratios, 
                    target_total,
                    max_iter=100,
                    tol=0.005
                )
                if weights is not None:
                    # å­˜å‚¨é‡‡æ ·ç»“æœå’Œè¯¯å·®ä¿¡æ¯
                    sampled_df = sample_dataset(df, weights, target_total)
                    st.session_state.sampled_df = sampled_df
                    st.session_state.final_errors = final_errors  # å­˜å‚¨è¯¯å·®ä¿¡æ¯
                    # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                    st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                    st.sidebar.info(f"å®é™…æ€»é‡: {sampled_df['token_count'].sum()/1e9:.2f}B tokens")
                    # æ˜¾ç¤ºæ”¶æ•›çŠ¶æ€
                    if converged:
                        st.sidebar.success("âœ… æ‰€æœ‰ç»´åº¦é…æ¯”å‡å·²æ»¡è¶³ï¼")
                    else:
                        st.sidebar.warning("âš ï¸ éƒ¨åˆ†ç»´åº¦é…æ¯”æœªå®Œå…¨æ»¡è¶³ï¼Œè¯·æ£€æŸ¥è¯¯å·®æŠ¥å‘Š")
        else:  # streaming mode
            with st.spinner("æ­£åœ¨æµå¼é‡‡æ ·å¤§æ•°æ®é›†..."):
                sampler = st.session_state.sampler
                target_ratios = st.session_state.target_ratios
                # è·å–è¾“å‡ºè·¯å¾„
                output_path = st.sidebar.text_input("è¾“å‡ºè·¯å¾„", value="./sampled_datasets")
                shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
                # æ‰§è¡Œæµå¼é‡‡æ ·
                total_written = sampler.sample_data_streaming(
                    target_ratios, 
                    target_total, 
                    output_path, 
                    shard_size
                )
                st.sidebar.success(f"æµå¼é‡‡æ ·å®Œæˆï¼å…±å†™å…¥ {total_written/1e9:.2f}B tokens")
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    max_export_workers = st.sidebar.slider("å¯¼å‡ºå¹¶è¡Œçº¿ç¨‹æ•°", min_value=1, max_value=16, value=4)
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                export_shards_parallel(st.session_state.sampled_df, output_path, shard_size, max_export_workers)
    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    if processing_mode_session == "å†…å­˜":
        # å†…å­˜æ¨¡å¼çš„å›¾è¡¨å±•ç¤º
        df = st.session_state.df
        total_tokens = st.session_state.total_tokens
        # ç¡®ä¿token_binåˆ—å­˜åœ¨
        if 'token_bin' not in df.columns:
            df['token_bin'] = df['token_count'].apply(get_token_bin)
        # åˆ›å»ºå›¾è¡¨å¸ƒå±€
        col1, col2 = st.columns(2)
        col3, col4 = st.columns(2)
        col5, col6 = st.columns(2)
        # 1. Source é…æ¯”å›¾
        with col1:
            st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
            source_dist = calculate_distribution_cached(df, 'source')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        # 2. Category é…æ¯”å›¾
        with col2:
            st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
            category_dist = calculate_distribution_cached(df, 'category')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        # 3. Domain é…æ¯”å›¾
        with col3:
            st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
            domain_dist = calculate_distribution_cached(df, 'domain')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        # 4. Language é…æ¯”å›¾
        with col4:
            st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
            lang_dist = calculate_distribution_cached(df, 'language')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        # 5. Token Count é…æ¯”å›¾
        with col5:
            st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
            token_dist = calculate_distribution_cached(df, 'token_bin')
            # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨å¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—
            ordered_labels = [label for _, _, label in TOKEN_BINS]
            for label in ordered_labels:
                if label not in token_dist:
                    token_dist[label] = 0.0
            token_dist = token_dist.reindex(ordered_labels)
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.bar(token_dist.index, token_dist.values)
            ax.set_ylabel('Ratio')
            ax.set_title('Token length distribution')
            for i, v in enumerate(token_dist.values):
                ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
            st.pyplot(fig)
        # 6. å­ç±»åˆ†å¸ƒå›¾
        with col6:
            st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
            # åˆ›å»ºå­ç±»ç»„åˆ
            df['subclass'] = df['source'] + "+" + df['category'] + "+" + df['domain'] + "+" + df['language']
            subclass_dist = calculate_distribution_cached(df, 'subclass')
            # å–Top 10
            top10 = subclass_dist.head(10)
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.barh(top10.index, top10.values)
            ax.set_xlabel('æ¯”ä¾‹')
            ax.set_title('Top 10 distribution of subclass combinations')
            # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
            for i, v in enumerate(top10.values):
                ax.text(v + 0.005, i, f'{v:.1%}', va='center')
            plt.tight_layout()
            st.pyplot(fig)
        # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
        st.divider()
        st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
        st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
        st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
        st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")
        # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
        if 'sampled_df' in st.session_state:
            st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
            sampled_df = st.session_state.sampled_df
            sampled_tokens = sampled_df['token_count'].sum()
            # ç¡®ä¿é‡‡æ ·æ•°æ®ä¹Ÿæœ‰token_binåˆ—
            if 'token_bin' not in sampled_df.columns:
                sampled_df['token_bin'] = sampled_df['token_count'].apply(get_token_bin)
            st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
            st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")
            # æ¯”è¾ƒå…³é”®ç»´åº¦
            st.subheader("ğŸ“ˆ é…æ¯”å¯¹æ¯”åˆ†æ")
            comparison_cols = st.columns(len(['language', 'domain', 'category', 'token_bin']))
            # è·å–å­˜å‚¨çš„è¯¯å·®ä¿¡æ¯
            final_errors = st.session_state.get('final_errors', {})
            for i, dim in enumerate(['language', 'domain', 'category', 'token_bin']):
                with comparison_cols[i]:
                    if dim in final_errors:
                        # ä½¿ç”¨IPFæ±‚è§£å™¨è®¡ç®—å‡ºçš„å‡†ç¡®è¯¯å·®
                        error = final_errors[dim]
                        st.metric(f"{dim.capitalize()}", f"{error*100:.1f}%", "æœ€å¤§è¯¯å·®")
                    else:
                        # å¤‡ç”¨è®¡ç®—æ–¹æ³•
                        orig_dist = calculate_distribution_cached(df, dim)
                        sampled_dist = calculate_distribution_cached(sampled_df, dim)
                        # è®¡ç®—æœ€å¤§è¯¯å·®
                        max_error = 0
                        for cat in orig_dist.index:
                            orig = orig_dist.get(cat, 0)
                            sampled = sampled_dist.get(cat, 0)
                            error = abs(orig - sampled)
                            max_error = max(max_error, error)
                        st.metric(f"{dim.capitalize()}", f"{max_error*100:.1f}%", "æœ€å¤§è¯¯å·®")
    else:
        # æµå¼æ¨¡å¼çš„ç»Ÿè®¡ä¿¡æ¯å±•ç¤º
        stats = st.session_state.stats
        st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯")
        st.write(f"**æ€»æ ·æœ¬æ•°**: {stats['total_samples']:,}")
        st.write(f"**æ€»Tokenæ•°**: {stats['total_tokens']/1e9:.2f} B (10äº¿)")
        st.write(f"**å¹³å‡Tokené•¿åº¦**: {stats['total_tokens']/stats['total_samples']:.0f}")
        # æ˜¾ç¤ºå„ç»´åº¦åˆ†å¸ƒ
        st.subheader("å„ç»´åº¦åˆ†å¸ƒ")
        for dim in ['source', 'category', 'domain', 'language', 'token_bin']:
            if dim in stats['dimensions']:
                st.write(f"**{dim.capitalize()} åˆ†å¸ƒ**:")
                dist_data = [(k, v) for k, v in stats['dimensions'][dim].items()]
                dist_data.sort(key=lambda x: x[1], reverse=True)
                # æ˜¾ç¤ºå‰10ä¸ª
                for key, ratio in dist_data[:10]:
                    st.write(f"  {key}: {ratio:.2%}")
                if len(dist_data) > 10:
                    st.write(f"  ... è¿˜æœ‰ {len(dist_data) - 10} ä¸ªç±»åˆ«")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png", width=300)

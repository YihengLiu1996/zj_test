import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls
from concurrent.futures import ThreadPoolExecutor, as_completed
import math
import dask.dataframe as dd
from dask import delayed
import gc
from dask.distributed import Client
import psutil

# é…ç½®Daskè°ƒåº¦å™¨
try:
    client = Client(processes=False, threads_per_worker=4, n_workers=2, memory_limit='4GB')
    st.sidebar.success("âœ… Daskè°ƒåº¦å™¨å·²å¯åŠ¨")
except Exception as e:
    st.sidebar.warning(f"âš ï¸ Daskè°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
    client = None

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·ï¼ˆæ”¯æŒTBçº§æ•°æ®ï¼‰")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4096, "0-4k"),
    (4096, 8192, "4k-8k"),
    (8192, 16384, "8k-16k"),
    (16384, 32768, "16k-32k"),
    (32768, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low <= token_count < high:
            return label
    return ">32k"

def calculate_distribution_chunked(df_chunk, column, weights_chunk=None):
    """è®¡ç®—åˆ†å—åŠ æƒåˆ†å¸ƒ"""
    if weights_chunk is None:
        weights_chunk = df_chunk['token_count']
    chunk_total = weights_chunk.sum()
    if chunk_total == 0:
        return pd.Series(dtype=float)
    chunk_dist = df_chunk.groupby(column).apply(lambda x: np.sum(weights_chunk[x.index]) / chunk_total)
    return chunk_dist.sort_values(ascending=False)

@st.cache_data
def calculate_distribution_cached(df_path, column):
    """ç¼“å­˜ç‰ˆæœ¬çš„åˆ†å¸ƒè®¡ç®—ï¼ˆæ”¯æŒå¤§æ–‡ä»¶ï¼‰"""
    # ä½¿ç”¨Daskè¯»å–å¹¶è®¡ç®—åˆ†å¸ƒ
    df = dd.read_json(df_path, lines=True)
    df['token_bin'] = df['token_count'].apply(get_token_bin, meta=('token_bin', 'object'))
    total_tokens = df['token_count'].sum().compute()
    dist = df.groupby(column)['token_count'].sum().compute() / total_tokens
    return dist.sort_values(ascending=False)

# ========== å…¨å±€åˆ†å¸ƒè®¡ç®—å‡½æ•° ==========
@st.cache_data(ttl=3600)
def compute_global_distribution(df_paths, dimensions):
    """ä½¿ç”¨ Dask è®¡ç®—æ‰€æœ‰ç»´åº¦çš„ç¡®åˆ‡åˆ†å¸ƒ"""
    st.info("ğŸ”„ æ­£åœ¨è®¡ç®—å…¨å±€åˆ†å¸ƒ...")
    
    # åˆå§‹åŒ–å„ç»´åº¦çš„ç»Ÿè®¡ç»“æœ
    results = {}
    
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶ä¸ºä¸€ä¸ª Dask DataFrame
    ddf_list = [dd.read_json(path, lines=True) for path in df_paths]
    combined_ddf = dd.concat(ddf_list, interleave_partitions=True)
    
    # æ·»åŠ  token_bin åˆ—
    combined_ddf['token_bin'] = combined_ddf['token_count'].apply(get_token_bin, meta=('token_bin', 'object'))
    
    # æ€» token æ•°
    total_tokens = delayed(sum)([ddf['token_count'].sum() for ddf in ddf_list])
    total_tokens = total_tokens.compute()
    
    # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œåˆ†ç»„ç»Ÿè®¡
    for dim in dimensions:
        dist = combined_ddf.groupby(dim)['token_count'].sum().compute() / total_tokens
        results[dim] = dist.sort_values(ascending=False)
    
    st.success("âœ… å…¨å±€åˆ†å¸ƒè®¡ç®—å®Œæˆï¼")
    return results, total_tokens

def advanced_ipf_solver_chunked(df_paths, target_ratios, target_total, priority_order, max_iter=100, tol=0.005):
    """
    æ”¹è¿›çš„IPFæ±‚è§£å™¨ - æ”¯æŒå¤§æ–‡ä»¶åˆ†å—å¤„ç†
    :param df_paths: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param priority_order: ä¼˜å…ˆçº§é¡ºåºåˆ—è¡¨
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(0.5%)
    :return: é‡‡æ ·æƒé‡å­—å…¸, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    
    # åˆå§‹åŒ–ï¼šè®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    st.info("æ­£åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯...")
    global_stats = {}
    total_tokens = 0
    
    # åˆ†å—è®¡ç®—æ€»tokenæ•°å’Œå„ç»´åº¦åŸå§‹æ¯”ä¾‹
    for dim in target_ratios.keys():
        global_stats[dim] = {}
    
    for file_path in df_paths:
        df_chunk = dd.read_json(file_path, lines=True)
        chunk_total = df_chunk['token_count'].sum().compute()
        total_tokens += chunk_total
        
        for dim in target_ratios.keys():
            dim_stats = df_chunk.groupby(dim)['token_count'].sum().compute()
            for cat, count in dim_stats.items():
                if cat not in global_stats[dim]:
                    global_stats[dim][cat] = 0
                global_stats[dim][cat] += count
    
    # è®¡ç®—åŸå§‹æ¯”ä¾‹
    orig_ratios = {}
    for dim, cats in global_stats.items():
        orig_ratios[dim] = {cat: count/total_tokens for cat, count in cats.items()}
    
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        for cat, ratio in targets.items():
            if cat not in orig_ratios.get(dim, {}):
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            orig_ratio = orig_ratios[dim][cat]
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})å¯èƒ½è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})")
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False

    if set(priority_order) != set(target_ratios.keys()):
        st.error(f"é”™è¯¯ï¼šä¼˜å…ˆçº§é¡ºåºå¿…é¡»åŒ…å«æ‰€æœ‰ç»´åº¦ä¸”ä¸èƒ½é‡å¤")
        return None, None, False

    # åˆå§‹åŒ–æƒé‡å­—å…¸ï¼ˆæŒ‰æ–‡ä»¶å’Œç´¢å¼•å­˜å‚¨ï¼‰
    st.info("æ­£åœ¨åˆå§‹åŒ–æƒé‡...")
    weights_dict = {}  # {file_path: {index: weight}}
    
    # ç¬¬ä¸€æ¬¡éå†ï¼šåˆå§‹åŒ–æƒé‡
    initial_scale_guess = target_total / total_tokens if total_tokens > 0 else 1.0
    for file_path in df_paths:
        df_chunk = dd.read_json(file_path, lines=True)
        # è·å–æ‰€æœ‰åˆ†åŒºçš„ç´¢å¼•
        partitions = df_chunk.to_delayed()
        weights_dict[file_path] = {}
        for i, partition in enumerate(partitions):
            partition_df = partition.compute()
            weights_dict[file_path][i] = np.full(len(partition_df), initial_scale_guess)

    # å¼€å§‹IPFè¿­ä»£
    all_dims = list(target_ratios.keys())
    
    for iter in range(max_iter):
        st.info(f"æ­£åœ¨è¿›è¡Œç¬¬ {iter+1} è½®è¿­ä»£...")
        max_errors = {}
        
        # è®¡ç®—å½“å‰æ€»tokenæ•°
        current_total = 0
        for file_path in df_paths:
            for partition_idx, partition_weights in weights_dict[file_path].items():
                df_partition = dd.read_json(file_path, lines=True).get_partition(partition_idx).compute()
                current_total += np.sum(partition_weights * df_partition['token_count'])
        
        # è®¡ç®—æ€»é‡è¯¯å·®å› å­
        total_factor = (target_total / current_total) if current_total > 1e-5 else 1.0
        total_factor = max(0.8, min(1.2, total_factor))

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºè¿­ä»£è°ƒæ•´ç»´åº¦
        for dim in priority_order:
            targets = target_ratios[dim]
            dim_max_error = 0
            
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰è¯¥ç±»åˆ«çš„å®é™…æ¯”ä¾‹
                current_cat_tokens = 0
                for file_path in df_paths:
                    for partition_idx, partition_weights in weights_dict[file_path].items():
                        df_partition = dd.read_json(file_path, lines=True).get_partition(partition_idx).compute()
                        mask = (df_partition[dim] == cat)
                        current_cat_tokens += np.sum(partition_weights[mask] * df_partition.loc[mask, 'token_count'])
                
                current_ratio = current_cat_tokens / current_total if current_total > 1e-5 else 0.0
                
                # è®¡ç®—æ¯”ä¾‹è°ƒæ•´å› å­
                if current_ratio > 1e-5 and target_ratio > 0:
                    ratio_factor = target_ratio / current_ratio
                    ratio_factor = max(0.7, min(1.4, ratio_factor))
                    combined_factor = ratio_factor * total_factor
                    
                    # æ›´æ–°æƒé‡
                    for file_path in df_paths:
                        for partition_idx, partition_weights in weights_dict[file_path].items():
                            df_partition = dd.read_json(file_path, lines=True).get_partition(partition_idx).compute()
                            mask = (df_partition[dim] == cat)
                            partition_weights[mask] *= combined_factor
                
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                dim_max_error = max(dim_max_error, error)
            
            max_errors[dim] = dim_max_error

        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•›
        if all(error < tol for error in max_errors.values()):
            st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
            break

    # è®¡ç®—æœ€ç»ˆçš„å®é™…åˆ†å¸ƒ
    st.info("æ­£åœ¨è®¡ç®—æœ€ç»ˆåˆ†å¸ƒ...")
    actual_dist = {}
    final_errors = {}
    
    # æœ€ç»ˆæ€»é‡æ ¡å‡†
    current_total = 0
    for file_path in df_paths:
        for partition_idx, partition_weights in weights_dict[file_path].items():
            df_partition = dd.read_json(file_path, lines=True).get_partition(partition_idx).compute()
            current_total += np.sum(partition_weights * df_partition['token_count'])
    
    if current_total > 0:
        final_scale_factor = target_total / current_total
        for file_path in df_paths:
            for partition_idx in weights_dict[file_path].keys():
                weights_dict[file_path][partition_idx] *= final_scale_factor
        current_total = target_total

    # è®¡ç®—å„ç»´åº¦å®é™…åˆ†å¸ƒ
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        dim_max_error = 0
        
        for cat in target_ratios[dim].keys():
            actual_cat_tokens = 0
            for file_path in df_paths:
                for partition_idx, partition_weights in weights_dict[file_path].items():
                    df_partition = dd.read_json(file_path, lines=True).get_partition(partition_idx).compute()
                    mask = (df_partition[dim] == cat)
                    actual_cat_tokens += np.sum(partition_weights[mask] * df_partition.loc[mask, 'token_count'])
            
            actual_ratio = actual_cat_tokens / current_total
            actual_dist[dim][cat] = actual_ratio
            target_ratio = target_ratios[dim][cat]
            error = abs(actual_ratio - target_ratio)
            dim_max_error = max(dim_max_error, error)
        
        final_errors[dim] = dim_max_error

    # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·®
    st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
    for dim in priority_order:
        error = final_errors[dim]
        if error <= tol:
            st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
            
    is_converged = all(error <= tol for error in final_errors.values())
    return weights_dict, actual_dist, is_converged

def sample_dataset_streaming(df_paths, weights_dict, target_total, output_path, shard_size_gb=1):
    """æµå¼é‡‡æ ·å¹¶å¯¼å‡º"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    
    current_shard = []
    current_size = 0
    shard_idx = 1
    total_sampled_tokens = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_files = len(df_paths)
    
    for file_idx, file_path in enumerate(df_paths):
        df_chunk = dd.read_json(file_path, lines=True)
        partitions = df_chunk.to_delayed()
        
        for partition_idx, partition in enumerate(partitions):
            partition_df = partition.compute()
            partition_weights = weights_dict[file_path][partition_idx]
            
            # ç”Ÿæˆä¿ç•™æ¦‚ç‡
            probs = np.minimum(partition_weights, 1.0)
            retained = np.random.random(len(partition_df)) < probs
            
            # å¤„ç†ä¿ç•™çš„è¡Œ
            for idx, row in partition_df[retained].iterrows():
                record = {
                    'source': row['source'],
                    'category': row['category'],
                    'domain': row['domain'],
                    'language': row['language'],
                    'token_count': row['token_count'],
                    'text': row['text']
                }
                
                line = json.dumps(record, ensure_ascii=False) + '\n'
                line_bytes = len(line.encode('utf-8'))
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°åˆ†ç‰‡
                if current_size + line_bytes > shard_size_bytes and current_shard:
                    # å†™å…¥å½“å‰åˆ†ç‰‡
                    shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
                    with open(shard_path, 'w', encoding='utf-8') as f:
                        f.writelines(current_shard)
                    
                    current_shard = []
                    current_size = 0
                    shard_idx += 1
                
                current_shard.append(line)
                current_size += line_bytes
                total_sampled_tokens += row['token_count']
            
            # æ›´æ–°è¿›åº¦
            progress = min((file_idx + (partition_idx + 1) / len(partitions)) / total_files, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"é‡‡æ ·è¿›åº¦: {file_idx+1}/{total_files} æ–‡ä»¶ | åˆ†ç‰‡: {shard_idx} | å·²é‡‡æ ·: {total_sampled_tokens/1e9:.2f}B tokens")
    
    # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
    if current_shard:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        with open(shard_path, 'w', encoding='utf-8') as f:
            f.writelines(current_shard)
    
    progress_bar.empty()
    status_text.empty()
    
    return total_sampled_tokens

def write_shard_batch(rows, shard_path):
    """æ‰¹é‡å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
    try:
        with open(shard_path, 'w', encoding='utf-8') as f:
            for row in rows:
                f.write(json.dumps({
                    'source': row['source'],
                    'category': row['category'],
                    'domain': row['domain'],
                    'language': row['language'],
                    'token_count': row['token_count'],
                    'text': row['text']
                }, ensure_ascii=False) + '\n')
        return True, shard_path
    except Exception as e:
        return False, f"Error writing {shard_path}: {str(e)}"

def parse_jsonl_file_pandas(file_path, chunksize=50000):
    """ä½¿ç”¨pandasé«˜æ•ˆè§£æJSONLæ–‡ä»¶ï¼ˆæ”¯æŒåˆ†å—è¯»å–ï¼‰"""
    records = []
    try:
        chunk_iter = pd.read_json(file_path, lines=True, chunksize=chunksize)
        if not hasattr(chunk_iter, '__iter__'):
            chunk_iter = [chunk_iter]
        for chunk in chunk_iter:
            required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
            if all(col in chunk.columns for col in required_fields):
                chunk = chunk[required_fields]
                chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                chunk.dropna(subset=['token_count'], inplace=True)
                chunk['token_count'] = chunk['token_count'].astype(int)
                string_fields = ['source', 'category', 'domain', 'language', 'text']
                for field in string_fields:
                    chunk[field] = chunk[field].astype(str)
                records.extend(chunk.to_dict(orient='records'))
            else:
                print(f"Missing required fields in {file_path}")
                continue
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
    return records

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        try:
            files = os.listdir(data_path)
            jsonl_files = [f for f in files if f.lower().endswith('.jsonl')]
            st.sidebar.info(f"åŒ…å« {len(files)} ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­ {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        except Exception as e:
            st.sidebar.warning(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        data_path = os.path.normpath(data_path)
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                jsonl_files = []
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            jsonl_files.append(os.path.join(root, file))
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
                    st.sidebar.caption("- è·¯å¾„æ˜¯å¦æ­£ç¡®")
                    st.sidebar.caption("- æ–‡ä»¶åç¼€æ˜¯å¦ä¸º.jsonlï¼ˆé.JSONLï¼‰")
                    st.sidebar.caption("- æ˜¯å¦æœ‰æ–‡ä»¶è®¿é—®æƒé™")
                    st.stop()
                
                # å­˜å‚¨æ–‡ä»¶è·¯å¾„åˆ°session state
                st.session_state.df_paths = jsonl_files
                st.session_state.data_path = data_path
                
                # è®¡ç®—æ€»æ ·æœ¬æ•°å’Œtokenæ•°ï¼ˆé‡‡æ ·ä¼°ç®—ï¼‰
                sample_file = jsonl_files[0]
                try:
                    with open(sample_file, 'r', encoding='utf-8') as f:
                        sample_lines = [next(f).strip() for _ in range(3)]
                    st.sidebar.caption(f"ğŸ“„ é¢„è§ˆ {os.path.basename(sample_file)}:")
                    for line in sample_lines:
                        st.sidebar.caption(f"`{line[:100]}...`")
                except Exception as e:
                    st.sidebar.warning(f"âš ï¸ æ— æ³•è¯»å–ç¤ºä¾‹æ–‡ä»¶: {str(e)}")
                
                # ä¼°ç®—æ€»æ•°æ®é‡
                st.sidebar.info("æ­£åœ¨ä¼°ç®—æ•°æ®æ€»é‡...")
                total_samples = 0
                total_tokens = 0
                sample_count = min(10, len(jsonl_files))  # é‡‡æ ·å‰10ä¸ªæ–‡ä»¶
                
                for i, file_path in enumerate(jsonl_files[:sample_count]):
                    try:
                        df_sample = dd.read_json(file_path, lines=True).head(1000)  # è¯»å–å‰1000è¡Œ
                        total_samples += len(df_sample)
                        total_tokens += df_sample['token_count'].sum()
                    except Exception as e:
                        st.sidebar.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
                
                if total_samples > 0:
                    avg_tokens_per_sample = total_tokens / total_samples
                    estimated_total_samples = int(total_samples / sample_count * len(jsonl_files))
                    estimated_total_tokens = int(estimated_total_samples * avg_tokens_per_sample)
                    st.session_state.estimated_total_tokens = estimated_total_tokens
                    st.session_state.estimated_total_samples = estimated_total_samples
                    st.sidebar.success(f"ğŸ‰ æ•°æ®æ‰«æå®Œæˆï¼")
                    st.sidebar.info(f"ä¼°ç®—æ ·æœ¬æ•°: {estimated_total_samples:,}")
                    st.sidebar.info(f"ä¼°ç®—Tokenæ•°: {estimated_total_tokens/1e9:.2f}B tokens")
                else:
                    st.sidebar.error("âŒ æ— æ³•ä¼°ç®—æ•°æ®é‡")
                    st.stop()
                    
            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df_paths' in st.session_state:
    df_paths = st.session_state.df_paths
    estimated_total_tokens = st.session_state.get('estimated_total_tokens', 0)
    
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    
    # å®šä¹‰ç»´åº¦
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    
    # ä¼˜å…ˆçº§æ’åºè¾“å…¥
    st.sidebar.subheader("ğŸ“Œ ä¼˜å…ˆçº§æ’åº")
    st.sidebar.info("è¯·æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½é€‰æ‹©ç»´åº¦")
    
    if 'priority_order' not in st.session_state:
        st.session_state.priority_order = dimensions.copy()
        
    priority_placeholders = {}
    temp_priority_order = st.session_state.priority_order.copy()
    
    for i in range(len(dimensions)):
        with st.sidebar.container():
            cols = st.sidebar.columns([1, 4])
            cols[0].markdown(f"**{i+1}.**")
            available_dims = [dim for dim in dimensions if dim not in temp_priority_order[:i]]
            selected_dim = cols[1].selectbox(
                f"é€‰æ‹©ç¬¬ {i+1} ä¼˜å…ˆçº§ç»´åº¦",
                options=available_dims,
                index=available_dims.index(temp_priority_order[i]) if temp_priority_order[i] in available_dims else 0,
                key=f"priority_{i}"
            )
            temp_priority_order[i] = selected_dim
            
    if temp_priority_order != st.session_state.priority_order:
        st.session_state.priority_order = temp_priority_order
        st.rerun()
    
    st.sidebar.caption(f"å½“å‰ä¼˜å…ˆçº§é¡ºåº: {' > '.join(st.session_state.priority_order)}")

    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    target_ratios = {}
    if 'target_ratios' not in st.session_state:
        st.session_state.target_ratios = {}
    
    token_bin_order = [label for _, _, label in TOKEN_BINS]
    
    # è·å–ç»´åº¦çš„å”¯ä¸€å€¼ï¼ˆåŸºäºé‡‡æ ·æ•°æ®ï¼‰
    st.sidebar.info("æ­£åœ¨åˆ†æç»´åº¦åˆ†å¸ƒ...")
    dimension_values = {}
    for dim in dimensions:
        if dim == 'token_bin':
            dimension_values[dim] = token_bin_order
        else:
            all_values = set()
            sample_files = df_paths[:min(5, len(df_paths))]  # é‡‡æ ·å‰5ä¸ªæ–‡ä»¶
            for file_path in sample_files:
                try:
                    df_sample = dd.read_json(file_path, lines=True)
                    if dim in df_sample.columns:
                        unique_vals = df_sample[dim].drop_duplicates().compute()
                        all_values.update(unique_vals.tolist())
                except Exception as e:
                    st.sidebar.warning(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
            dimension_values[dim] = sorted(list(all_values))
    
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        values = dimension_values[dim]
        
        if dim not in st.session_state.target_ratios:
            st.session_state.target_ratios[dim] = {}
        target_ratios[dim] = {}
        total_ratio = 0.0
        
        items_per_row = 3
        for i_start in range(0, len(values), items_per_row):
            cols = st.sidebar.columns(items_per_row)
            for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                with cols[i_offset]:
                    ratio = st.number_input(
                        label=f"{val}",
                        min_value=0.0,
                        max_value=1.0,
                        value=0.0,
                        step=0.01,
                        format="%.3f",
                        key=f"{dim}_{val}"
                    )
                    st.session_state.target_ratios[dim][val] = ratio
                    target_ratios[dim][val] = ratio
                    total_ratio += ratio
        
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
            
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            target_ratios = st.session_state.target_ratios
            priority_order = st.session_state.priority_order
            
            # è¿è¡Œæ”¹è¿›çš„IPFæ±‚è§£å™¨
            weights_dict, actual_dist, converged = advanced_ipf_solver_chunked(
                df_paths, 
                target_ratios, 
                target_total,
                priority_order,
                max_iter=50,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥æé«˜é€Ÿåº¦
                tol=0.01      # æ”¾å®½è¯¯å·®å®¹å¿åº¦
            )
            
            if weights_dict is not None:
                st.session_state.weights_dict = weights_dict
                st.session_state.actual_dist = actual_dist
                st.session_state.converged = converged
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                if converged:
                    st.sidebar.success("âœ… æ‰€æœ‰ç»´åº¦é…æ¯”å‡å·²æ»¡è¶³ï¼")
                else:
                    st.sidebar.warning("âš ï¸ éƒ¨åˆ†ç»´åº¦é…æ¯”æœªå®Œå…¨æ»¡è¶³ï¼Œè¯·æ£€æŸ¥è¯¯å·®æŠ¥å‘Š")
                    
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'weights_dict' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                sampled_tokens = sample_dataset_streaming(
                    df_paths, 
                    st.session_state.weights_dict, 
                    target_total, 
                    output_path, 
                    shard_size
                )
                st.sidebar.success(f"å¯¼å‡ºå®Œæˆï¼å®é™…é‡‡æ ·: {sampled_tokens/1e9:.2f}B tokens")

    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    
    # è°ƒç”¨å…¨å±€åˆ†å¸ƒè®¡ç®—
    with st.spinner("ğŸ” æ­£åœ¨åˆ†æå…¨å±€åˆ†å¸ƒ..."):
        global_dist, actual_total_tokens = compute_global_distribution(df_paths, dimensions)

    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        try:
            source_dist = global_dist['source']
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        try:
            category_dist = global_dist['category']
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        try:
            domain_dist = global_dist['domain']
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        try:
            lang_dist = global_dist['language']
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        try:
            token_dist = global_dist['token_bin']
            ordered_labels = [label for _, _, label in TOKEN_BINS]
            dist_values = [token_dist.get(label, 0) for label in ordered_labels]

            fig, ax = plt.subplots(figsize=(8, 4))
            ax.bar(ordered_labels, dist_values)
            ax.set_ylabel('Ratio')
            ax.set_title('Token length distribution')
            for i, v in enumerate(dist_values):
                ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # 6. å­ç±»ç»„åˆåˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
        try:
            subclass_data = []
            for file_path in df_paths[:min(3, len(df_paths))]:  # æ§åˆ¶é‡‡æ ·æ•°é‡
                df_sample = dd.read_json(file_path, lines=True)
                df_sample['subclass'] = df_sample['source'] + "+" + df_sample['category'] + "+" + df_sample['domain'] + "+" + df_sample['language']
                subclass_data.append(df_sample)
            if subclass_data:
                combined_df = dd.concat(subclass_data)
                total_tokens = combined_df['token_count'].sum().compute()
                subclass_dist = combined_df.groupby('subclass')['token_count'].sum().compute() / total_tokens
                top10 = subclass_dist.nlargest(10)
                
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.barh(top10.index, top10.values)
                ax.set_xlabel('æ¯”ä¾‹')
                ax.set_title('Top 10 distribution of subclass combinations')
                for i, v in enumerate(top10.values):
                    ax.text(v + 0.005, i, f'{v:.1%}', va='center')
                plt.tight_layout()
                st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ–‡ä»¶æ•°é‡**: {len(df_paths)}")
    st.write(f"**å®é™…æ€»Tokenæ•°**: {actual_total_tokens / 1e9:.2f} B (10äº¿)")
    
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png", width=300)

# å†…å­˜ç›‘æ§
if st.sidebar.checkbox("ğŸ” æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ"):
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    st.sidebar.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.2f} MB")
    if client:
        st.sidebar.info(f"Daské›†ç¾¤çŠ¶æ€: {client.dashboard_link}")

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls
from concurrent.futures import ThreadPoolExecutor, as_completed
import math
import dask.dataframe as dd
from dask import delayed
import gc
import logging
from threading import Lock
import pickle

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·ï¼ˆæ”¯æŒTBçº§æ•°æ®ï¼‰")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4096, "0-4k"),
    (4096, 8192, "4k-8k"),
    (8192, 16384, "8k-16k"),
    (16384, 32768, "16k-32k"),
    (32768, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# æ–‡ä»¶å†™å…¥é”
file_lock = Lock()

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low <= token_count < high:
            return label
    return ">32k"

def calculate_distribution_chunked(df_chunk, column, weights_chunk=None):
    """è®¡ç®—åˆ†å—åŠ æƒåˆ†å¸ƒ"""
    if weights_chunk is None:
        weights_chunk = df_chunk['token_count']
    chunk_total = weights_chunk.sum()
    if chunk_total == 0:
        return pd.Series(dtype=float)
    chunk_dist = df_chunk.groupby(column).apply(lambda x: np.sum(weights_chunk[x.index]) / chunk_total)
    return chunk_dist.sort_values(ascending=False)

@st.cache_resource
def load_file_partitions(df_paths):
    """é¢„åŠ è½½æ‰€æœ‰æ–‡ä»¶çš„åˆ†åŒºä¿¡æ¯ï¼Œé¿å…é‡å¤è¯»å–"""
    file_partitions = {}
    for file_path in df_paths:
        try:
            df_chunk = dd.read_json(file_path, lines=True)
            partitions = df_chunk.to_delayed()
            file_partitions[file_path] = partitions
            logger.info(f"é¢„åŠ è½½æ–‡ä»¶ {file_path}ï¼Œåˆ†åŒºæ•°: {len(partitions)}")
        except Exception as e:
            logger.error(f"é¢„åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            file_partitions[file_path] = []
    return file_partitions

@st.cache_resource
def calculate_exact_distribution(df_paths, column):
    """ç²¾ç¡®è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æ•´ä½“åˆ†å¸ƒï¼ˆä½¿ç”¨Daskï¼‰"""
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # åˆ†æ‰¹è¯»å–æ–‡ä»¶ä»¥é¿å…å†…å­˜é—®é¢˜
        batch_size = 10
        all_dfs = []
        total_files = len(df_paths)
        
        for i in range(0, total_files, batch_size):
            batch_paths = df_paths[i:i + batch_size]
            batch_dfs = []
            
            for j, f in enumerate(batch_paths):
                df_sample = dd.read_json(f, lines=True)
                if column == 'token_bin':
                    df_sample = df_sample.assign(token_bin=df_sample['token_count'].apply(get_token_bin, meta=('token_bin', 'object')))
                batch_dfs.append(df_sample)
                progress = (i + j + 1) / total_files
                progress_bar.progress(progress)
                progress_text.text(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {i + j + 1}/{total_files}")
            
            batch_combined = dd.concat(batch_dfs)
            all_dfs.append(batch_combined)
        
        progress_text.text("æ­£åœ¨åˆå¹¶æ•°æ®...")
        combined_df = dd.concat(all_dfs)
        
        progress_text.text("æ­£åœ¨è®¡ç®—åˆ†å¸ƒ...")
        total_tokens = combined_df['token_count'].sum().compute()
        
        # å¤„ç†ç©ºåˆ†ç»„æƒ…å†µ
        try:
            token_sum = combined_df.groupby(column)['token_count'].sum().compute()
            if total_tokens > 0 and len(token_sum) > 0:
                dist = token_sum / total_tokens
            else:
                dist = pd.Series(dtype=float)
        except Exception as e:
            logger.warning(f"è®¡ç®—åˆ†å¸ƒæ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
            dist = pd.Series(dtype=float)
        
        progress_bar.empty()
        progress_text.empty()
        return dist.sort_values(ascending=False)
        
    except Exception as e:
        progress_bar.empty()
        progress_text.empty()
        logger.error(f"è®¡ç®—åˆ†å¸ƒå¤±è´¥: {str(e)}")
        raise e

def advanced_ipf_solver_chunked(df_paths, target_ratios, target_total, priority_order, max_iter=50, tol=0.01):
    """
    æ”¹è¿›çš„IPFæ±‚è§£å™¨ - æ”¯æŒå¤§æ–‡ä»¶åˆ†å—å¤„ç†
    :param df_paths: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param priority_order: ä¼˜å…ˆçº§é¡ºåºåˆ—è¡¨
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(1%)
    :return: é‡‡æ ·æƒé‡å­—å…¸, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    
    # é¢„åŠ è½½æ–‡ä»¶åˆ†åŒºä¿¡æ¯
    file_partitions = load_file_partitions(df_paths)
    
    # åˆå§‹åŒ–ï¼šè®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text("æ­£åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯...")
    
    global_stats = {}
    total_tokens = 0
    
    # åˆ†å—è®¡ç®—æ€»tokenæ•°å’Œå„ç»´åº¦åŸå§‹æ¯”ä¾‹
    for dim in target_ratios.keys():
        global_stats[dim] = {}
    
    total_files = len(df_paths)
    for idx, file_path in enumerate(df_paths):
        try:
            partitions = file_partitions[file_path]
            file_total_tokens = 0
            file_dim_stats = {dim: {} for dim in target_ratios.keys()}
            
            for partition in partitions:
                partition_df = partition.compute()
                partition_tokens = partition_df['token_count'].sum()
                file_total_tokens += partition_tokens
                
                for dim in target_ratios.keys():
                    if dim in partition_df.columns:
                        dim_stats = partition_df.groupby(dim)['token_count'].sum()
                        for cat, count in dim_stats.items():
                            if cat not in file_dim_stats[dim]:
                                file_dim_stats[dim][cat] = 0
                            file_dim_stats[dim][cat] += count
            
            total_tokens += file_total_tokens
            
            # åˆå¹¶åˆ°å…¨å±€ç»Ÿè®¡
            for dim, dim_stats in file_dim_stats.items():
                for cat, count in dim_stats.items():
                    if cat not in global_stats[dim]:
                        global_stats[dim][cat] = 0
                    global_stats[dim][cat] += count
            
            progress = (idx + 1) / total_files
            progress_bar.progress(progress)
            progress_text.text(f"æ­£åœ¨åˆ†ææ–‡ä»¶: {idx + 1}/{total_files}")
            
        except Exception as e:
            st.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {str(e)}")
            logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {str(e)}")
            continue
    
    progress_bar.empty()
    progress_text.empty()
    
    # è®¡ç®—åŸå§‹æ¯”ä¾‹
    orig_ratios = {}
    for dim, cats in global_stats.items():
        orig_ratios[dim] = {cat: count/total_tokens if total_tokens > 0 else 0 for cat, count in cats.items()}
    
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        for cat, ratio in targets.items():
            if cat not in orig_ratios.get(dim, {}):
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            orig_ratio = orig_ratios[dim][cat]
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})å¯èƒ½è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})")
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False

    if set(priority_order) != set(target_ratios.keys()):
        st.error(f"é”™è¯¯ï¼šä¼˜å…ˆçº§é¡ºåºå¿…é¡»åŒ…å«æ‰€æœ‰ç»´åº¦ä¸”ä¸èƒ½é‡å¤")
        return None, None, False

    # åˆå§‹åŒ–æƒé‡å­—å…¸ï¼ˆæŒ‰æ–‡ä»¶å’Œåˆ†åŒºå­˜å‚¨ï¼‰
    progress_text.text("æ­£åœ¨åˆå§‹åŒ–æƒé‡...")
    weights_dict = {}  # {file_path: {partition_idx: weight_array}}
    
    # ç¬¬ä¸€æ¬¡éå†ï¼šåˆå§‹åŒ–æƒé‡
    initial_scale_guess = target_total / total_tokens if total_tokens > 0 else 1.0
    for file_path in df_paths:
        partitions = file_partitions[file_path]
        weights_dict[file_path] = {}
        for i, partition in enumerate(partitions):
            partition_df = partition.compute()
            weights_dict[file_path][i] = np.full(len(partition_df), initial_scale_guess)

    # å¼€å§‹IPFè¿­ä»£
    all_dims = list(target_ratios.keys())
    
    for iter in range(max_iter):
        progress_text.text(f"æ­£åœ¨è¿›è¡Œç¬¬ {iter+1} è½®è¿­ä»£...")
        max_errors = {}
        
        # è®¡ç®—å½“å‰æ€»tokenæ•°ï¼ˆæ‰¹é‡è®¡ç®—ï¼‰
        current_total = 0
        current_totals = {}  # ç¼“å­˜æ¯ä¸ªæ–‡ä»¶çš„å½“å‰æ€»tokenæ•°
        
        for file_path in df_paths:
            file_current_total = 0
            for partition_idx, partition_weights in weights_dict[file_path].items():
                partition_df = file_partitions[file_path][partition_idx].compute()
                file_current_total += np.sum(partition_weights * partition_df['token_count'])
            current_totals[file_path] = file_current_total
            current_total += file_current_total
        
        # è®¡ç®—æ€»é‡è¯¯å·®å› å­
        total_factor = (target_total / current_total) if current_total > 1e-5 else 1.0
        total_factor = max(0.8, min(1.2, total_factor))

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºè¿­ä»£è°ƒæ•´ç»´åº¦
        for dim in priority_order:
            targets = target_ratios[dim]
            dim_max_error = 0
            
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰è¯¥ç±»åˆ«çš„å®é™…æ¯”ä¾‹ï¼ˆæ‰¹é‡è®¡ç®—ï¼‰
                current_cat_tokens = 0
                for file_path in df_paths:
                    partitions = file_partitions[file_path]
                    for partition_idx, partition_weights in weights_dict[file_path].items():
                        partition_df = partitions[partition_idx].compute()
                        if dim in partition_df.columns:
                            mask = (partition_df[dim] == cat)
                            current_cat_tokens += np.sum(partition_weights[mask] * partition_df.loc[mask, 'token_count'])
                
                current_ratio = current_cat_tokens / current_total if current_total > 1e-5 else 0.0
                
                # è®¡ç®—æ¯”ä¾‹è°ƒæ•´å› å­
                if current_ratio > 1e-5 and target_ratio > 0:
                    ratio_factor = target_ratio / current_ratio
                    ratio_factor = max(0.7, min(1.4, ratio_factor))
                    combined_factor = ratio_factor * total_factor
                    
                    # æ›´æ–°æƒé‡ï¼ˆä¿®å¤ï¼šç›´æ¥æ›´æ–°weights_dictï¼‰
                    for file_path in df_paths:
                        partitions = file_partitions[file_path]
                        for partition_idx, partition_weights in weights_dict[file_path].items():
                            partition_df = partitions[partition_idx].compute()
                            if dim in partition_df.columns:
                                mask = (partition_df[dim] == cat)
                                # ä¿®å¤ï¼šç›´æ¥æ›´æ–°åŸå§‹æ•°ç»„
                                weights_dict[file_path][partition_idx][mask] *= combined_factor
                
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                dim_max_error = max(dim_max_error, error)
            
            max_errors[dim] = dim_max_error

        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•›
        if all(error < tol for error in max_errors.values()):
            st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
            break

    # è®¡ç®—æœ€ç»ˆçš„å®é™…åˆ†å¸ƒ
    progress_text.text("æ­£åœ¨è®¡ç®—æœ€ç»ˆåˆ†å¸ƒ...")
    actual_dist = {}
    final_errors = {}
    
    # æœ€ç»ˆæ€»é‡æ ¡å‡†
    current_total = 0
    for file_path in df_paths:
        for partition_idx, partition_weights in weights_dict[file_path].items():
            partition_df = file_partitions[file_path][partition_idx].compute()
            current_total += np.sum(partition_weights * partition_df['token_count'])
    
    if current_total > 0:
        final_scale_factor = target_total / current_total
        for file_path in df_paths:
            for partition_idx in weights_dict[file_path].keys():
                weights_dict[file_path][partition_idx] *= final_scale_factor
        current_total = target_total

    # è®¡ç®—å„ç»´åº¦å®é™…åˆ†å¸ƒ
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        dim_max_error = 0
        
        for cat in target_ratios[dim].keys():
            actual_cat_tokens = 0
            for file_path in df_paths:
                partitions = file_partitions[file_path]
                for partition_idx, partition_weights in weights_dict[file_path].items():
                    partition_df = partitions[partition_idx].compute()
                    if dim in partition_df.columns:
                        mask = (partition_df[dim] == cat)
                        actual_cat_tokens += np.sum(partition_weights[mask] * partition_df.loc[mask, 'token_count'])
            
            actual_ratio = actual_cat_tokens / current_total if current_total > 0 else 0
            actual_dist[dim][cat] = actual_ratio
            target_ratio = target_ratios[dim][cat]
            error = abs(actual_ratio - target_ratio)
            dim_max_error = max(dim_max_error, error)
        
        final_errors[dim] = dim_max_error

    # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·®
    st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
    for dim in priority_order:
        error = final_errors[dim]
        if error <= tol:
            st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
            
    is_converged = all(error <= tol for error in final_errors.values())
    progress_text.empty()
    
    # ä¿å­˜æƒé‡å­—å…¸åˆ°session stateï¼Œé¿å…é‡å¤è®¡ç®—
    st.session_state.weights_dict_cache = weights_dict
    st.session_state.file_partitions_cache = file_partitions
    
    return weights_dict, actual_dist, is_converged

def sample_dataset_streaming(df_paths, weights_dict, target_total, output_path, shard_size_gb=1):
    """æµå¼é‡‡æ ·å¹¶å¯¼å‡º"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    
    current_shard = []
    current_size = 0
    shard_idx = 1
    total_sampled_tokens = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_files = len(df_paths)
    
    # è·å–æ–‡ä»¶åˆ†åŒºä¿¡æ¯
    file_partitions = st.session_state.get('file_partitions_cache', load_file_partitions(df_paths))
    
    for file_idx, file_path in enumerate(df_paths):
        partitions = file_partitions[file_path]
        
        for partition_idx, partition in enumerate(partitions):
            partition_df = partition.compute()
            partition_weights = weights_dict[file_path][partition_idx]
            
            # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆä¿®å¤ï¼šä½¿ç”¨clipç¡®ä¿èŒƒå›´æ­£ç¡®ï¼‰
            probs = np.clip(partition_weights, 0.0, 1.0)
            retained = np.random.random(len(partition_df)) < probs
            
            # å¤„ç†ä¿ç•™çš„è¡Œ
            for idx, row in partition_df[retained].iterrows():
                record = {
                    'source': row['source'],
                    'category': row['category'],
                    'domain': row['domain'],
                    'language': row['language'],
                    'token_count': row['token_count'],
                    'text': row['text']
                }
                
                line = json.dumps(record, ensure_ascii=False) + '\n'
                line_bytes = len(line.encode('utf-8'))
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°åˆ†ç‰‡
                if current_size + line_bytes > shard_size_bytes and current_shard:
                    # å†™å…¥å½“å‰åˆ†ç‰‡ï¼ˆä½¿ç”¨é”é¿å…å¹¶å‘å†²çªï¼‰
                    shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
                    with file_lock:
                        with open(shard_path, 'w', encoding='utf-8') as f:
                            f.writelines(current_shard)
                    
                    current_shard = []
                    current_size = 0
                    shard_idx += 1
                
                current_shard.append(line)
                current_size += line_bytes
                total_sampled_tokens += row['token_count']
            
            # æ›´æ–°è¿›åº¦
            progress = min((file_idx + (partition_idx + 1) / len(partitions)) / total_files, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"é‡‡æ ·è¿›åº¦: {file_idx+1}/{total_files} æ–‡ä»¶ | åˆ†ç‰‡: {shard_idx} | å·²é‡‡æ ·: {total_sampled_tokens/1e9:.2f}B tokens")
    
    # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
    if current_shard:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        with file_lock:
            with open(shard_path, 'w', encoding='utf-8') as f:
                f.writelines(current_shard)
    
    progress_bar.empty()
    status_text.empty()
    
    return total_sampled_tokens

def write_shard_batch(rows, shard_path):
    """æ‰¹é‡å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
    try:
        with file_lock:  # æ·»åŠ æ–‡ä»¶é”
            with open(shard_path, 'w', encoding='utf-8') as f:
                for row in rows:
                    f.write(json.dumps({
                        'source': row['source'],
                        'category': row['category'],
                        'domain': row['domain'],
                        'language': row['language'],
                        'token_count': row['token_count'],
                        'text': row['text']
                    }, ensure_ascii=False) + '\n')
        return True, shard_path
    except Exception as e:
        return False, f"Error writing {shard_path}: {str(e)}"

def parse_jsonl_file_pandas(file_path, chunksize=50000):
    """ä½¿ç”¨pandasé«˜æ•ˆè§£æJSONLæ–‡ä»¶ï¼ˆæ”¯æŒåˆ†å—è¯»å–ï¼‰"""
    records = []
    try:
        chunk_iter = pd.read_json(file_path, lines=True, chunksize=chunksize)
        if not hasattr(chunk_iter, '__iter__'):
            chunk_iter = [chunk_iter]
        for chunk in chunk_iter:
            required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
            if all(col in chunk.columns for col in required_fields):
                chunk = chunk[required_fields]
                chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                chunk.dropna(subset=['token_count'], inplace=True)
                chunk['token_count'] = chunk['token_count'].astype(int)
                string_fields = ['source', 'category', 'domain', 'language', 'text']
                for field in string_fields:
                    chunk[field] = chunk[field].astype(str)
                records.extend(chunk.to_dict(orient='records'))
            else:
                print(f"Missing required fields in {file_path}")
                continue
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
    return records

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        try:
            files = os.listdir(data_path)
            jsonl_files = [f for f in files if f.lower().endswith('.jsonl')]
            st.sidebar.info(f"åŒ…å« {len(files)} ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­ {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        except Exception as e:
            st.sidebar.warning(f"æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {str(e)}")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        data_path = os.path.normpath(data_path)
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                jsonl_files = []
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            jsonl_files.append(os.path.join(root, file))
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
                    st.sidebar.caption("- è·¯å¾„æ˜¯å¦æ­£ç¡®")
                    st.sidebar.caption("- æ–‡ä»¶åç¼€æ˜¯å¦ä¸º.jsonlï¼ˆé.JSONLï¼‰")
                    st.sidebar.caption("- æ˜¯å¦æœ‰æ–‡ä»¶è®¿é—®æƒé™")
                    st.stop()
                
                # å­˜å‚¨æ–‡ä»¶è·¯å¾„åˆ°session state
                st.session_state.df_paths = jsonl_files
                st.session_state.data_path = data_path
                
                # è®¡ç®—æ€»æ ·æœ¬æ•°å’Œtokenæ•°ï¼ˆç²¾ç¡®è®¡ç®—ï¼‰
                sample_file = jsonl_files[0]
                try:
                    with open(sample_file, 'r', encoding='utf-8') as f:
                        sample_lines = [next(f).strip() for _ in range(3)]
                    st.sidebar.caption(f"ğŸ“„ é¢„è§ˆ {os.path.basename(sample_file)}:")
                    for line in sample_lines:
                        st.sidebar.caption(f"`{line[:100]}...`")
                except Exception as e:
                    st.sidebar.warning(f"âš ï¸ æ— æ³•è¯»å–ç¤ºä¾‹æ–‡ä»¶: {str(e)}")
                
                # ç²¾ç¡®è®¡ç®—æ€»æ•°æ®é‡
                st.sidebar.info("æ­£åœ¨è®¡ç®—æ•°æ®æ€»é‡...")
                total_samples = 0
                total_tokens = 0
                
                # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè®¡ç®—è¿›åº¦
                progress_text = st.sidebar.empty()
                progress_bar = st.sidebar.progress(0)
                total_files = len(jsonl_files)
                
                for i, file_path in enumerate(jsonl_files):
                    try:
                        df_file = dd.read_json(file_path, lines=True)
                        file_samples = len(df_file)
                        file_tokens = df_file['token_count'].sum().compute()
                        total_samples += file_samples
                        total_tokens += file_tokens
                        
                        progress = (i + 1) / total_files
                        progress_bar.progress(progress)
                        progress_text.text(f"æ­£åœ¨è®¡ç®—: {i + 1}/{total_files} æ–‡ä»¶ | å½“å‰æ€»è®¡: {total_tokens/1e9:.2f}B tokens")
                        
                    except Exception as e:
                        st.sidebar.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
                        logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
                
                progress_bar.empty()
                progress_text.empty()
                
                if total_samples > 0:
                    st.session_state.total_tokens = total_tokens
                    st.session_state.total_samples = total_samples
                    st.sidebar.success(f"ğŸ‰ æ•°æ®åŠ è½½å®Œæˆï¼")
                    st.sidebar.info(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
                    st.sidebar.info(f"æ€»Tokenæ•°: {total_tokens/1e9:.2f}B tokens")
                else:
                    st.sidebar.error("âŒ æ— æ³•è®¡ç®—æ•°æ®é‡")
                    st.stop()
                    
            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                logger.exception(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df_paths' in st.session_state:
    df_paths = st.session_state.df_paths
    total_tokens = st.session_state.get('total_tokens', 0)
    
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    
    # å®šä¹‰ç»´åº¦
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    
    # ä¼˜å…ˆçº§æ’åºè¾“å…¥
    st.sidebar.subheader("ğŸ“Œ ä¼˜å…ˆçº§æ’åº")
    st.sidebar.info("è¯·æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½é€‰æ‹©ç»´åº¦")
    
    if 'priority_order' not in st.session_state:
        st.session_state.priority_order = dimensions.copy()
        
    priority_placeholders = {}
    temp_priority_order = st.session_state.priority_order.copy()
    
    for i in range(len(dimensions)):
        with st.sidebar.container():
            cols = st.sidebar.columns([1, 4])
            cols[0].markdown(f"**{i+1}.**")
            available_dims = [dim for dim in dimensions if dim not in temp_priority_order[:i]]
            selected_dim = cols[1].selectbox(
                f"é€‰æ‹©ç¬¬ {i+1} ä¼˜å…ˆçº§ç»´åº¦",
                options=available_dims,
                index=available_dims.index(temp_priority_order[i]) if temp_priority_order[i] in available_dims else 0,
                key=f"priority_{i}"
            )
            temp_priority_order[i] = selected_dim
            
    if temp_priority_order != st.session_state.priority_order:
        st.session_state.priority_order = temp_priority_order
        st.rerun()
    
    st.sidebar.caption(f"å½“å‰ä¼˜å…ˆçº§é¡ºåº: {' > '.join(st.session_state.priority_order)}")

    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    target_ratios = {}
    if 'target_ratios' not in st.session_state:
        st.session_state.target_ratios = {}
    
    token_bin_order = [label for _, _, label in TOKEN_BINS]
    
    # è·å–ç»´åº¦çš„å”¯ä¸€å€¼ï¼ˆåŸºäºæ‰€æœ‰æ•°æ®ï¼‰
    st.sidebar.info("æ­£åœ¨åˆ†æç»´åº¦åˆ†å¸ƒ...")
    dimension_values = {}
    progress_text = st.sidebar.empty()
    progress_bar = st.sidebar.progress(0)
    
    for idx, dim in enumerate(dimensions):
        if dim == 'token_bin':
            dimension_values[dim] = token_bin_order
        else:
            try:
                # ä½¿ç”¨æ‰€æœ‰æ–‡ä»¶è®¡ç®—å”¯ä¸€å€¼
                all_dfs = [dd.read_json(f, lines=True) for f in df_paths]
                combined_df = dd.concat(all_dfs)
                unique_vals = combined_df[dim].drop_duplicates().compute()
                dimension_values[dim] = sorted(unique_vals.tolist())
                progress_text.text(f"åˆ†æç»´åº¦: {dim}")
                progress_bar.progress((idx + 1) / len(dimensions))
            except Exception as e:
                st.sidebar.warning(f"è¯»å–ç»´åº¦ {dim} æ—¶å‡ºé”™: {str(e)}")
                logger.warning(f"è¯»å–ç»´åº¦ {dim} æ—¶å‡ºé”™: {str(e)}")
                dimension_values[dim] = []
    
    progress_bar.empty()
    progress_text.empty()
    
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        values = dimension_values[dim]
        
        if dim not in st.session_state.target_ratios:
            st.session_state.target_ratios[dim] = {}
        target_ratios[dim] = {}
        total_ratio = 0.0
        
        items_per_row = 3
        for i_start in range(0, len(values), items_per_row):
            cols = st.sidebar.columns(items_per_row)
            for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                with cols[i_offset]:
                    ratio = st.number_input(
                        label=f"{val}",
                        min_value=0.0,
                        max_value=1.0,
                        value=0.0,
                        step=0.01,
                        format="%.3f",
                        key=f"{dim}_{val}"
                    )
                    st.session_state.target_ratios[dim][val] = ratio
                    target_ratios[dim][val] = ratio
                    total_ratio += ratio
        
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
            
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            target_ratios = st.session_state.target_ratios
            priority_order = st.session_state.priority_order
            
            # è¿è¡Œæ”¹è¿›çš„IPFæ±‚è§£å™¨
            weights_dict, actual_dist, converged = advanced_ipf_solver_chunked(
                df_paths, 
                target_ratios, 
                target_total,
                priority_order,
                max_iter=50,
                tol=0.01
            )
            
            if weights_dict is not None:
                st.session_state.weights_dict = weights_dict
                st.session_state.actual_dist = actual_dist
                st.session_state.converged = converged
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                if converged:
                    st.sidebar.success("âœ… æ‰€æœ‰ç»´åº¦é…æ¯”å‡å·²æ»¡è¶³ï¼")
                else:
                    st.sidebar.warning("âš ï¸ éƒ¨åˆ†ç»´åº¦é…æ¯”æœªå®Œå…¨æ»¡è¶³ï¼Œè¯·æ£€æŸ¥è¯¯å·®æŠ¥å‘Š")
                    
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'weights_dict' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                sampled_tokens = sample_dataset_streaming(
                    df_paths, 
                    st.session_state.weights_dict, 
                    target_total, 
                    output_path, 
                    shard_size
                )
                st.sidebar.success(f"å¯¼å‡ºå®Œæˆï¼å®é™…é‡‡æ ·: {sampled_tokens/1e9:.2f}B tokens")

    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    
    # åŸºäºæ‰€æœ‰æ–‡ä»¶è¿›è¡Œç²¾ç¡®åˆ†æ
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        try:
            source_dist = calculate_exact_distribution(df_paths, 'source')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"Sourceåˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        try:
            category_dist = calculate_exact_distribution(df_paths, 'category')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"Categoryåˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        try:
            domain_dist = calculate_exact_distribution(df_paths, 'domain')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"Domainåˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        try:
            language_dist = calculate_exact_distribution(df_paths, 'language')
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(language_dist, labels=language_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"Languageåˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        try:
            token_dist = calculate_exact_distribution(df_paths, 'token_bin')
            ordered_labels = [label for _, _, label in TOKEN_BINS]
            dist_values = [token_dist.get(label, 0) for label in ordered_labels]
            
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.bar(ordered_labels, dist_values)
            ax.set_ylabel('Ratio')
            ax.set_title('Token length distribution')
            for i, v in enumerate(dist_values):
                ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"Tokené•¿åº¦åˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
        try:
            # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜é—®é¢˜
            batch_size = 10
            all_subclass_data = []
            total_files = len(df_paths)
            
            progress_text = st.empty()
            progress_bar = st.progress(0)
            
            for i in range(0, total_files, batch_size):
                batch_paths = df_paths[i:i + batch_size]
                batch_dfs = []
                
                for j, f in enumerate(batch_paths):
                    df_sample = dd.read_json(f, lines=True)
                    df_sample['subclass'] = df_sample['source'] + "+" + df_sample['category'] + "+" + df_sample['domain'] + "+" + df_sample['language']
                    batch_dfs.append(df_sample)
                    progress = (i + j + 1) / total_files
                    progress_bar.progress(progress)
                    progress_text.text(f"æ­£åœ¨å¤„ç†å­ç±»ç»„åˆ: {i + j + 1}/{total_files}")
                
                batch_combined = dd.concat(batch_dfs)
                all_subclass_data.append(batch_combined)
            
            progress_text.text("æ­£åœ¨åˆå¹¶å­ç±»æ•°æ®...")
            combined_df = dd.concat(all_subclass_data)
            
            progress_text.text("æ­£åœ¨è®¡ç®—å­ç±»åˆ†å¸ƒ...")
            total_tokens = combined_df['token_count'].sum().compute()
            
            # å¤„ç†ç©ºåˆ†ç»„æƒ…å†µ
            try:
                subclass_token_sum = combined_df.groupby('subclass')['token_count'].sum().compute()
                if total_tokens > 0 and len(subclass_token_sum) > 0:
                    subclass_dist = subclass_token_sum / total_tokens
                    top10 = subclass_dist.nlargest(10)
                else:
                    top10 = pd.Series(dtype=float)
            except Exception as e:
                logger.warning(f"è®¡ç®—å­ç±»åˆ†å¸ƒæ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
                top10 = pd.Series(dtype=float)
            
            if len(top10) > 0:
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.barh(top10.index, top10.values)
                ax.set_xlabel('æ¯”ä¾‹')
                ax.set_title('Top 10 distribution of subclass combinations')
                for i, v in enumerate(top10.values):
                    ax.text(v + 0.005, i, f'{v:.1%}', va='center')
                plt.tight_layout()
                st.pyplot(fig)
            else:
                st.info("æš‚æ— å­ç±»æ•°æ®")
            
            progress_bar.empty()
            progress_text.empty()
            
        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {str(e)}")
            logger.error(f"å­ç±»åˆ†å¸ƒå›¾ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ–‡ä»¶æ•°é‡**: {len(df_paths)}")
    if total_tokens > 0:
        st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png", width=300)

import os
import json
import asyncio
import random
import argparse
import time
import threading
import re  # âœ… è¡¥å……å¯¼å…¥
from tqdm import tqdm
from openai import AsyncOpenAI
from transformers import AutoTokenizer
from typing import List, Dict, Optional, Tuple  # âœ… è¡¥å……å¯¼å…¥
from load_prompt import PromptsLoader
import json_repair


"""
å¤§æ¨¡å‹è’¸é¦æ•°æ®ç”Ÿæˆç³»ç»Ÿï¼ˆå¼‚æ­¥ + é‡è¯• + å‚æ•°åŒ–ï¼‰
åŠŸèƒ½è¯´æ˜ï¼š
1. ä»åŸå§‹æ–‡æœ¬ç”Ÿæˆå¤šè½®å¯¹è¯æ•°æ®ï¼ˆé—®é¢˜-æ€è€ƒ-å›ç­”ï¼‰
2. æ”¯æŒä¸‰ä¸ªç‹¬ç«‹æ¨¡å‹ï¼šæé—®æ¨¡å‹(questioner)ã€å›ç­”æ¨¡å‹(answerer)ã€è¿‡æ»¤æ¨¡å‹(filter)
3. æ¯ä¸ªæ¨¡å‹ä½¿ç”¨ç‹¬ç«‹çš„APIé…ç½®æ± ï¼ˆapi_path, model_name, api_keyï¼‰ï¼Œé¿å…æ··ç”¨
4. æ™ºèƒ½æ§åˆ¶tokené•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶
5. æ”¯æŒå›ºå®šå¹¶å‘æ•°ï¼Œä»»åŠ¡å®Œæˆå³è¡¥ä½ï¼ˆæ— é•¿å°¾ç©ºæ³¡ï¼‰
6. è‡ªåŠ¨é‡è¯•å¤±è´¥è¯·æ±‚ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
7. æ‰€æœ‰å‚æ•°é€šè¿‡ argparse é…ç½®

è®¾è®¡åŸåˆ™ï¼š
- æ¨¡å—åŒ–ï¼šå°†ä¸åŒåŠŸèƒ½æ‹†åˆ†ä¸ºç‹¬ç«‹å‡½æ•°
- å¯é…ç½®ï¼šå…³é”®å‚æ•°é›†ä¸­ç®¡ç†
- å¥å£®æ€§ï¼šå¼‚å¸¸å¤„ç†ã€é‡è¯•ã€è¿›åº¦è·Ÿè¸ª
- å¯æ‰©å±•ï¼šä¾¿äºæ·»åŠ æ–°æ¨¡å‹æˆ–åŠŸèƒ½
"""

## ================= é…ç½®é¡¹ï¼ˆéƒ¨åˆ†ç§»åˆ° argparseï¼‰ ================= ##
QUES_NUM_PER = 4096  # æ¯QUES_NUM_PERä¸ªtokenæå‡ºä¸€ä¸ªé—®é¢˜
limit_tokens = 500    # è·ç¦»32ké•¿åº¦è¿˜æœ‰limit_tokensæ—¶åœæ­¢ç”Ÿæˆ

## ================= å·¥å…·å‡½æ•° ================= ##
def append_num_to_filename(file_path: str, num: int) -> str:
    """åœ¨æ–‡ä»¶ååæ·»åŠ åºå·ï¼Œç”¨äºåˆ†ç‰‡å­˜å‚¨"""
    base_path, file_name = os.path.split(file_path)
    name, ext = os.path.splitext(file_name)
    return os.path.join(base_path, f"{name}_{num}{ext}")

def get_uuid_jsonl(file_path: str) -> List[str]:
    """ä»è¾“å‡ºç›®å½•è·å–å·²å¤„ç†çš„UUIDåˆ—è¡¨"""
    uuid_list = []
    for root, _, files in os.walk(file_path):
        for file in files:
            if file.endswith(".jsonl"):
                try:
                    uuid = file.split("_")[-1].rsplit(".", 1)[0]
                    uuid_list.append(uuid)
                except (IndexError, ValueError):
                    continue
    return uuid_list

def read_jsonl(file_path: str, output_dir: str) -> List[Dict]:
    """è¯»å–JSONLæ–‡ä»¶ï¼Œè·³è¿‡å·²å¤„ç†çš„æ•°æ®"""
    data_list = []
    processed_uuids = set(get_uuid_jsonl(output_dir))
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if data.get("id", "") not in processed_uuids:
                    data_list.append(data)
            except json.JSONDecodeError as e:
                print(f"è§£æJSONé”™è¯¯: {e} - è·³è¿‡è¯¥è¡Œ")
    return data_list

def write_single_jsonl(file_path, data, num, mode="a"):
    new_file_path = append_num_to_filename(file_path, num)
    os.makedirs(os.path.dirname(new_file_path), exist_ok=True)
    with open(new_file_path, mode, encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")
        
def write_messages_to_jsonl(
    file_path: str, 
    messages: List[Dict], 
    shard_num: int, 
    mode: str = "a", 
    task_type: Optional[str] = None
) -> None:
    """å°†å¯¹è¯æ¶ˆæ¯å†™å…¥JSONLæ–‡ä»¶ï¼Œæ”¯æŒåˆ†ç‰‡å­˜å‚¨"""
    new_file_path = append_num_to_filename(file_path, shard_num)
    os.makedirs(os.path.dirname(new_file_path), exist_ok=True)
    
    data = {"messages": messages}
    if task_type:
        data["task"] = task_type
    
    with open(new_file_path, mode, encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")

def cal_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    return len(tokenizer.tokenize(text))

## ================= æ ¸å¿ƒå¤„ç†å‡½æ•° ================= ##
def extract_think_answer(text):
    pattern = r'(.*?)(?:<think>(.*?)</think>)(.*)'
    match = re.search(pattern, text, re.DOTALL)

    if not match:
        if '</think>' in text:
            text_list = text.split('</think>')
            answer_content = "".join(text_list[1:])
            return {"think": text_list[0].replace("<think>", "").strip(), "answer": answer_content.strip()}
        else:
            return {"think": "", "answer": text.replace("<think>", "").strip()}

    before_think = match.group(1) or ""
    think_content = match.group(2) or ""
    after_think = match.group(3) or ""

    answer_content = after_think.strip()

    return {
        "think": think_content.strip(),
        "answer": answer_content
    }

def generate_rounds() -> int:
    possible_rounds = [0, 1, 2, 3, 4, 5, 6]
    weights = [0.8] + [0.2 / 6] * 6
    return random.choices(possible_rounds, weights=weights, k=1)[0]

## ================= å¼‚æ­¥ + é‡è¯•å°è£… ================= ##
async def with_retry(coro_func, max_retries=3, base_delay=1.0):
    """å¼‚æ­¥é‡è¯•è£…é¥°å™¨ï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
    for attempt in range(max_retries + 1):
        try:
            return await coro_func()
        except Exception as e:
            if attempt == max_retries:
                raise e
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            print(f"è¯·æ±‚å¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.2f} ç§’...")
            await asyncio.sleep(delay)

## ================= æ ¸å¿ƒå¼‚æ­¥å¤„ç†å‡½æ•° ================= ##
async def generate_initial_questions_async(
    text: str, 
    language: str, 
    client: AsyncOpenAI,
    model_name: str
) -> Tuple[str, List[str], str, str]:
    text_token = cal_tokens(text)
    ques_num = max(1, int(text_token / QUES_NUM_PER))
    prompt = prompts_loader.get_prompt(language, "generate_1_question").format(
        text=text, 
        ques_num=str(ques_num)
    )

    async def _call():
        completion = await client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            timeout=36000,
        )
        return completion

    completion = await with_retry(_call, max_retries=3)

    result = extract_think_answer(completion.choices[0].message.content)
    questions = [q.strip() for q in result["answer"].strip().split("\n") if q.strip()]

    if not questions:
        raise ValueError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„é—®é¢˜åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæˆ–æç¤ºæ¨¡æ¿ã€‚")
    
    return result["answer"], questions, random.choice(questions), prompt

async def generate_initial_answer_async(
    text: str, 
    question: str, 
    language: str, 
    client: AsyncOpenAI,
    model_name: str
) -> Tuple[str, str, List[Dict]]:
    prompt = prompts_loader.get_prompt(language, "answer_1_question").format(text=text)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": question}
    ]

    async def _call():
        completion = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            timeout=36000,
        )
        return completion

    completion = await with_retry(_call, max_retries=3)

    try:
        if hasattr(completion.choices[0].message, 'reasoning_content') and completion.choices[0].message.reasoning_content:
            result = {
                "think": completion.choices[0].message.reasoning_content,
                "answer": completion.choices[0].message.content
            }
        else:
            result = extract_think_answer(completion.choices[0].message.content)
    except:
        result = extract_think_answer(completion.choices[0].message.content)
    
    return result["answer"].strip(), result["think"].strip(), messages

async def generate_follow_up_answer_async(
    text: str, 
    question: str, 
    language: str, 
    prv_questions: List[str], 
    prv_answers: List[str], 
    client: AsyncOpenAI,
    model_name: str
) -> Tuple[str, str, List[Dict], List[Dict]]:
    prompt = prompts_loader.get_prompt(language, "answer_question").format(text=text)
    messages = [{"role": "system", "content": prompt}]
    
    for q, a in zip(prv_questions, prv_answers):
        messages.append({"role": "user", "content": q})
        messages.append({"role": "assistant", "content": a})
    
    messages.append({"role": "user", "content": question})
    current_messages = [{"role": "user", "content": question}]

    async def _call():
        completion = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            timeout=36000,
        )
        return completion

    completion = await with_retry(_call, max_retries=3)

    result = extract_think_answer(completion.choices[0].message.content)
    return (
        result["answer"].strip(), 
        result["think"].strip(), 
        messages, 
        current_messages
    )

async def generate_follow_up_question_async(
    text: str, 
    prev_round: List[Dict], 
    language: str, 
    client: AsyncOpenAI,
    model_name: str
) -> Tuple[str, str]:
    history = "\n".join([
        f"Q: {r['question']}\nA: {r['answer']}"
        for r in prev_round
    ])
    
    prompt = prompts_loader.get_prompt(
        language, "generate_new_question"
    ).format(text=text, prev_round=history)

    async def _call():
        completion = await client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            timeout=36000,
        )
        return completion

    completion = await with_retry(_call, max_retries=3)

    result = extract_think_answer(completion.choices[0].message.content)
    return result['answer'].strip(), prompt

async def apply_max_filtration_async(
    language: str, 
    think: str, 
    client: AsyncOpenAI,
    model_name: str
) -> Tuple[str, List[Dict]]:
    prompt = prompts_loader.get_prompt(language, "max_user")
    messages = [{
        "role": "user",
        "content": f"{prompt}\n---\n{think}"
    }]

    async def _call():
        completion = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            timeout=36000,
            extra_body={
                "chat_template_kwargs": {"enable_thinking": False},
            }
        )
        return completion

    completion = await with_retry(_call, max_retries=3)

    result = extract_think_answer(completion.choices[0].message.content)
    return result['answer'].strip(), messages

## ================= æ•°æ®å¤„ç†æ ¸å¿ƒï¼ˆå¼‚æ­¥ç‰ˆï¼‰ ================= ##
async def process_data_async(
    global_idx: int,
    data: Dict,
    output_path: str,
    clients: Dict[str, AsyncOpenAI],
    model_names: Dict[str, str],
    progress_lock: threading.Lock,
    total_pbar: Optional[tqdm] = None
) -> None:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªæ•°æ®é¡¹
    """
    try:
        text = data.get("text", "")
        language = data.get("language", "CN").upper()
        shard_num = data.get("id", "0")
        # ===== æ„é€ è¾“å‡ºè·¯å¾„ =====
        multiturn_question_path = os.path.join(args.output_dir, "multiturn_question", "multiturn_question.jsonl")
        multiturn_answer_path = os.path.join(args.output_dir, "multiturn_answer", "multiturn_answer.jsonl")
        multiturn_filter_path = os.path.join(args.output_dir, "multiturn_filter", "multiturn_filter.jsonl")
        final_output_path = os.path.join(args.output_dir, "final_output", "sft_data.jsonl")

        # ===== é˜¶æ®µ1: ç”Ÿæˆåˆå§‹é—®é¢˜ =====
        ori_questions, questions, initial_question, init_question_prompt = await generate_initial_questions_async(
            text, language, clients["questioner"], model_names["questioner"]
        )

        for initial_question in questions: 
            # ===== é˜¶æ®µ2: ç”Ÿæˆåˆå§‹å›ç­” =====
            answer0, think0, answer_messages = await generate_initial_answer_async(
                text, initial_question, language, clients["answerer"], model_names["answerer"]
            )
            
            # ===== é˜¶æ®µ3: ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ =====
            max_think0, filter_messages = await apply_max_filtration_async(
                language, think0, clients["filter"], model_names["filter"]
            )

            conversation = []
            question_history = []
            filter_history = []

            conversation.append({"role": "user", "content": initial_question})
            conversation.append({
                "role": "assistant",
                "think": max_think0,
                "answer": answer0
            })

            question_history.append({"role": "user", "content": init_question_prompt})
            question_history.append({"role": "assistant", "content": ori_questions})

            filter_history.extend(filter_messages)
            filter_history.append({"role": "assistant", "content": max_think0})

            # ä¿å­˜ä¸­é—´ç»“æœ
            write_messages_to_jsonl(
                multiturn_question_path, 
                question_history, 
                shard_num, 
                task_type='max_question'
            )
            write_messages_to_jsonl(
                multiturn_filter_path, 
                filter_history, 
                shard_num, 
                task_type='max_filter'
            )

            total_tokens = (
                cal_tokens(initial_question) +
                cal_tokens(answer0) +
                cal_tokens(max_think0) +
                cal_tokens(text) +
                1000
            )

            # ===== é˜¶æ®µ4: ç”Ÿæˆå¤šè½®å¯¹è¯ =====
            num_rounds = generate_rounds()
            # print(f"æ•°æ® {global_idx} ç”Ÿæˆ {num_rounds} è½®å¯¹è¯")

            if num_rounds > 0:
                for round_num in range(1, num_rounds + 1):
                    prev_round = [
                        {
                            "question": m["content"],
                            "answer": f'<think>\n{a["think"]}\n</think>\n\n{a["answer"]}'
                        }
                        for m, a in zip(
                            [m for m in conversation if m["role"] == "user"],
                            [m for m in conversation if m["role"] == "assistant"]
                        )
                    ]

                    next_question, next_question_prompt = await generate_follow_up_question_async(
                        text, prev_round, language, clients["questioner"], model_names["questioner"]
                    )

                    prv_questions = [r["question"] for r in prev_round]
                    prv_answers = [r["answer"] for r in prev_round]
                    
                    answer, think, all_messages, cur_messages = await generate_follow_up_answer_async(
                        text, next_question, language, prv_questions, prv_answers,
                        clients["answerer"], model_names["answerer"]
                    )
                    
                    max_thinking, max_think_messages = await apply_max_filtration_async(
                        language, think, clients["filter"], model_names["filter"]
                    )

                    round_tokens = (
                        cal_tokens(next_question) +
                        cal_tokens(answer) +
                        cal_tokens(max_thinking)
                    )
                    
                    if total_tokens + round_tokens > 25000 - limit_tokens:
                        print(f"æ•°æ® {global_idx} ç¬¬{round_num}è½®è¶…è¿‡tokené™åˆ¶ï¼Œç»ˆæ­¢ç”Ÿæˆ")
                        break
                    
                    total_tokens += round_tokens

                    conversation.append({"role": "user", "content": next_question})
                    conversation.append({
                        "role": "assistant",
                        "think": max_thinking,
                        "answer": answer
                    })

                    question_history.append({"role": "user", "content": next_question_prompt})
                    question_history.append({"role": "assistant", "content": next_question})

                    filter_history.extend(max_think_messages)
                    filter_history.append({"role": "assistant", "content": max_thinking})

                    write_messages_to_jsonl(
                        multiturn_question_path, 
                        question_history, 
                        shard_num, 
                        task_type='max_question'
                    )
                    write_messages_to_jsonl(
                        multiturn_filter_path, 
                        filter_history, 
                        shard_num, 
                        task_type='max_filter'
                    )

                    print(f"æ•°æ® {global_idx} å®Œæˆç¬¬ {round_num} è½®ï¼Œæ€»tokens: {total_tokens}")

            # ===== é˜¶æ®µ5: ä¿å­˜æœ€ç»ˆç»“æœ =====
            data["messages"] = conversation
            write_messages_to_jsonl(
                multiturn_answer_path, 
                answer_messages + [
                    {"role": "assistant", "content": f'<think>\n{think0}\n</think>\n\n{answer0}'}
                ],
                shard_num,
                task_type='plus_answer'
            )
            write_single_jsonl(
                final_output_path,
                data, shard_num, mode="a"
            )

        # ===== è¿›åº¦æ›´æ–° =====
        with progress_lock:
            if total_pbar:
                total_pbar.update(1)

    except Exception as e:
        print(f"å¤„ç†æ•°æ® {global_idx} æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        # ===== è¿›åº¦æ›´æ–° =====
        with progress_lock:
            if total_pbar:
                total_pbar.update(1)

## ================= Worker åç¨‹ ================= ##
async def worker(
    worker_id: int,
    task_queue: asyncio.Queue,
    questioner_configs: List[Dict],
    answerer_configs: List[Dict],
    filter_configs: List[Dict],
    progress_lock: threading.Lock,
    total_pbar: tqdm
):
    while True:
        try:
            try:
                global_idx, data = task_queue.get_nowait()
            except asyncio.QueueEmpty:
                break

            # ğŸ¯ éšæœºé€‰æ‹©é…ç½®ç»„
            q_config = random.choice(questioner_configs)
            a_config = random.choice(answerer_configs)
            f_config = random.choice(filter_configs)

            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            clients = {
                "questioner": AsyncOpenAI(
                    api_key=q_config["api_key"],
                    base_url=q_config["api_path"]
                ),
                "answerer": AsyncOpenAI(
                    api_key=a_config["api_key"],
                    base_url=a_config["api_path"]
                ),
                "filter": AsyncOpenAI(
                    api_key=f_config["api_key"],
                    base_url=f_config["api_path"]
                ),
            }

            model_names = {
                "questioner": q_config["model_name"],
                "answerer": a_config["model_name"],
                "filter": f_config["model_name"],
            }

            await process_data_async(
                global_idx,
                data,
                os.path.join(args.output_dir, "final_output", "sft_data.jsonl"),
                clients,
                model_names,
                progress_lock,
                total_pbar
            )

            task_queue.task_done()

        except Exception as e:
            print(f"Worker {worker_id} å¤„ç†æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            task_queue.task_done()

## ================= ä¸»è°ƒåº¦å™¨ ================= ##
async def main_task_scheduler(
    data_list: List[Dict],
    args,
    progress_lock: threading.Lock,
    total_pbar: tqdm
):
    # è§£æé…ç½®ç»„
    questioner_configs = parse_model_configs(args.questioner_configs)
    answerer_configs = parse_model_configs(args.answerer_configs)
    filter_configs = parse_model_configs(args.filter_configs)

    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = asyncio.Queue()
    for global_idx, data in enumerate(data_list):
        task_queue.put_nowait((global_idx, data))

    # å¯åŠ¨ Workers
    workers = [
        asyncio.create_task(
            worker(i, task_queue, questioner_configs, answerer_configs, filter_configs, progress_lock, total_pbar)
        )
        for i in range(args.concurrent_workers)
    ]

    await task_queue.join()

    for w in workers:
        w.cancel()
    await asyncio.gather(*workers, return_exceptions=True)

## ================= é…ç½®è§£æå·¥å…· ================= ##
def parse_model_configs(config_str: str) -> List[Dict[str, str]]:
    """è§£ææ¨¡å‹é…ç½®å­—ç¬¦ä¸²ï¼Œè¿”å›é…ç½®å­—å…¸åˆ—è¡¨"""
    configs = []
    groups = config_str.split(';')
    for group in groups:
        group = group.strip()
        if not group:
            continue
        parts = group.split(',')
        if len(parts) != 3:
            raise ValueError(f"é…ç½®æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 'api_path,model_name,api_key'ï¼Œå®é™…: {group}")
        api_path, model_name, api_key = parts
        configs.append({
            "api_path": api_path.strip(),
            "model_name": model_name.strip(),
            "api_key": api_key.strip()
        })
    if not configs:
        raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªé…ç½®ç»„")
    return configs

## ================= å‚æ•°è§£æ ================= ##
def parse_args():
    parser = argparse.ArgumentParser(description="å¤§æ¨¡å‹è’¸é¦æ•°æ®ç”Ÿæˆç³»ç»Ÿ")

    # Questioner æ¨¡å‹é…ç½®ç»„ï¼ˆæ”¯æŒå¤šä¸ªå®ä¾‹ï¼‰
    parser.add_argument("--questioner_configs", type=str, required=True,
                        help='Questioner é…ç½®ç»„ï¼Œæ ¼å¼: "api_path,model_name,api_key;api_path2,model_name2,api_key2"')

    # Answerer æ¨¡å‹é…ç½®ç»„
    parser.add_argument("--answerer_configs", type=str, required=True,
                        help='Answerer é…ç½®ç»„ï¼Œæ ¼å¼åŒä¸Š')

    # Filter æ¨¡å‹é…ç½®ç»„
    parser.add_argument("--filter_configs", type=str, required=True,
                        help='Filter é…ç½®ç»„ï¼Œæ ¼å¼åŒä¸Š')

    # å¹¶å‘é…ç½®
    parser.add_argument("--concurrent_workers", type=int, default=3, help="æ€»å¹¶å‘ Worker æ•°")

    # è¾“å…¥è¾“å‡º
    parser.add_argument("--input_file_path", type=str, required=True, help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•è·¯å¾„")

    return parser.parse_args()

## ================= åˆå§‹åŒ– & ä¸»ç¨‹åº ================= ##
if __name__ == "__main__":
    args = parse_args()

    # ç¡®ä¿è¾“å‡ºç›®å½•ç»“æ„
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, "final_output"), exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, "multiturn_question"), exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, "multiturn_answer"), exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, "multiturn_filter"), exist_ok=True)

    # åŠ è½½åˆ†è¯å™¨
    model_dir = r"/mnt/sizjwb25c1g7/nanhu_lyh/code_tx/think_multi_turn_QA_gen/qwen_model_fold"
    global tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)

    # åŠ è½½æç¤ºè¯
    PROMPT_path = r"./prompts.json"
    with open(PROMPT_path, "r", encoding="utf-8") as f:
        PROMPTS = json.load(f)
        global prompts_loader
        prompts_loader = PromptsLoader(PROMPTS)

    # åŠ è½½æ•°æ®
    start_time = time.time()
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    data_list = read_jsonl(args.input_file_path, args.output_dir)
    total_items = len(data_list)

    if total_items == 0:
        print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†ï¼Œç¨‹åºé€€å‡º")
        exit(0)

    print(f"æ‰¾åˆ° {total_items} æ¡å¾…å¤„ç†æ•°æ®")

    # è¿›åº¦æ¡
    progress_lock = threading.Lock()
    total_pbar = tqdm(
        total=total_items,
        desc="æ€»è¿›åº¦",
        unit="item",
        dynamic_ncols=True,
        position=0
    )

    # å¯åŠ¨ä¸»è°ƒåº¦å™¨
    try:
        asyncio.run(
            main_task_scheduler(data_list, args, progress_lock, total_pbar)
        )
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

    total_pbar.close()

    end_time = time.time()
    execution_time = end_time - start_time
    print(f"\nå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {execution_time:.2f}ç§’")
    print(f"å¹³å‡é€Ÿåº¦: {total_items/execution_time:.2f} æ¡/ç§’")
    print(f"æˆåŠŸå¤„ç† {total_items} æ¡æ•°æ®")

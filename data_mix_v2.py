import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls
import concurrent.futures
from threading import Thread
import queue
import mmap

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4000, "0-4k"),
    (4000, 8000, "4k-8k"),
    (8000, 16000, "8k-16k"),
    (16000, 32000, "16k-32k"),
    (32000, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low < token_count <= high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

def ipf_solver(df, target_ratios, target_total, max_iter=50, tol=0.01):
    """
    IPFè¿­ä»£æ¯”ä¾‹æ‹Ÿåˆæ±‚è§£å™¨
    :param df: æ•°æ®DataFrame
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(1%)
    :return: é‡‡æ ·æƒé‡æ•°ç»„, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    # åˆå§‹åŒ–æƒé‡
    weights = np.ones(len(df))
    total_tokens = df['token_count'].sum()
    
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        dim_total = 0
        for cat, ratio in targets.items():
            # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
            if cat not in df[dim].values:
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            
            # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
            orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²ï¼ˆIPFå¯å¾®è°ƒï¼‰
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®æ»¡è¶³")
        
        # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False
    
    # å¼€å§‹IPFè¿­ä»£
    for iter in range(max_iter):
        prev_weights = weights.copy()
        max_error = 0
        
        # æŒ‰ç»´åº¦è¿­ä»£è°ƒæ•´
        for dim, targets in target_ratios.items():
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / np.sum(weights * df['token_count'])
                
                # è®¡ç®—è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5:
                    factor = target_ratio / current_ratio
                    weights[mask] *= factor
                
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                max_error = max(max_error, error)
        
        # æ£€æŸ¥æ”¶æ•›
        if max_error < tol:
            break
            
        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-4:
            break
    
    # ç¼©æ”¾è‡³ç›®æ ‡æ€»é‡
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        weights *= (target_total / current_total)
    
    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            actual_dist[dim][cat] = np.sum(weights[mask] * df.loc[mask, 'token_count']) / target_total
    
    return weights, actual_dist, (max_error < tol)

def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])
    
    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        remaining['prob'] = (additional * remaining['token_count'] / 
                            remaining['token_count'].sum() / 
                            remaining['token_count'])
        retained[~retained] = np.random.random(len(remaining)) < np.minimum(remaining['prob'], 1.0)
    
    return df[retained].copy()

def export_shards(df, output_path, shard_size_gb=1):
    """åˆ†ç‰‡å¯¼å‡ºJSONLæ–‡ä»¶"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    current_size = 0
    shard_idx = 1
    buffer = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, row in df.iterrows():
        # è®¡ç®—å½“å‰æ ·æœ¬å­—èŠ‚æ•°
        sample_bytes = len(row['text'].encode('utf-8')) + 1  # +1 for newline
        
        # å¦‚æœå½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œå†™å…¥æ–‡ä»¶
        if current_size + sample_bytes > shard_size_bytes and buffer:
            shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
            with open(shard_path, 'w', encoding='utf-8') as f:
                f.write("".join(buffer))
            buffer = []
            current_size = 0
            shard_idx += 1
        
        # æ·»åŠ æ ·æœ¬åˆ°ç¼“å†²åŒº
        buffer.append(json.dumps({
            'source': row['source'],
            'category': row['category'],
            'domain': row['domain'],
            'language': row['language'],
            'token_count': row['token_count'],
            'text': row['text']
        }, ensure_ascii=False) + '\n')
        current_size += sample_bytes
        
        # æ›´æ–°è¿›åº¦
        if idx % 1000 == 0:
            progress = (idx + 1) / len(df)
            progress_bar.progress(min(progress, 1.0))
            status_text.text(f"å¤„ç†æ ·æœ¬ {idx+1}/{len(df)} | å½“å‰åˆ†ç‰‡: {shard_idx}")
    
    # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
    if buffer:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        with open(shard_path, 'w', encoding='utf-8') as f:
            f.write("".join(buffer))
    
    progress_bar.empty()
    status_text.empty()
    st.success(f"å¯¼å‡ºå®Œæˆï¼å…± {shard_idx} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

# ========== ä¼˜åŒ–çš„æ–‡ä»¶åŠ è½½å‡½æ•° ==========
def load_file_batched(file, batch_size=1000):
    """æ‰¹é‡åŠ è½½æ–‡ä»¶ï¼ˆç»“åˆå†…å­˜æ˜ å°„å’Œæ‰¹é‡å¤„ç†ï¼‰"""
    file_data = []
    try:
        file_size = os.path.getsize(file)
        
        # å¯¹äºå¤§æ–‡ä»¶ä½¿ç”¨å†…å­˜æ˜ å°„
        if file_size > 100 * 1024 * 1024:  # >100MB
            with open(file, 'r', encoding='utf-8') as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    batch = []
                    valid_count = 0
                    
                    for line in iter(mm.readline, b""):
                        try:
                            sample = json.loads(line.decode('utf-8', errors='replace'))
                            required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                            if all(k in sample for k in required_fields):
                                try:
                                    token_count = int(float(sample['token_count']))
                                    batch.append({
                                        'source': str(sample['source']),
                                        'category': str(sample['category']),
                                        'domain': str(sample['domain']),
                                        'language': str(sample['language']),
                                        'token_count': token_count,
                                        'text': str(sample['text'])
                                    })
                                    valid_count += 1
                                except (ValueError, TypeError):
                                    pass
                        except json.JSONDecodeError:
                            pass
                        
                        # æ‰¹é‡å¤„ç†
                        if len(batch) >= batch_size:
                            file_data.extend(batch)
                            batch = []
                    
                    # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                    if batch:
                        file_data.extend(batch)
        else:
            # å°æ–‡ä»¶ä½¿ç”¨æ™®é€šè¯»å–
            with open(file, 'r', encoding='utf-8') as f:
                batch = []
                valid_count = 0
                
                for line_num, line in enumerate(f):
                    try:
                        sample = json.loads(line)
                        required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                        if all(k in sample for k in required_fields):
                            try:
                                token_count = int(float(sample['token_count']))
                                batch.append({
                                    'source': str(sample['source']),
                                    'category': str(sample['category']),
                                    'domain': str(sample['domain']),
                                    'language': str(sample['language']),
                                    'token_count': token_count,
                                    'text': str(sample['text'])
                                })
                                valid_count += 1
                            except (ValueError, TypeError):
                                pass
                    except json.JSONDecodeError:
                        pass
                    
                    # æ‰¹é‡å¤„ç†
                    if len(batch) >= batch_size:
                        file_data.extend(batch)
                        batch = []
                
                # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                if batch:
                    file_data.extend(batch)
                    
    except Exception as e:
        pass  # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…å½±å“å…¶ä»–æ–‡ä»¶
    
    return file, file_data, len(file_data)

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        items = os.listdir(data_path)
        st.sidebar.info(f"åŒ…å« {len(items)} ä¸ªé¡¹ç›®")
        # æ˜¾ç¤ºå‰5ä¸ªJSONLæ–‡ä»¶
        jsonl_count = sum(1 for item in items if item.lower().endswith('.jsonl'))
        st.sidebar.info(f"JSONLæ–‡ä»¶: {jsonl_count} ä¸ª")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›† (å¤šçº¿ç¨‹+æ‰¹é‡ä¼˜åŒ–)", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        # å…³é”®ä¿®å¤ï¼šè§„èŒƒåŒ–è·¯å¾„
        data_path = os.path.normpath(data_path)
        
        start_time = time.time()
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                # æ‰«ææ–‡ä»¶
                jsonl_files = []
                total_size = 0
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            full_path = os.path.join(root, file)
                            jsonl_files.append(full_path)
                            total_size += os.path.getsize(full_path)
                
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶ | æ€»å¤§å°: {total_size/(1024**3):.1f}GB")
                
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
                    st.sidebar.caption("- è·¯å¾„æ˜¯å¦æ­£ç¡®")
                    st.sidebar.caption("- æ–‡ä»¶åç¼€æ˜¯å¦ä¸º.jsonl")
                    st.stop()
                
                # æ˜¾ç¤ºæ–‡ä»¶é¢„è§ˆï¼ˆä»…ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
                sample_file = jsonl_files[0]
                try:
                    with open(sample_file, 'r', encoding='utf-8') as f:
                        sample_lines = [next(f).strip() for _ in range(3)]
                    st.sidebar.caption(f"ğŸ“„ é¢„è§ˆ {os.path.basename(sample_file)}:")
                    for line in sample_lines:
                        st.sidebar.caption(f"`{line[:100]}...`")
                except Exception as e:
                    st.sidebar.warning(f"âš ï¸ æ— æ³•è¯»å–ç¤ºä¾‹æ–‡ä»¶: {str(e)}")
                
                # å¤šçº¿ç¨‹å¹¶è¡ŒåŠ è½½ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
                all_data = []
                progress_bar = st.sidebar.progress(0)
                status_text = st.sidebar.empty()
                
                # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°ï¼ˆæ ¹æ®æ–‡ä»¶æ•°é‡ï¼‰
                max_workers = min(32, max(4, len(jsonl_files)))
                st.sidebar.info(f"âš¡ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡ŒåŠ è½½")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_file = {executor.submit(load_file_batched, file): file for file in jsonl_files}
                    
                    # æ”¶é›†ç»“æœ
                    completed = 0
                    total_valid_samples = 0
                    for future in concurrent.futures.as_completed(future_to_file):
                        file, file_data, valid_count = future.result()
                        all_data.extend(file_data)
                        total_valid_samples += valid_count
                        completed += 1
                        status_text.text(f"âœ… å®Œæˆ {completed}/{len(jsonl_files)} æ–‡ä»¶ | æœ‰æ•ˆæ ·æœ¬: {total_valid_samples:,}")
                        progress_bar.progress(completed / len(jsonl_files))
                
                progress_bar.empty()
                status_text.empty()
                
                if all_data:
                    # è½¬ä¸ºDataFrame
                    df = pd.DataFrame(all_data)
                    total_tokens = df['token_count'].sum()
                    
                    # å­˜å‚¨åˆ°session state
                    st.session_state.df = df
                    st.session_state.total_tokens = total_tokens
                    st.session_state.token_bins = [get_token_bin(tc) for tc in df['token_count']]
                    
                    # è®¡ç®—åŠ è½½æ€§èƒ½
                    elapsed = time.time() - start_time
                    speed = len(all_data) / elapsed if elapsed > 0 else 0
                    
                    st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(df):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{total_tokens/1e9:.2f}B tokens")
                    st.sidebar.info(f"â±ï¸ è€—æ—¶: {elapsed:.1f}ç§’ | é€Ÿåº¦: {speed:.0f} æ ·æœ¬/ç§’")
                else:
                    st.sidebar.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                    st.sidebar.info("æœ‰æ•ˆJSONLæ ·æœ¬ç¤ºä¾‹:")
                    st.sidebar.code('''{"source": "CCI4", "category": "book", "domain": "science", "language": "CN", "token_count": 1234, "text": "ç¤ºä¾‹æ–‡æœ¬..."}''')
                    st.stop()
                    
            except Exception as e:
                st.sidebar.error(f"åŠ è½½å¤±è´¥: {str(e)}")
                st.stop()

# ä¿ç•™åŸå§‹ä¸²è¡ŒåŠ è½½ä½œä¸ºå¤‡é€‰
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›† (ä¸²è¡Œ-å…¼å®¹æ¨¡å¼)"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        data_path = os.path.normpath(data_path)
        
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                # æ‰«ææ–‡ä»¶
                jsonl_files = []
                for root, _, files in os.walk(data_path):
                    for file in files:
                        if file.lower().endswith('.jsonl'):
                            jsonl_files.append(os.path.join(root, file))
                
                st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
                
                if not jsonl_files:
                    st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶...")
                    st.stop()
                
                # åŸå§‹ä¸²è¡ŒåŠ è½½é€»è¾‘
                all_data = []
                progress_bar = st.sidebar.progress(0)
                status_text = st.sidebar.empty()
                
                for i, file in enumerate(jsonl_files):
                    try:
                        with open(file, 'r', encoding='utf-8') as f:
                            valid_count = 0
                            for line_num, line in enumerate(f):
                                try:
                                    sample = json.loads(line)
                                    required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                                    if all(k in sample for k in required_fields):
                                        try:
                                            token_count = int(float(sample['token_count']))
                                            all_data.append({
                                                'source': str(sample['source']),
                                                'category': str(sample['category']),
                                                'domain': str(sample['domain']),
                                                'language': str(sample['language']),
                                                'token_count': token_count,
                                                'text': str(sample['text'])
                                            })
                                            valid_count += 1
                                        except (ValueError, TypeError):
                                            st.sidebar.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file)} ç¬¬{line_num}è¡Œ: token_countéæ•°å­—")
                                    else:
                                        missing = [k for k in required_fields if k not in sample]
                                        st.sidebar.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file)} ç¬¬{line_num}è¡Œ: ç¼ºå°‘å­—æ®µ {missing}")
                                except json.JSONDecodeError:
                                    st.sidebar.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file)} ç¬¬{line_num}è¡Œ: JSONè§£æå¤±è´¥")
                        
                        status_text.text(f"âœ… {os.path.basename(file)}: {valid_count} æœ‰æ•ˆæ ·æœ¬")
                    except Exception as e:
                        st.sidebar.error(f"âŒ è·³è¿‡ {os.path.basename(file)}: {str(e)}")
                    
                    progress_bar.progress((i + 1) / len(jsonl_files))
                
                progress_bar.empty()
                status_text.empty()
                
                if all_data:
                    df = pd.DataFrame(all_data)
                    total_tokens = df['token_count'].sum()
                    
                    st.session_state.df = df
                    st.session_state.total_tokens = total_tokens
                    st.session_state.token_bins = [get_token_bin(tc) for tc in df['token_count']]
                    
                    st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(df):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{total_tokens/1e9:.2f}B tokens")
                else:
                    st.sidebar.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®...")
                    st.stop()
                    
            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df' in st.session_state:
    df = st.session_state.df
    total_tokens = st.session_state.total_tokens
    
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    
    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    target_ratios = {}
    
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        
        # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼
        if dim == 'token_bin':
            values = pd.Series(st.session_state.token_bins).unique()
        else:
            values = df[dim].unique()
        
        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        if dim == 'token_bin':
            current_dist = df.groupby(pd.Series(st.session_state.token_bins))['token_count'].sum() / total_tokens
        else:
            current_dist = df.groupby(dim)['token_count'].sum() / total_tokens
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
        target_ratios[dim] = {}
        total_ratio = 0.0
        cols = st.sidebar.columns(len(values))
        
        for i, val in enumerate(values):
            current_ratio = current_dist.get(val, 0.0)
            with cols[i % len(cols)]:
                ratio = st.number_input(
                    f"{val}", 
                    min_value=0.0, 
                    max_value=1.0, 
                    value=float(current_ratio),
                    step=0.01,
                    key=f"{dim}_{val}"
                )
                target_ratios[dim][val] = ratio
                total_ratio += ratio
        
        # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            # è¿è¡ŒIPFæ±‚è§£å™¨
            weights, actual_dist, converged = ipf_solver(
                df, 
                target_ratios, 
                target_total,
                tol=0.01  # 1%è¯¯å·®
            )
            
            if weights is not None:
                # å­˜å‚¨é‡‡æ ·ç»“æœ
                sampled_df = sample_dataset(df, weights, target_total)
                st.session_state.sampled_df = sampled_df
                
                # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                st.sidebar.info(f"å®é™…æ€»é‡: {sampled_df['token_count'].sum()/1e9:.2f}B tokens")
                
                # æ˜¾ç¤ºå…³é”®ç»´åº¦è¯¯å·®
                for dim in ['language', 'domain']:
                    if dim in actual_dist:
                        max_error = 0
                        for cat in actual_dist[dim]:
                            target = target_ratios[dim].get(cat, 0)
                            actual = actual_dist[dim].get(cat, 0)
                            error = abs(target - actual)
                            max_error = max(max_error, error)
                        st.sidebar.caption(f"{dim}: æœ€å¤§è¯¯å·® {max_error:.1%}")
    
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                export_shards(st.session_state.sampled_df, output_path, shard_size)
    
    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    
    # åˆ›å»ºå›¾è¡¨å¸ƒå±€
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        source_dist = calculate_distribution(df, 'source')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        category_dist = calculate_distribution(df, 'category')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        domain_dist = calculate_distribution(df, 'domain')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        lang_dist = calculate_distribution(df, 'language')
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
        ax.axis('equal')
        st.pyplot(fig)
    
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        df['token_bin'] = st.session_state.token_bins
        token_dist = calculate_distribution(df, 'token_bin')
        
        # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨
        for _, _, label in TOKEN_BINS:
            if label not in token_dist:
                token_dist[label] = 0.0
        
        token_dist = token_dist.reindex([label for _, _, label in TOKEN_BINS])
        
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.bar(token_dist.index, token_dist.values)
        ax.set_ylabel('Ratio')
        ax.set_title('Token length distribution')
        for i, v in enumerate(token_dist.values):
            ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
        st.pyplot(fig)
    
    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 50)")
        # åˆ›å»ºå­ç±»ç»„åˆ
        df['subclass'] = df['source'] + "+" + df['category'] + "+" + df['domain'] + "+" + df['language']
        subclass_dist = calculate_distribution(df, 'subclass')
        
        # å–Top 50
        top50 = subclass_dist.head(50)
        
        fig, ax = plt.subplots(figsize=(50, 5))
        ax.barh(top50.index, top50.values)
        ax.set_xlabel('Ratio')
        ax.set_title('Top 50 distribution of subclass combinations')
        
        # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
        for i, v in enumerate(top50.values):
            ax.text(v + 0.005, i, f'{v:.1%}', va='center')
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
    st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")
    
    # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
    if 'sampled_df' in st.session_state:
        st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
        sampled_df = st.session_state.sampled_df
        sampled_tokens = sampled_df['token_count'].sum()
        
        st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
        st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")
        
        # æ¯”è¾ƒå…³é”®ç»´åº¦
        col1, col2, col3 = st.columns(3)
        for i, dim in enumerate(['language', 'domain', 'source']):
            orig_dist = calculate_distribution(df, dim)
            sampled_dist = calculate_distribution(sampled_df, dim)
            
            # è®¡ç®—æœ€å¤§è¯¯å·®
            max_error = 0
            for cat in orig_dist.index:
                orig = orig_dist.get(cat, 0)
                sampled = sampled_dist.get(cat, 0)
                error = abs(orig - sampled)
                max_error = max(max_error, error)
            
            if i == 0:
                col1.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            elif i == 1:
                col2.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            else:
                col3.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    # ç”¨æœ¬åœ°SVGæ›¿ä»£ç½‘ç»œå›¾ç‰‡
    st.markdown("""
    <div style="text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px;">
        <svg xmlns="http://www.w3.org/2000/svg" width="300" height="300" viewBox="0 0 300 300">
            <rect width="100%" height="100%" fill="#ffffff"/>
            <text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" 
                  font-family="Arial" font-size="20px" fill="#000000">
                æ•°æ®é…æ¯”å·¥å…·
            </text>
            <text x="50%" y="65%" dominant-baseline="middle" text-anchor="middle" 
                  font-family="Arial" font-size="14px" fill="#666666">
                è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»"åŠ è½½æ•°æ®é›†"
            </text>
        </svg>
    </div>
    """, unsafe_allow_html=True)

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import mmap
import concurrent.futures
import time
import linecache
import psutil
import hashlib
import re
from tqdm import tqdm
from scipy.optimize import nnls
import logging
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('data_balancer')

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·", page_icon="ğŸ“Š")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4000, "0-4k"),
    (4000, 8000, "4k-8k"),
    (8000, 16000, "8k-16k"),
    (16000, 32000, "16k-32k"),
    (32000, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# å·¥å…·å‡½æ•°
def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low < token_count <= high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    if total == 0:
        return pd.Series()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

def ipf_solver(df, target_ratios, target_total, max_iter=50, tol=0.01):
    """
    IPFè¿­ä»£æ¯”ä¾‹æ‹Ÿåˆæ±‚è§£å™¨
    :param df: æ•°æ®DataFrame
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(1%)
    :return: é‡‡æ ·æƒé‡æ•°ç»„, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    # åˆå§‹åŒ–æƒé‡
    weights = np.ones(len(df))
    total_tokens = df['token_count'].sum()
    
    if total_tokens == 0:
        st.error("é”™è¯¯ï¼šæ•°æ®é›†ä¸­token_countæ€»å’Œä¸º0")
        return None, None, False
    
    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        dim_total = 0
        for cat, ratio in targets.items():
            # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
            if cat not in df[dim].values:
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            
            # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
            orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²ï¼ˆIPFå¯å¾®è°ƒï¼‰
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®æ»¡è¶³")
        
        # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False
    
    # å¼€å§‹IPFè¿­ä»£
    for iter in range(max_iter):
        prev_weights = weights.copy()
        max_error = 0
        
        # æŒ‰ç»´åº¦è¿­ä»£è°ƒæ•´
        for dim, targets in target_ratios.items():
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / np.sum(weights * df['token_count'])
                
                # è®¡ç®—è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5:
                    factor = target_ratio / current_ratio
                    weights[mask] *= factor
                
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                max_error = max(max_error, error)
        
        # æ£€æŸ¥æ”¶æ•›
        if max_error < tol:
            break
            
        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-4:
            break
    
    # ç¼©æ”¾è‡³ç›®æ ‡æ€»é‡
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        weights *= (target_total / current_total)
    
    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            actual_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / target_total
            actual_dist[dim][cat] = actual_ratio
    
    return weights, actual_dist, (max_error < tol)

def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])
    
    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        if not remaining.empty:
            remaining['prob'] = (additional * remaining['token_count'] / 
                                remaining['token_count'].sum() / 
                                remaining['token_count'])
            retained[~retained] = np.random.random(len(remaining)) < np.minimum(remaining['prob'], 1.0)
    
    return df[retained].copy()

def get_verified_text(file_path, offset, expected_id=None, expected_hash=None):
    """å¸¦éªŒè¯çš„æ–‡æœ¬è·å–ï¼ˆæ ¸å¿ƒï¼šä¿è¯100%å‡†ç¡®æ€§ï¼‰"""
    try:
        with open(file_path, 'rb') as f:
            f.seek(offset)
            line = f.readline()
            
            # 1. éªŒè¯ç‰©ç†å®Œæ•´æ€§
            if expected_hash:
                actual_hash = hashlib.md5(line).hexdigest()
                if actual_hash != expected_hash:
                    logger.error(f"æ•°æ®ç¯¡æ”¹æ£€æµ‹: {file_path}:{offset} | æœŸæœ›å“ˆå¸Œ: {expected_hash} | å®é™…: {actual_hash}")
                    return f"[ERROR: DATA CORRUPTED AT {offset}]"
            
            # 2. è§£æJSON
            try:
                data = json.loads(line.decode('utf-8', errors='replace'))
            except json.JSONDecodeError:
                logger.error(f"JSONè§£æå¤±è´¥: {file_path}:{offset}")
                return f"[ERROR: INVALID JSON AT {offset}]"
            
            # 3. éªŒè¯é€»è¾‘IDï¼ˆå¦‚æœæä¾›ï¼‰
            if expected_id is not None:
                actual_id = data.get('id')
                if actual_id != expected_id:
                    logger.warning(f"IDä¸åŒ¹é…: æœŸæœ› {expected_id} ä½†å¾—åˆ° {actual_id} | {file_path}:{offset}")
            
            return data.get('text', "")
            
    except Exception as e:
        logger.exception(f"è¯»å–å¤±è´¥ {file_path}:{offset} - {str(e)}")
        return f"[ERROR: READ FAILED AT {offset}]"

def export_shards_verified(df, output_path, shard_size_gb=1):
    """å¸¦éªŒè¯çš„åˆ†ç‰‡å¯¼å‡ºï¼ˆä¿è¯100%æ•°æ®å‡†ç¡®æ€§ï¼‰"""
    # ç¡®ä¿è¾“å‡ºè·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    output_path = os.path.abspath(output_path)
    os.makedirs(output_path, exist_ok=True)
    
    shard_size_bytes = shard_size_gb * GB
    current_size = 0
    shard_idx = 1
    buffer = []
    
    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_container = st.empty()
    status_text = st.sidebar.empty()
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„å¤„ç†ï¼ˆå‡å°‘æ–‡ä»¶æ‰“å¼€æ¬¡æ•°ï¼‰
    total_samples = len(df)
    processed = 0
    
    for (file_path, offset), group in df.groupby(['file_path', 'offset']):
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        abs_file_path = os.path.abspath(file_path)
        
        for _, row in group.iterrows():
            # å…³é”®ï¼šä½¿ç”¨åŒé‡éªŒè¯è·å–æ–‡æœ¬
            text = get_verified_text(
                abs_file_path,
                offset,
                expected_id=row.get('id'),
                expected_hash=row.get('line_hash')
            )
            
            # åˆ›å»ºæ ·æœ¬
            sample = {
                'id': row.get('id'),
                'source': row['source'],
                'category': row['category'],
                'domain': row['domain'],
                'language': row['language'],
                'token_count': row['token_count'],
                'text': text
            }
            
            # åºåˆ—åŒ–ä¸ºJSONL
            try:
                sample_json = json.dumps(sample, ensure_ascii=False) + '\n'
                sample_bytes = len(sample_json.encode('utf-8'))
            except Exception as e:
                logger.error(f"åºåˆ—åŒ–å¤±è´¥: {str(e)} | æ ·æœ¬: {sample}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°åˆ†ç‰‡
            if current_size + sample_bytes > shard_size_bytes and buffer:
                shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
                try:
                    with open(shard_path, 'w', encoding='utf-8') as out_f:
                        out_f.writelines(buffer)
                except Exception as e:
                    logger.error(f"å†™å…¥åˆ†ç‰‡å¤±è´¥ {shard_path}: {str(e)}")
                    st.sidebar.error(f"å†™å…¥åˆ†ç‰‡å¤±è´¥: {str(e)}")
                    return
                buffer = []
                current_size = 0
                shard_idx += 1
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            buffer.append(sample_json)
            current_size += sample_bytes
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ¯100æ ·æœ¬ï¼‰
            processed += 1
            if processed % 100 == 0:
                with progress_container.container():
                    progress = processed / total_samples
                    st.progress(min(progress, 1.0))
                    st.caption(f"å¤„ç†æ ·æœ¬ {processed}/{total_samples} | å½“å‰åˆ†ç‰‡: {shard_idx}")
                status_text.text(f"å¯¼å‡ºè¿›åº¦: {progress:.1%} | åˆ†ç‰‡: {shard_idx}")
    
    # å†™å…¥æœ€åä¸€ä¸ªåˆ†ç‰‡
    if buffer:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        try:
            with open(shard_path, 'w', encoding='utf-8') as f:
                f.writelines(buffer)
        except Exception as e:
            logger.error(f"å†™å…¥æœ€ç»ˆåˆ†ç‰‡å¤±è´¥ {shard_path}: {str(e)}")
            st.sidebar.error(f"å†™å…¥æœ€ç»ˆåˆ†ç‰‡å¤±è´¥: {str(e)}")
            return
    
    progress_container.empty()
    status_text.empty()
    st.sidebar.success(f"å¯¼å‡ºå®Œæˆï¼å…± {shard_idx} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

# ========== ä¼˜åŒ–åçš„æ•°æ®åŠ è½½å‡½æ•° ==========
def load_dataset_parallel(data_path):
    """å¹¶è¡ŒåŠ è½½JSONLæ•°æ®é›†ï¼Œè¿”å›å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯"""
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    data_path = os.path.abspath(data_path)
    
    # 1. æ‰«ææ‰€æœ‰JSONLæ–‡ä»¶ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    jsonl_files = []
    total_size = 0
    for root, _, files in os.walk(data_path):
        for file in files:
            if file.lower().endswith('.jsonl'):
                file_path = os.path.abspath(os.path.join(root, file))
                jsonl_files.append(file_path)
                total_size += os.path.getsize(file_path)
    
    if not jsonl_files:
        return None, f"æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {data_path}"
    
    st.sidebar.info(f"ğŸ“ æ‰«æåˆ° {len(jsonl_files)} ä¸ªæ–‡ä»¶ | æ€»å¤§å°: {total_size/(1024**3):.1f} GB")
    
    # 2. å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼ˆä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUæ ¸å¿ƒï¼‰
    all_metadata = []
    progress_small = st.sidebar.empty()
    
    # è‡ªåŠ¨ç¡®å®šå·¥ä½œè¿›ç¨‹æ•°ï¼ˆä¸è¶…è¿‡32ï¼Œé¿å…è¿‡åº¦è°ƒåº¦ï¼‰
    max_workers = min(32, os.cpu_count() or 1)
    
    def process_file(file):
        """å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶è®°å½•ç²¾ç¡®å…ƒæ•°æ®"""
        metadata = []
        try:
            with open(file, 'rb') as f:  # å¿…é¡»ç”¨äºŒè¿›åˆ¶æ¨¡å¼
                offset = 0
                while True:
                    line = f.readline()
                    if not line:
                        break
                    
                    try:
                        # è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆç”¨äºåç»­éªŒè¯ï¼‰
                        line_hash = hashlib.md5(line).hexdigest()
                        
                        # å°è¯•è§£æJSON
                        try:
                            data = json.loads(line.decode('utf-8', errors='replace'))
                        except json.JSONDecodeError:
                            offset += len(line)
                            continue
                        
                        # éªŒè¯å¿…è¦å­—æ®µ
                        required_fields = ['source', 'category', 'domain', 'language', 'token_count']
                        if all(k in data for k in required_fields):
                            # ç¡®ä¿token_countæ˜¯æ•°å­—
                            try:
                                token_count = int(float(data['token_count']))
                                
                                # æå–IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                sample_id = data.get('id')
                                if sample_id is not None:
                                    sample_id = str(sample_id)
                                
                                meta = {
                                    'id': sample_id,  # ä¿å­˜UUIDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                    'source': str(data['source']),
                                    'category': str(data['category']),
                                    'domain': str(data['domain']),
                                    'language': str(data['language']),
                                    'token_count': token_count,
                                    'file_path': file,
                                    'offset': offset,
                                    'line_hash': line_hash
                                }
                                metadata.append(meta)
                            except (ValueError, TypeError):
                                pass
                    except Exception as e:
                        logger.debug(f"å¤„ç†æ–‡ä»¶ {file} åç§»é‡ {offset} æ—¶å‡ºé”™: {str(e)}")
                    
                    # æ›´æ–°åç§»é‡
                    offset += len(line)
        except Exception as e:
            logger.exception(f"å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™")
            return file, str(e), []
        
        return file, None, metadata
    
    # å¹¶è¡Œå¤„ç†
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_file, file) for file in jsonl_files]
        
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            file, error, metadata = future.result()
            if error:
                st.sidebar.warning(f"âš ï¸ {os.path.basename(file)}: {error}")
            else:
                all_metadata.extend(metadata)
                progress_small.text(f"âœ… å¤„ç† {i+1}/{len(jsonl_files)} | æ ·æœ¬: {len(all_metadata):,}")
    
    progress_small.empty()
    
    if not all_meta
        return None, "æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®æ ·æœ¬"
    
    # 3. åˆ›å»ºå…ƒæ•°æ®DataFrame
    df = pd.DataFrame(all_metadata)
    total_tokens = df['token_count'].sum()
    
    # 4. è®¡ç®—tokenåˆ†ç»„
    token_bins = [get_token_bin(tc) for tc in df['token_count']]
    
    # 5. è®°å½•å…³é”®æŒ‡æ ‡
    logger.info(f"åŠ è½½å®Œæˆ: {len(df)} æ ·æœ¬ | {total_tokens/1e9:.2f}B tokens")
    
    return {
        'df': df,
        'total_tokens': total_tokens,
        'token_bins': token_bins
    }, None

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")

# è·¯å¾„è¯Šæ–­å·¥å…·
st.sidebar.subheader("ğŸ” è·¯å¾„è¯Šæ–­")
diagnose = st.sidebar.checkbox("å¯ç”¨è·¯å¾„è¯Šæ–­", value=False)

if diagnose:
    data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value=os.getcwd())
    
    if data_path:
        abs_path = os.path.abspath(data_path)
        st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
        
        if os.path.exists(abs_path):
            st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
            st.sidebar.info(f"åŒ…å« {len(os.listdir(abs_path))} ä¸ªé¡¹ç›®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰JSONLæ–‡ä»¶
            has_jsonl = any(f.lower().endswith('.jsonl') for f in os.listdir(abs_path))
            st.sidebar.info(f"åŒ…å«JSONLæ–‡ä»¶: {'æ˜¯' if has_jsonl else 'å¦'}")
        else:
            st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨")
else:
    data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value=os.getcwd())

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›† (æé€Ÿæ¨¡å¼)", type="primary"):
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    abs_data_path = os.path.abspath(data_path)
    
    if not data_path or not os.path.exists(abs_data_path):
        st.sidebar.error("âŒ è¯·æä¾›æœ‰æ•ˆçš„ç»å¯¹è·¯å¾„")
    else:
        # æ˜¾ç¤ºå†…å­˜ç›‘æ§
        mem_col1, mem_col2 = st.sidebar.columns(2)
        mem_usage = psutil.virtual_memory().percent
        mem_col1.metric("å†…å­˜ä½¿ç”¨", f"{mem_usage:.1f}%")
        mem_col2.metric("å¯ç”¨å†…å­˜", f"{psutil.virtual_memory().available/(1024**3):.1f} GB")
        
        if mem_usage > 80:
            st.sidebar.warning("âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼ŒåŠ è½½å¯èƒ½å¤±è´¥")
        
        start_time = time.time()
        with st.spinner("âš¡ æ­£åœ¨å¹¶è¡ŒåŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰..."):
            result, error = load_dataset_parallel(abs_data_path)
            
            if error:
                st.sidebar.error(f"åŠ è½½å¤±è´¥: {error}")
            else:
                # å­˜å‚¨åˆ°session state
                st.session_state.df = result['df']
                st.session_state.total_tokens = result['total_tokens']
                st.session_state.token_bins = result['token_bins']
                
                # è®¡ç®—åŠ è½½é€Ÿåº¦
                elapsed = time.time() - start_time
                speed = result['total_tokens'] / elapsed / 1e6  # MB tokens/s
                
                st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(result['df']):,} ä¸ªæ ·æœ¬")
                st.sidebar.info(f"â±ï¸ è€—æ—¶: {elapsed:.1f}ç§’ | é€Ÿåº¦: {speed:.1f}M tokens/ç§’")
                st.sidebar.info(f"ğŸ“Š æ€»Tokenæ•°: {result['total_tokens']/1e9:.2f}B")
                
                # æ˜¾ç¤ºIDç»Ÿè®¡
                if 'id' in result['df'] and not pd.isna(result['df']['id']).all():
                    unique_ids = result['df']['id'].nunique()
                    total = len(result['df'])
                    st.sidebar.info(f"ğŸ”‘ å”¯ä¸€ID: {unique_ids:,} / {total:,} ({unique_ids/total:.1%})")

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df' in st.session_state:
    df = st.session_state.df
    total_tokens = st.session_state.total_tokens
    
    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")
    
    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)
    
    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']
    target_ratios = {}
    
    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")
        
        # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼
        if dim == 'token_bin':
            values = pd.Series(st.session_state.token_bins).unique()
        else:
            values = df[dim].unique()
        
        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        if dim == 'token_bin':
            current_dist = df.groupby(pd.Series(st.session_state.token_bins))['token_count'].sum() / total_tokens
        else:
            current_dist = df.groupby(dim)['token_count'].sum() / total_tokens
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
        target_ratios[dim] = {}
        total_ratio = 0.0
        cols = st.sidebar.columns(min(3, len(values)))  # é™åˆ¶æ¯è¡Œæœ€å¤š3ä¸ª
        
        for i, val in enumerate(values):
            current_ratio = current_dist.get(val, 0.0)
            with cols[i % len(cols)]:
                ratio = st.number_input(
                    f"{val}", 
                    min_value=0.0, 
                    max_value=1.0, 
                    value=float(current_ratio),
                    step=0.01,
                    format="%.4f",
                    key=f"{dim}_{val}"
                )
                target_ratios[dim][val] = ratio
                total_ratio += ratio
        
        # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")
    
    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            # è¿è¡ŒIPFæ±‚è§£å™¨
            weights, actual_dist, converged = ipf_solver(
                df, 
                target_ratios, 
                target_total,
                tol=0.01  # 1%è¯¯å·®
            )
            
            if weights is not None:
                # å­˜å‚¨é‡‡æ ·ç»“æœ
                sampled_df = sample_dataset(df, weights, target_total)
                st.session_state.sampled_df = sampled_df
                
                # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                actual_tokens = sampled_df['token_count'].sum()
                st.sidebar.info(f"å®é™…æ€»é‡: {actual_tokens/1e9:.2f}B tokens ({actual_tokens/target_total:.1%} of target)")
                
                # æ˜¾ç¤ºå…³é”®ç»´åº¦è¯¯å·®
                for dim in ['language', 'domain', 'source']:
                    if dim in actual_dist:
                        max_error = 0
                        for cat in actual_dist[dim]:
                            target = target_ratios[dim].get(cat, 0)
                            actual = actual_dist[dim].get(cat, 0)
                            error = abs(target - actual)
                            max_error = max(max_error, error)
                        st.sidebar.caption(f"{dim}: æœ€å¤§è¯¯å·® {max_error:.1%}")
    
    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    
    # ç¡®ä¿å¯¼å‡ºè·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    output_path = st.sidebar.text_input(
        "å¯¼å‡ºè·¯å¾„ (ç»å¯¹è·¯å¾„)", 
        value=os.path.abspath("./balanced_datasets")
    )
    
    # éªŒè¯å¯¼å‡ºè·¯å¾„
    if output_path:
        abs_output_path = os.path.abspath(output_path)
        st.sidebar.caption(f"è§„èŒƒè·¯å¾„: {abs_output_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å¯å†™
        try:
            test_file = os.path.join(abs_output_path, ".test_write")
            os.makedirs(abs_output_path, exist_ok=True)
            with open(test_file, 'w') as f:
                f.write("test")
            os.remove(test_file)
            st.sidebar.success("âœ… è·¯å¾„å¯å†™")
        except Exception as e:
            st.sidebar.error(f"âŒ è·¯å¾„ä¸å¯å†™: {str(e)}")
    
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)
    
    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            abs_output_path = os.path.abspath(output_path)
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                export_shards_verified(st.session_state.sampled_df, abs_output_path, shard_size)
    
    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========
    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")
    
    # åˆ›å»ºå›¾è¡¨å¸ƒå±€
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)
    
    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        source_dist = calculate_distribution(df, 'source')
        if not source_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— sourceåˆ†å¸ƒæ•°æ®")
    
    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        category_dist = calculate_distribution(df, 'category')
        if not category_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— categoryåˆ†å¸ƒæ•°æ®")
    
    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        domain_dist = calculate_distribution(df, 'domain')
        if not domain_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— domainåˆ†å¸ƒæ•°æ®")
    
    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        lang_dist = calculate_distribution(df, 'language')
        if not lang_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— languageåˆ†å¸ƒæ•°æ®")
    
    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        df['token_bin'] = st.session_state.token_bins
        token_dist = calculate_distribution(df, 'token_bin')
        
        # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨
        for _, _, label in TOKEN_BINS:
            if label not in token_dist:
                token_dist[label] = 0.0
        
        token_dist = token_dist.reindex([label for _, _, label in TOKEN_BINS])
        
        if not token_dist.empty:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.bar(token_dist.index, token_dist.values)
            ax.set_ylabel('æ¯”ä¾‹')
            ax.set_title('Tokené•¿åº¦åˆ†å¸ƒ')
            for i, v in enumerate(token_dist.values):
                ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
            st.pyplot(fig)
        else:
            st.info("æ— token countåˆ†å¸ƒæ•°æ®")
    
    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
        # åˆ›å»ºå­ç±»ç»„åˆ
        df['subclass'] = df['source'] + "+" + df['category'] + "+" + df['domain'] + "+" + df['language']
        subclass_dist = calculate_distribution(df, 'subclass')
        
        if not subclass_dist.empty:
            # å–Top 10
            top10 = subclass_dist.head(10)
            
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.barh(top10.index, top10.values)
            ax.set_xlabel('æ¯”ä¾‹')
            ax.set_title('Top 10 å­ç±»ç»„åˆåˆ†å¸ƒ')
            
            # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
            for i, v in enumerate(top10.values):
                ax.text(v + 0.005, i, f'{v:.1%}', va='center')
            
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.info("æ— å­ç±»ç»„åˆåˆ†å¸ƒæ•°æ®")
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
    st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")
    
    # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
    if 'sampled_df' in st.session_state:
        st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
        sampled_df = st.session_state.sampled_df
        sampled_tokens = sampled_df['token_count'].sum()
        
        st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
        st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")
        
        # æ¯”è¾ƒå…³é”®ç»´åº¦
        col1, col2, col3 = st.columns(3)
        for i, dim in enumerate(['language', 'domain', 'source']):
            orig_dist = calculate_distribution(df, dim)
            sampled_dist = calculate_distribution(sampled_df, dim)
            
            if orig_dist.empty or sampled_dist.empty:
                continue
                
            # è®¡ç®—æœ€å¤§è¯¯å·®
            max_error = 0
            for cat in orig_dist.index:
                orig = orig_dist.get(cat, 0)
                sampled = sampled_dist.get(cat, 0)
                error = abs(orig - sampled)
                max_error = max(max_error, error)
            
            if i == 0:
                col1.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            elif i == 1:
                col2.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
            else:
                col3.metric(f"{dim.capitalize()} æœ€å¤§è¯¯å·®", f"{max_error:.1%}")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    # ç”¨æœ¬åœ°SVGæ›¿ä»£ç½‘ç»œå›¾ç‰‡ï¼ˆé¿å…CDNé—®é¢˜ï¼‰
    st.markdown("""
    <div style="text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px;">
        <svg xmlns="http://www.w3.org/2000/svg" width="300" height="300" viewBox="0 0 300 300">
            <rect width="100%" height="100%" fill="#ffffff"/>
            <text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" 
                  font-family="Arial" font-size="20px" fill="#000000">
                æ•°æ®é…æ¯”å·¥å…·
            </text>
            <text x="50%" y="65%" dominant-baseline="middle" text-anchor="middle" 
                  font-family="Arial" font-size="14px" fill="#666666">
                è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»"åŠ è½½æ•°æ®é›†"
            </text>
        </svg>
    </div>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    st.subheader("ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. **åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„**ï¼ˆå¿…é¡»æ˜¯åŒ…å«JSONLæ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼‰
    2. **ç‚¹å‡»'åŠ è½½æ•°æ®é›†'**ï¼ˆè·¯å¾„è¯Šæ–­å¯å¸®åŠ©ç¡®è®¤è·¯å¾„æœ‰æ•ˆæ€§ï¼‰
    3. **åˆ†ææ•°æ®åˆ†å¸ƒ**ï¼ˆå³ä¾§å›¾è¡¨å®æ—¶æ˜¾ç¤ºï¼‰
    4. **è°ƒæ•´é…æ¯”å‚æ•°**ï¼ˆå¯åŒæ—¶è°ƒæ•´å¤šä¸ªç»´åº¦ï¼‰
    5. **å¯¼å‡ºé…æ¯”æ•°æ®é›†**ï¼ˆæŒ‡å®šç»å¯¹è·¯å¾„å’Œåˆ†ç‰‡å¤§å°ï¼‰
    
    ğŸ’¡ **æç¤º**ï¼š 
    - ç¡®ä¿è·¯å¾„æ˜¯**ç»å¯¹è·¯å¾„**
    - ç³»ç»Ÿä¼šè‡ªåŠ¨é€’å½’æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶
    - æ”¯æŒTBçº§æ•°æ®é›†ï¼ˆåˆ©ç”¨æœåŠ¡å™¨å¤šæ ¸CPUåŠ é€Ÿï¼‰
    """)

# æç®€è°ƒè¯•ç‰ˆæœ¬ - ä»…æµ‹è¯•æŒ‰é’®å“åº”å’ŒåŸºæœ¬çŠ¶æ€
import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tqdm import tqdm
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import tempfile
import shutil

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…· - æç®€è°ƒè¯•ç‰ˆ")

# ========== å·¦ä¾§é…ç½®æ  ==========
st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="./test_data")

# åˆå§‹åŒ–å¤„ç†æ¨¡å¼çŠ¶æ€
if 'processing_mode' not in st.session_state:
    st.session_state.processing_mode = "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰" # é»˜è®¤å€¼

# æ•°æ®å¤„ç†æ¨¡å¼é€‰æ‹© (ä¸ session_state åŒæ­¥)
selected_mode = st.sidebar.radio(
    "å¤„ç†æ¨¡å¼",
    ["å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰", "æµå¼æ¨¡å¼ï¼ˆå¤§æ•°æ®ï¼‰"],
    index=0 if st.session_state.processing_mode == "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰" else 1,
    help="å†…å­˜æ¨¡å¼é€‚ç”¨äº<100GBæ•°æ®ï¼Œæµå¼æ¨¡å¼é€‚ç”¨äº>100GBæ•°æ®"
)
# æ›´æ–° session_state (å¦‚æœç”¨æˆ·æ”¹å˜äº†é€‰æ‹©)
st.session_state.processing_mode = selected_mode

# --- ç®€åŒ–åˆ°æè‡´çš„åŠ è½½æŒ‰é’® ---
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    st.write("DEBUG: Load button clicked!") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 1
    st.write(f"DEBUG: Current processing mode is '{st.session_state.processing_mode}'") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 2
    
    # 1. æ£€æŸ¥è·¯å¾„
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
        st.stop() # <-- ç¡®ä¿è„šæœ¬åœæ­¢
    
    # 2. è§„èŒƒåŒ–è·¯å¾„
    data_path_normalized = os.path.normpath(data_path)
    st.sidebar.info(f"æ­£åœ¨å¤„ç†è·¯å¾„: {data_path_normalized}")
    st.write(f"DEBUG: Path normalized to '{data_path_normalized}'") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 3

    # 3. æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒé€»è¾‘ (ç®€åŒ–)
    try:
        if st.session_state.processing_mode == "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰":
            st.write("DEBUG: Inside Memory Mode Logic") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 4
            # --- æç®€å†…å­˜æ¨¡å¼é€»è¾‘ ---
            # a. æ‰«ææ–‡ä»¶ (ç®€åŒ–)
            jsonl_files = []
            for root, _, files in os.walk(data_path_normalized):
                for file in files:
                    if file.lower().endswith('.jsonl'):
                        jsonl_files.append(os.path.join(root, file))
            st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
            st.write(f"DEBUG: Found {len(jsonl_files)} JSONL files") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 5
            
            if not jsonl_files:
                 st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶")
                 st.stop()
            
            # b. è¯»å–ä¸€ä¸ªæ–‡ä»¶çš„å‰å‡ è¡Œä½œä¸ºç¤ºä¾‹ (ä¸å¹¶è¡Œï¼Œä¸å¤„ç†å…¨éƒ¨)
            sample_file = jsonl_files[0]
            sample_data = []
            try:
                with open(sample_file, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= 5: # åªè¯»5è¡Œ
                            break
                        try:
                            sample_data.append(json.loads(line))
                        except json.JSONDecodeError:
                            st.sidebar.warning(f"æ–‡ä»¶ {sample_file} ç¬¬ {i+1} è¡Œ JSON è§£æå¤±è´¥")
            except Exception as e:
                 st.sidebar.error(f"è¯»å–æ–‡ä»¶ {sample_file} å¤±è´¥: {e}")
                 st.stop()
            
            st.write("DEBUG: Sample data read successfully") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 6
            # c. åˆ›å»ºä¸€ä¸ªéå¸¸å°çš„ DataFrame (ä»…ç”¨ç¤ºä¾‹æ•°æ®)
            if sample_data:
                df_sample = pd.DataFrame(sample_data)
                if 'token_count' in df_sample.columns:
                    df_sample['token_count'] = pd.to_numeric(df_sample['token_count'], errors='coerce').fillna(0).astype(int)
                
                # d. å­˜å‚¨åˆ° session state
                st.session_state.df = df_sample
                st.session_state.total_tokens = df_sample['token_count'].sum() if 'token_count' in df_sample.columns else 0
                st.session_state.processing_mode = "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰" # ç¡®ä¿çŠ¶æ€ä¸€è‡´
                
                st.sidebar.success(f"ğŸ‰ æç®€åŠ è½½æˆåŠŸï¼ç¤ºä¾‹æ•°æ® {len(df_sample)} è¡Œ")
                st.write("DEBUG: Data stored in session_state") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 7
            else:
                st.sidebar.warning("âš ï¸ æœªè¯»å–åˆ°æœ‰æ•ˆç¤ºä¾‹æ•°æ®")
                st.stop()

        else: # æµå¼æ¨¡å¼
            st.write("DEBUG: Inside Streaming Mode Logic") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 4
            # --- æç®€æµå¼æ¨¡å¼é€»è¾‘ ---
            # a. åˆå§‹åŒ– Sampler
            sampler = LargeDataSampler(data_path_normalized)
            # b. æ‰«ææ–‡ä»¶
            file_count = sampler.scan_files()
            st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {file_count} ä¸ªJSONLæ–‡ä»¶")
            st.write(f"DEBUG: Streaming mode found {file_count} files") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 5
            
            if file_count == 0:
                st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶")
                st.stop()
            
            # c. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ (ç®€åŒ–ç‰ˆï¼Œåªå¤„ç†ä¸€ä¸ªæ–‡ä»¶)
            if sampler.jsonl_files:
                stats = sampler._calculate_statistics_single_file(sampler.jsonl_files[0]) # ä½¿ç”¨ç®€åŒ–æ–¹æ³•
                # d. å­˜å‚¨åˆ° session state
                st.session_state.sampler = sampler
                st.session_state.stats = stats
                st.session_state.processing_mode = "æµå¼æ¨¡å¼ï¼ˆå¤§æ•°æ®ï¼‰" # ç¡®ä¿çŠ¶æ€ä¸€è‡´
                
                st.sidebar.success(f"ğŸ‰ æç®€æµå¼ç»Ÿè®¡å®Œæˆï¼")
                st.write("DEBUG: Streaming stats stored in session_state") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ 7
            else:
                st.sidebar.warning("âš ï¸ æ— æ–‡ä»¶å¯ç»Ÿè®¡")
                st.stop()

    except Exception as e:
        st.sidebar.error(f"åŠ è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        st.write(f"DEBUG: Exception during loading: {e}") # <-- å…³é”®è°ƒè¯•ä¿¡æ¯ (å¦‚æœå‡ºé”™)
        import traceback
        st.code(traceback.format_exc()) # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯å †æ ˆ
        st.stop()

# --- ç®€åŒ–åˆ°æè‡´çš„ UI æ˜¾ç¤º ---
st.header("ğŸ”„ ç®€åŒ–çŠ¶æ€æ˜¾ç¤º")
if 'df' in st.session_state and st.session_state.processing_mode == "å†…å­˜æ¨¡å¼ï¼ˆå°æ•°æ®ï¼‰":
    st.success("âœ… å†…å­˜æ¨¡å¼æ•°æ®å·²åŠ è½½")
    st.write("**ç¤ºä¾‹æ•°æ®:**")
    st.dataframe(st.session_state.df.head())
    st.write(f"**æ€»Tokenæ•° (ç¤ºä¾‹):** {st.session_state.total_tokens}")

elif 'sampler' in st.session_state and st.session_state.processing_mode == "æµå¼æ¨¡å¼ï¼ˆå¤§æ•°æ®ï¼‰":
    st.success("âœ… æµå¼æ¨¡å¼æ•°æ®å·²åŠ è½½")
    st.write("**ç¤ºä¾‹ç»Ÿè®¡ä¿¡æ¯:**")
    stats_to_show = st.session_state.stats
    if isinstance(stats_to_show, dict) and 'dimensions' in stats_to_show:
        for dim, counts in list(stats_to_show['dimensions'].items())[:2]: # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªç»´åº¦
            st.write(f"- **{dim}:** {dict(list(counts.items())[:3])}...") # åªæ˜¾ç¤ºå‰3ä¸ªç±»åˆ«
    else:
        st.write(stats_to_show)

else:
    st.info("ğŸ‘ˆ è¯·ç‚¹å‡»å·¦ä¾§ 'åŠ è½½æ•°æ®é›†' æŒ‰é’®")
    st.write("å½“å‰å¤„ç†æ¨¡å¼:", st.session_state.processing_mode)
    st.write("Session State Keys:", list(st.session_state.keys()))


# --- ç®€åŒ–ç‰ˆ LargeDataSampler (ä»…ç”¨äºè°ƒè¯•) ---
class LargeDataSampler:
    """å¤„ç†å¤§å®¹é‡æ•°æ®çš„é‡‡æ ·å™¨ - ç®€åŒ–è°ƒè¯•ç‰ˆ"""
    def __init__(self, data_path, chunk_size=1000): # å‡å° chunk_size ç”¨äºè°ƒè¯•
        self.data_path = data_path
        self.chunk_size = chunk_size
        self.jsonl_files = []
        self.stats = {}

    def scan_files(self):
        """æ‰«ææ‰€æœ‰JSONLæ–‡ä»¶"""
        self.jsonl_files = []
        for root, _, files in os.walk(self.data_path):
            for file in files:
                if file.lower().endswith('.jsonl'):
                    self.jsonl_files.append(os.path.join(root, file))
        return len(self.jsonl_files)

    def _calculate_statistics_single_file(self, file_path):
        """ç®€åŒ–ç‰ˆï¼šåªè®¡ç®—å•ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': 0,
            'total_tokens': 0,
            'dimensions': defaultdict(lambda: defaultdict(int))
        }
        try:
            chunk_iter = pd.read_json(file_path, lines=True, chunksize=self.chunk_size)
            if not hasattr(chunk_iter, '__iter__'):
                chunk_iter = [chunk_iter]
            
            # åªå¤„ç†ç¬¬ä¸€ä¸ª chunk
            for chunk in chunk_iter:
                 required_fields = ['source', 'category', 'domain', 'language', 'token_count', 'text']
                 if all(col in chunk.columns for col in required_fields):
                     chunk['token_count'] = pd.to_numeric(chunk['token_count'], errors='coerce')
                     chunk.dropna(subset=['token_count'], inplace=True)
                     chunk['token_count'] = chunk['token_count'].astype(int)
                     
                     stats['total_samples'] += len(chunk)
                     stats['total_tokens'] += chunk['token_count'].sum()
                     
                     for dim in ['source', 'category']:
                         dim_counts = chunk[dim].value_counts()
                         for val, count in dim_counts.items():
                             stats['dimensions'][dim][str(val)] += count
                 break # <-- åªå¤„ç†ä¸€ä¸ª chunk å°±åœæ­¢
                 
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
        # è½¬æ¢ä¸ºæ¯”ä¾‹ (ç®€åŒ–)
        for dim in stats['dimensions']:
            total = sum(stats['dimensions'][dim].values())
            if total > 0:
                for val in stats['dimensions'][dim]:
                    stats['dimensions'][dim][val] = stats['dimensions'][dim][val] / total
        return stats

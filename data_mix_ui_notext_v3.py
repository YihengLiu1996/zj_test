import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import glob
import matplotlib.pyplot as plt
from tqdm import tqdm
import re
from io import StringIO
import time
from scipy.optimize import nnls
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
import math
import hashlib
import logging
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('data_balancer')

# é…ç½®é¡µé¢
st.set_page_config(layout="wide", page_title="æ•°æ®é…æ¯”å·¥å…·")
st.title("ğŸ“Š æ•°æ®é…æ¯”åˆ†æä¸è°ƒæ•´å·¥å…·")

# å…¨å±€å¸¸é‡
TOKEN_BINS = [
    (0, 4096, "0-4k"),
    (4096, 8192, "4k-8k"),
    (8192, 16384, "8k-16k"),
    (16384, 32768, "16k-32k"),
    (32768, float('inf'), ">32k")
]
GB = 1024 * 1024 * 1024  # 1GB in bytes

# ========== æ ¸å¿ƒå·¥å…·å‡½æ•° ==========

def get_token_bin(token_count):
    """ç¡®å®štoken_countæ‰€å±åŒºé—´"""
    for low, high, label in TOKEN_BINS:
        if low <= token_count < high:
            return label
    return ">32k"

def calculate_distribution(df, column, weights=None):
    """è®¡ç®—åŠ æƒåˆ†å¸ƒ"""
    if weights is None:
        weights = df['token_count']
    total = weights.sum()
    if total == 0:
        return pd.Series()
    dist = df.groupby(column).apply(lambda x: np.sum(weights[x.index]) / total)
    return dist.sort_values(ascending=False)

# ========== æ–°å¢ï¼šç”¨äºç¼“å­˜çš„å“ˆå¸Œç”Ÿæˆå™¨ ==========
def get_dataframe_hash(df):
    """
    ç”ŸæˆDataFrameçš„å“ˆå¸Œå€¼ï¼Œç”¨äºå›¾è¡¨ç¼“å­˜ã€‚
    ä½¿ç”¨å‰å‡ è¡Œå’Œæ€»è¡Œæ•°ã€æ€»tokenæ•°æ¥ç”Ÿæˆä¸€ä¸ªè½»é‡çº§å“ˆå¸Œã€‚
    """
    sample_str = str(df.head(10).to_dict()) + str(len(df)) + str(df['token_count'].sum())
    return hashlib.md5(sample_str.encode('utf-8')).hexdigest()

@st.cache_data(show_spinner=False)
def calculate_distribution_cached_wrapper(_df, column, weights=None):
    """
    åŒ…è£…å™¨å‡½æ•°ï¼Œç”¨äºç¼“å­˜åˆ†å¸ƒè®¡ç®—ã€‚
    _df å‰ç¼€å‘Šè¯‰ Streamlit ä¸è¦å°è¯•å“ˆå¸Œè¿™ä¸ªå‚æ•°ï¼ˆæˆ‘ä»¬è‡ªå·±å¤„ç†ï¼‰ã€‚
    """
    return calculate_distribution(_df, column, weights)

# ========== æ”¹é€ åçš„å›¾è¡¨å‡½æ•°ï¼Œå…¨éƒ¨åŠ å…¥ç¼“å­˜ ==========
@st.cache_data(show_spinner=False)
def get_cached_pie_chart_data(_df, column, cache_key, data_version):
    """ç¼“å­˜é¥¼å›¾æ•°æ®"""
    return calculate_distribution_cached_wrapper(_df, column)

@st.cache_data(show_spinner=False)
def get_cached_bar_chart_data(_df, column, cache_key, data_version):
    """ç¼“å­˜æŸ±çŠ¶å›¾æ•°æ®"""
    return calculate_distribution_cached_wrapper(_df, column)

@st.cache_data(show_spinner=False)
def get_cached_subclass_data(_df, cache_key, data_version):
    """ç¼“å­˜å­ç±»ç»„åˆæ•°æ®"""
    _df['subclass'] = _df['source'] + "+" + _df['category'] + "+" + _df['domain'] + "+" + _df['language']
    subclass_dist = calculate_distribution_cached_wrapper(_df, 'subclass')
    return subclass_dist.head(10) if not subclass_dist.empty else pd.Series()

def advanced_ipf_solver(df, target_ratios, target_total, priority_order, max_iter=100, tol=0.005):
    """
    æ”¹è¿›çš„IPFæ±‚è§£å™¨ - æ”¯æŒå¤šç»´åº¦åŒæ—¶ä¼˜åŒ–ã€ä¼˜å…ˆçº§æ’åºï¼Œå¹¶åœ¨è¿­ä»£ä¸­è€ƒè™‘ç›®æ ‡æ€»é‡
    :param df: æ•°æ®DataFrame (ä»…åŒ…å«å…ƒæ•°æ®)
    :param target_ratios: ç›®æ ‡æ¯”ä¾‹å­—å…¸ {ç»´åº¦: {ç±»åˆ«: æ¯”ä¾‹}}
    :param target_total: ç›®æ ‡æ€»tokenæ•°
    :param priority_order: ä¼˜å…ˆçº§é¡ºåºåˆ—è¡¨
    :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param tol: è¯¯å·®å®¹å¿åº¦(0.5%)
    :return: é‡‡æ ·æƒé‡æ•°ç»„, å®é™…åˆ†å¸ƒ, æ˜¯å¦æ”¶æ•›
    """
    # åˆå§‹åŒ–æƒé‡: è€ƒè™‘åˆ°ç›®æ ‡æ€»é‡è¿œå°äºåŸå§‹æ€»é‡ï¼Œåˆå§‹æƒé‡åº”è¿œå°äº1
    total_tokens = df['token_count'].sum()
    if total_tokens == 0:
        st.error("é”™è¯¯ï¼šæ•°æ®é›†ä¸­token_countæ€»å’Œä¸º0")
        return None, None, False

    initial_scale_guess = target_total / total_tokens if total_tokens > 0 else 1.0
    weights = np.full(len(df), initial_scale_guess) # ä½¿ç”¨ initial_scale_guess åˆå§‹åŒ–æ‰€æœ‰æƒé‡

    # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹å¯è¡Œæ€§
    for dim, targets in target_ratios.items():
        for cat, ratio in targets.items():
            # æ£€æŸ¥è¯¥ç±»åˆ«åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨
            if cat not in df[dim].values:
                st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' ä¸­ä¸å­˜åœ¨ç±»åˆ« '{cat}'")
                return None, None, False
            # æ£€æŸ¥ç›®æ ‡æ¯”ä¾‹æ˜¯å¦è¶…è¿‡åŸå§‹æ•°æ®æœ€å¤§å¯èƒ½
            orig_ratio = (df[df[dim] == cat]['token_count'].sum() / total_tokens)
            if ratio > orig_ratio * 1.05:  # å…è®¸5%ç¼“å†²
                st.warning(f"è­¦å‘Šï¼š'{dim}'ä¸­'{cat}'ç›®æ ‡æ¯”ä¾‹({ratio:.2%})å¯èƒ½è¶…è¿‡åŸå§‹æ¯”ä¾‹({orig_ratio:.2%})")
        # æ£€æŸ¥ç»´åº¦å†…æ¯”ä¾‹å’Œ
        dim_sum = sum(targets.values())
        if not (0.99 <= dim_sum <= 1.01):
            st.error(f"é”™è¯¯ï¼šç»´åº¦ '{dim}' çš„ç›®æ ‡æ¯”ä¾‹å’Œ({dim_sum:.2%})ä¸åœ¨[99%, 101%]èŒƒå›´å†…")
            return None, None, False

    # æ£€æŸ¥ä¼˜å…ˆçº§é¡ºåºæ˜¯å¦å®Œæ•´ä¸”å”¯ä¸€
    if set(priority_order) != set(target_ratios.keys()):
        st.error(f"é”™è¯¯ï¼šä¼˜å…ˆçº§é¡ºåºå¿…é¡»åŒ…å«æ‰€æœ‰ç»´åº¦ä¸”ä¸èƒ½é‡å¤")
        return None, None, False

    # å¼€å§‹IPFè¿­ä»£ (ç§»é™¤ç»´åº¦å†»ç»“é€»è¾‘ï¼Œæ¯æ¬¡éƒ½æ£€æŸ¥æ‰€æœ‰ç»´åº¦)
    all_dims = list(target_ratios.keys()) # ä½¿ç”¨åˆ—è¡¨ä¿æŒé¡ºåºï¼Œè™½ç„¶è¿™é‡Œä¸å…³é”®
    for iter in range(max_iter):
        prev_weights = weights.copy()
        max_errors = {}
        # è®¡ç®—å½“å‰åŠ æƒæ€»å’Œ
        current_total = np.sum(weights * df['token_count'])
        # è®¡ç®—æ€»é‡è¯¯å·®å› å­ (é¿å…é™¤é›¶)
        total_factor = (target_total / current_total) if current_total > 1e-5 else 1.0
        # é™åˆ¶æ€»é‡è°ƒæ•´å¹…åº¦ï¼Œé˜²æ­¢å‰§çƒˆæ³¢åŠ¨
        total_factor = max(0.8, min(1.2, total_factor))

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºè¿­ä»£è°ƒæ•´ç»´åº¦
        for dim in priority_order:
            targets = target_ratios[dim]
            dim_max_error = 0
            for cat, target_ratio in targets.items():
                # è®¡ç®—å½“å‰ç»´åº¦ç±»åˆ«çš„åŠ æƒæ¯”ä¾‹
                mask = (df[dim] == cat)
                # ä½¿ç”¨å½“å‰çš„ weights è®¡ç®— current_ratio
                current_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / current_total if current_total > 1e-5 else 0.0
                # è®¡ç®—æ¯”ä¾‹è°ƒæ•´å› å­ï¼ˆé¿å…é™¤é›¶ï¼‰
                if current_ratio > 1e-5 and target_ratio > 0:
                    ratio_factor = target_ratio / current_ratio
                    # é™åˆ¶æ¯”ä¾‹è°ƒæ•´å¹…åº¦
                    ratio_factor = max(0.7, min(1.4, ratio_factor))
                    # æ›´æ–°æƒé‡ï¼šç»“åˆæ¯”ä¾‹å› å­å’Œæ€»é‡å› å­
                    # è¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ï¼šæƒé‡æ›´æ–°åŒæ—¶è€ƒè™‘äº†æ¯”ä¾‹å’Œæ€»é‡
                    combined_factor = ratio_factor * total_factor
                    weights[mask] *= combined_factor
                # è®°å½•æœ€å¤§è¯¯å·®
                error = abs(current_ratio - target_ratio)
                dim_max_error = max(dim_max_error, error)
            max_errors[dim] = dim_max_error
            # æ³¨æ„ï¼šä¸å†å°†ç»´åº¦åŠ å…¥ converged_dims é›†åˆ

        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦æ˜¯å¦éƒ½æ”¶æ•› (åœ¨æ¯æ¬¡è¿­ä»£åéƒ½æ£€æŸ¥)
        if all(error < tol for error in max_errors.values()):
            st.info(f"âœ… æ‰€æœ‰ç»´åº¦åœ¨ç¬¬ {iter+1} è½®è¿­ä»£åæ”¶æ•›")
            break

        # æ£€æŸ¥æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(weights - prev_weights) / (prev_weights + 1e-5))
        if weight_change < 1e-5:
            st.info(f"âš ï¸ æƒé‡å˜åŒ–è¿‡å°ï¼Œåœ¨ç¬¬ {iter+1} è½®è¿­ä»£ååœæ­¢")
            break

    # è¿­ä»£ç»“æŸåï¼Œè¿›è¡Œä¸€æ¬¡æœ€ç»ˆçš„æ€»é‡æ ¡å‡† (å¯é€‰ï¼Œä½†é€šå¸¸æ˜¯ä¸ªå¥½ä¸»æ„)
    # å› ä¸ºè¿­ä»£ä¸­çš„ total_factor æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼
    current_total = np.sum(weights * df['token_count'])
    if current_total > 0:
        final_scale_factor = target_total / current_total
        weights *= final_scale_factor
        # æ›´æ–° current_total ä»¥ç”¨äºåç»­è®¡ç®—
        current_total = target_total

    # è®¡ç®—å®é™…åˆ†å¸ƒï¼ˆç”¨äºéªŒè¯ï¼‰
    actual_dist = {}
    final_errors = {}
    # ä½¿ç”¨æœ€ç»ˆæ ¡å‡†åçš„ current_total (å³ target_total) æ¥è®¡ç®—å®é™…æ¯”ä¾‹
    for dim in target_ratios.keys():
        actual_dist[dim] = {}
        dim_max_error = 0
        for cat in target_ratios[dim].keys():
            mask = (df[dim] == cat)
            # ä½¿ç”¨æœ€ç»ˆçš„ weights å’Œ target_total è®¡ç®—å®é™…æ¯”ä¾‹
            actual_ratio = np.sum(weights[mask] * df.loc[mask, 'token_count']) / current_total
            actual_dist[dim][cat] = actual_ratio
            target_ratio = target_ratios[dim][cat]
            error = abs(actual_ratio - target_ratio)
            dim_max_error = max(dim_max_error, error)
        final_errors[dim] = dim_max_error

    # æ˜¾ç¤ºå„ç»´åº¦è¯¯å·® (æŒ‰ä¼˜å…ˆçº§é¡ºåºæ˜¾ç¤º)
    st.subheader("ğŸ“Š å„ç»´åº¦é…æ¯”è¯¯å·®")
    for dim in priority_order: # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ˜¾ç¤º
        error = final_errors[dim]
        if error <= tol:
            st.success(f"âœ… {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")
        else:
            st.warning(f"âš ï¸ {dim}: æœ€å¤§è¯¯å·® {error:.3f} ({error*100:.1f}%)")

    is_converged = all(error <= tol for error in final_errors.values())
    return weights, actual_dist, is_converged

def sample_dataset(df, weights, target_total):
    """æ ¹æ®æƒé‡è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·"""
    # ç”Ÿæˆä¿ç•™æ¦‚ç‡ï¼ˆæˆªæ–­åˆ°[0,1]ï¼‰
    probs = np.minimum(weights, 1.0)
    # ä¼¯åŠªåˆ©é‡‡æ ·
    retained = np.random.random(len(df)) < probs
    # è®¡ç®—å®é™…é‡‡æ ·æ€»é‡
    sampled_tokens = np.sum(df.loc[retained, 'token_count'])

    # è°ƒæ•´é‡‡æ ·ï¼ˆç¡®ä¿æ¥è¿‘ç›®æ ‡æ€»é‡ï¼‰
    if sampled_tokens < target_total * 0.95:  # ä½äº95%æ—¶è¡¥å……
        additional = target_total - sampled_tokens
        remaining = df[~retained].copy()
        if len(remaining) > 0:
            remaining_prob = (additional * remaining['token_count'] / 
                             remaining['token_count'].sum() if remaining['token_count'].sum() > 0 else 0)
            remaining['prob'] = remaining_prob
            retained[~retained] = np.random.random(len(remaining)) < np.minimum(remaining['prob'], 1.0)

    return df[retained].copy()

# ========== å…³é”®æ”¹é€ ï¼šå¸¦éªŒè¯çš„æ–‡æœ¬è·å–ä¸å¯¼å‡º ==========

def get_verified_text(file_path, offset, expected_id=None, expected_hash=None):
    """å¸¦éªŒè¯çš„æ–‡æœ¬è·å–ï¼ˆæ ¸å¿ƒï¼šä¿è¯100%å‡†ç¡®æ€§ï¼‰"""
    try:
        with open(file_path, 'rb') as f:
            f.seek(offset)
            line = f.readline()
            # 1. éªŒè¯ç‰©ç†å®Œæ•´æ€§
            if expected_hash:
                actual_hash = hashlib.md5(line).hexdigest()
                if actual_hash != expected_hash:
                    logger.error(f"æ•°æ®ç¯¡æ”¹æ£€æµ‹: {file_path}:{offset} | æœŸæœ›å“ˆå¸Œ: {expected_hash} | å®é™…: {actual_hash}")
                    return f"[ERROR: DATA CORRUPTED AT {offset}]"
            # 2. è§£æJSON
            try:
                data = json.loads(line.decode('utf-8', errors='replace'))
            except json.JSONDecodeError:
                logger.error(f"JSONè§£æå¤±è´¥: {file_path}:{offset}")
                return f"[ERROR: INVALID JSON AT {offset}]"
            # 3. éªŒè¯é€»è¾‘IDï¼ˆå¦‚æœæä¾›ï¼‰
            if expected_id is not None:
                actual_id = data.get('id')
                if actual_id != expected_id:
                    logger.warning(f"IDä¸åŒ¹é…: æœŸæœ› {expected_id} ä½†å¾—åˆ° {actual_id} | {file_path}:{offset}")
            return data.get('text', "")
    except Exception as e:
        logger.exception(f"è¯»å–å¤±è´¥ {file_path}:{offset} - {str(e)}")
        return f"[ERROR: READ FAILED AT {offset}]"

def export_shards_verified(df, output_path, shard_size_gb=1):
    """å¸¦éªŒè¯çš„åˆ†ç‰‡å¯¼å‡ºï¼ˆä¿è¯100%æ•°æ®å‡†ç¡®æ€§ï¼‰ - æ”¯æŒåˆ†ç‰‡å¹¶è¡Œå†™å…¥"""
    os.makedirs(output_path, exist_ok=True)
    shard_size_bytes = shard_size_gb * GB
    current_size = 0
    shard_idx = 1
    buffer = []
    shard_data_list = []  # å­˜å‚¨æ¯ä¸ªåˆ†ç‰‡çš„æ•°æ®åˆ—è¡¨

    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_container = st.empty()
    status_text = st.sidebar.empty()

    total_samples = len(df)
    processed = 0

    # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®åˆ†ç»„ï¼ˆä¸²è¡Œï¼Œç¡®ä¿é¡ºåºå’Œåˆ†ç‰‡å¤§å°å‡†ç¡®ï¼‰
    for (file_path, offset), group in df.groupby(['file_path', 'offset']):
        for _, row in group.iterrows():
            # å…³é”®ï¼šä½¿ç”¨åŒé‡éªŒè¯è·å–æ–‡æœ¬
            text = get_verified_text(
                file_path,
                offset,
                expected_id=row.get('id'),
                expected_hash=row.get('line_hash')
            )

            # åˆ›å»ºæ ·æœ¬
            sample = {
                'id': row.get('id'),
                'source': row['source'],
                'category': row['category'],
                'domain': row['domain'],
                'language': row['language'],
                'token_count': row['token_count'],
                'text': text
            }

            # åºåˆ—åŒ–ä¸ºJSONL
            try:
                sample_json = json.dumps(sample, ensure_ascii=False) + '\n'
                sample_bytes = len(sample_json.encode('utf-8'))
            except Exception as e:
                logger.error(f"åºåˆ—åŒ–å¤±è´¥: {str(e)} | æ ·æœ¬: {sample}")
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°åˆ†ç‰‡
            if current_size + sample_bytes > shard_size_bytes and buffer:
                # å°†å½“å‰ç¼“å†²åŒºçš„æ•°æ®å’Œåˆ†ç‰‡è·¯å¾„åŠ å…¥åˆ—è¡¨
                shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
                shard_data_list.append({
                    'shard_path': shard_path,
                    'data_lines': buffer.copy()  # å¤åˆ¶å½“å‰ç¼“å†²åŒº
                })
                buffer = []
                current_size = 0
                shard_idx += 1

            # æ·»åŠ åˆ°ç¼“å†²åŒº
            buffer.append(sample_json)
            current_size += sample_bytes

            # æ›´æ–°è¿›åº¦ï¼ˆæ¯100æ ·æœ¬ï¼‰
            processed += 1
            if processed % 100 == 0:
                with progress_container.container():
                    progress = processed / total_samples
                    st.progress(min(progress, 1.0))
                    st.caption(f"åˆ†ç»„æ ·æœ¬ {processed}/{total_samples} | å½“å‰åˆ†ç‰‡: {shard_idx}")
                status_text.text(f"åˆ†ç»„è¿›åº¦: {progress:.1%} | åˆ†ç‰‡: {shard_idx}")

    # å¤„ç†æœ€åä¸€ä¸ªåˆ†ç‰‡
    if buffer:
        shard_path = os.path.join(output_path, f"shard_{shard_idx:04d}.jsonl")
        shard_data_list.append({
            'shard_path': shard_path,
            'data_lines': buffer
        })

    progress_container.empty()
    status_text.empty()
    st.sidebar.info(f"æ•°æ®åˆ†ç»„å®Œæˆï¼å…±éœ€åˆ›å»º {len(shard_data_list)} ä¸ªåˆ†ç‰‡")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡Œå†™å…¥åˆ†ç‰‡ ==========
    if not shard_data_list:
        st.sidebar.warning("æ— æ•°æ®å¯å¯¼å‡º")
        return

    # åˆ›å»ºæ–°çš„è¿›åº¦æ¡ç”¨äºå†™å…¥é˜¶æ®µ
    write_progress = st.sidebar.progress(0)
    write_status = st.sidebar.empty()

    # å®šä¹‰å•ä¸ªåˆ†ç‰‡çš„å†™å…¥å‡½æ•°
    def write_single_shard(shard_info):
        """å†™å…¥å•ä¸ªåˆ†ç‰‡æ–‡ä»¶"""
        shard_path = shard_info['shard_path']
        data_lines = shard_info['data_lines']
        try:
            with open(shard_path, 'w', encoding='utf-8') as f:
                f.writelines(data_lines)
            return True, shard_path, None
        except Exception as e:
            error_msg = f"å†™å…¥åˆ†ç‰‡å¤±è´¥ {shard_path}: {str(e)}"
            logger.error(error_msg)
            return False, shard_path, error_msg

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå†™å…¥
    # çº¿ç¨‹æ•°è®¾ç½®ä¸º min(32, CPUæ ¸å¿ƒæ•° * 2)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œå¯æ ¹æ®æœåŠ¡å™¨è°ƒæ•´
    max_workers = min(32, (os.cpu_count() or 1) * 2)
    success_count = 0
    failed_shards = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰åˆ†ç‰‡å†™å…¥ä»»åŠ¡
        future_to_shard = {
            executor.submit(write_single_shard, shard_info): shard_info['shard_path']
            for shard_info in shard_data_list
        }

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(as_completed(future_to_shard)):
            success, shard_path, error_msg = future.result()
            if success:
                success_count += 1
            else:
                failed_shards.append(error_msg)

            # æ›´æ–°å†™å…¥è¿›åº¦
            progress = (i + 1) / len(shard_data_list)
            write_progress.progress(progress)
            write_status.text(f"å†™å…¥è¿›åº¦: {i+1}/{len(shard_data_list)} | æˆåŠŸ: {success_count}")

    # æ¸…ç†è¿›åº¦æ¡
    write_progress.empty()
    write_status.empty()

    # æŠ¥å‘Šæœ€ç»ˆç»“æœ
    if failed_shards:
        st.sidebar.warning(f"å¯¼å‡ºå®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_shards)}")
        for error in failed_shards[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            st.sidebar.error(error)
        if len(failed_shards) > 5:
            st.sidebar.error(f"... è¿˜æœ‰ {len(failed_shards) - 5} ä¸ªé”™è¯¯")
    else:
        st.sidebar.success(f"ğŸ‰ å¯¼å‡ºå®Œæˆï¼å…± {success_count} ä¸ªåˆ†ç‰‡ï¼Œè·¯å¾„: {output_path}")

# ========== ç‹¬ç«‹çš„æ–‡ä»¶å¤„ç†å‡½æ•°ï¼ˆä¿®å¤pickleé”™è¯¯ï¼‰ ==========

def process_file_for_parallel_load(file_path):
    """
    ç‹¬ç«‹çš„æ–‡ä»¶å¤„ç†å‡½æ•°ï¼Œç”¨äºå¹¶è¡ŒåŠ è½½ã€‚
    æ­¤å‡½æ•°å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ï¼Œä»¥ä¾¿è¢«pickleåºåˆ—åŒ–ã€‚
    """
    metadata = []
    try:
        with open(file_path, 'rb') as f:  # å¿…é¡»ç”¨äºŒè¿›åˆ¶æ¨¡å¼
            offset = 0
            while True:
                line = f.readline()
                if not line:
                    break
                try:
                    # è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆç”¨äºåç»­éªŒè¯ï¼‰
                    line_hash = hashlib.md5(line).hexdigest()
                    # å°è¯•è§£æJSON
                    try:
                        data = json.loads(line.decode('utf-8', errors='replace'))
                    except json.JSONDecodeError:
                        offset += len(line)
                        continue

                    # éªŒè¯å¿…è¦å­—æ®µ
                    required_fields = ['source', 'category', 'domain', 'language', 'token_count']
                    if all(k in data for k in required_fields):
                        # ç¡®ä¿token_countæ˜¯æ•°å­—
                        try:
                            token_count = int(float(data['token_count']))
                            # æå–IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            sample_id = data.get('id')
                            if sample_id is not None:
                                sample_id = str(sample_id)

                            # åªå­˜å‚¨å…ƒæ•°æ®å’Œå®šä½ä¿¡æ¯ï¼Œä¸å­˜å‚¨text
                            meta = {
                                'id': sample_id,  # ä¿å­˜UUIDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                'source': str(data['source']),
                                'category': str(data['category']),
                                'domain': str(data['domain']),
                                'language': str(data['language']),
                                'token_count': token_count,
                                'file_path': file_path,  # è®°å½•æ–‡ä»¶è·¯å¾„
                                'offset': offset,   # è®°å½•æ–‡ä»¶åç§»é‡
                                'line_hash': line_hash # è®°å½•è¡Œå“ˆå¸Œï¼Œç”¨äºéªŒè¯
                            }
                            metadata.append(meta)
                        except (ValueError, TypeError):
                            pass
                except Exception as e:
                    logger.debug(f"å¤„ç†æ–‡ä»¶ {file_path} åç§»é‡ {offset} æ—¶å‡ºé”™: {str(e)}")

                # æ›´æ–°åç§»é‡
                offset += len(line)
    except Exception as e:
        logger.exception(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™")
        return file_path, str(e), []

    return file_path, None, metadata

# ========== æ•°æ®åŠ è½½å‡½æ•°ï¼ˆæ”¹é€ æ ¸å¿ƒï¼‰ ==========

def load_dataset_parallel(data_path):
    """å¹¶è¡ŒåŠ è½½JSONLæ•°æ®é›†ï¼Œä»…è¿”å›å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸åŠ è½½textå­—æ®µï¼‰"""
    # 1. æ‰«ææ‰€æœ‰JSONLæ–‡ä»¶ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    jsonl_files = []
    total_size = 0
    for root, _, files in os.walk(data_path):
        for file in files:
            if file.lower().endswith('.jsonl'):
                file_path = os.path.abspath(os.path.join(root, file))
                jsonl_files.append(file_path)
                total_size += os.path.getsize(file_path)

    if not jsonl_files:
        return None, f"æœªæ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {data_path}"

    st.sidebar.info(f"ğŸ“ æ‰«æåˆ° {len(jsonl_files)} ä¸ªæ–‡ä»¶ | æ€»å¤§å°: {total_size/(1024**3):.1f} GB")

    # 2. å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼ˆä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUæ ¸å¿ƒï¼‰
    all_metadata = []
    # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()

    # è‡ªåŠ¨ç¡®å®šå·¥ä½œè¿›ç¨‹æ•°ï¼ˆä¸è¶…è¿‡32ï¼Œé¿å…è¿‡åº¦è°ƒåº¦ï¼‰
    max_workers = min(32, os.cpu_count() or 1)

    # å¹¶è¡Œå¤„ç† - ä½¿ç”¨å…¨å±€å®šä¹‰çš„ process_file_for_parallel_load
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤ä»»åŠ¡
        futures = [executor.submit(process_file_for_parallel_load, file) for file in jsonl_files]
        for i, future in enumerate(as_completed(futures)):
            file, error, metadata = future.result()
            if error:
                st.sidebar.warning(f"âš ï¸ {os.path.basename(file)}: {error}")
            else:
                all_metadata.extend(metadata)
                # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress = (i + 1) / len(jsonl_files)
                progress_bar.progress(progress)
                status_text.text(f"âœ… å¤„ç† {i+1}/{len(jsonl_files)} | æ ·æœ¬: {len(all_metadata):,}")

    # æ¸…ç†è¿›åº¦æ¡
    progress_bar.empty()
    status_text.empty()

    if not all_metadata:
        return None, "æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®æ ·æœ¬"

    # 3. åˆ›å»ºå…ƒæ•°æ®DataFrame
    df = pd.DataFrame(all_metadata)
    total_tokens = df['token_count'].sum()

    # 4. è®¡ç®—tokenåˆ†ç»„
    token_bins = [get_token_bin(tc) for tc in df['token_count']]

    # 5. è®°å½•å…³é”®æŒ‡æ ‡
    logger.info(f"åŠ è½½å®Œæˆ: {len(df)} æ ·æœ¬ | {total_tokens/1e9:.2f}B tokens")

    return {
        'df': df,
        'total_tokens': total_tokens,
        'token_bins': token_bins
    }, None

# ========== å·¦ä¾§é…ç½®æ  ==========

st.sidebar.header("ğŸ”§ é…ç½®é¢æ¿")
data_path = st.sidebar.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„", value="/path/to/datasets")

# æ·»åŠ è·¯å¾„è¯Šæ–­å·¥å…·
if st.sidebar.checkbox("ğŸ” å¯ç”¨è·¯å¾„è¯Šæ–­", value=False):
    st.sidebar.subheader("è·¯å¾„è¯Šæ–­")
    abs_path = os.path.abspath(data_path) if data_path else ""
    st.sidebar.code(f"ç»å¯¹è·¯å¾„: {abs_path}")
    if data_path and os.path.exists(data_path):
        st.sidebar.success("âœ… è·¯å¾„å­˜åœ¨")
        st.sidebar.info(f"åŒ…å« {len(os.listdir(data_path))} ä¸ªé¡¹ç›®")
    else:
        st.sidebar.error("âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆ")

# åŠ è½½æ•°æ®æŒ‰é’®
if st.sidebar.button("ğŸ“ åŠ è½½æ•°æ®é›†", type="primary"):
    if not data_path:
        st.sidebar.error("âŒ è¯·å…ˆè¾“å…¥è·¯å¾„")
    else:
        # å…³é”®ä¿®å¤ï¼šè§„èŒƒåŒ–è·¯å¾„ï¼ˆè§£å†³Windowså¤§å°å†™é—®é¢˜ï¼‰
        data_path = os.path.normpath(data_path)
        with st.spinner("ğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†æ–‡ä»¶..."):
            try:
                # è°ƒç”¨æ”¹é€ åçš„åŠ è½½å‡½æ•°
                result, error = load_dataset_parallel(data_path)
                if error:
                    st.sidebar.error(f"åŠ è½½å¤±è´¥: {error}")
                else:
                    # å­˜å‚¨åˆ°session state
                    st.session_state.df = result['df']
                    st.session_state.total_tokens = result['total_tokens']
                    st.session_state.token_bins = result['token_bins']
                    st.session_state.df['token_bin'] = st.session_state.token_bins

                    # æ–°å¢ï¼šé€’å¢æ•°æ®ç‰ˆæœ¬å·ï¼Œä½¿å›¾è¡¨ç¼“å­˜å¤±æ•ˆ
                    if 'data_version' not in st.session_state:
                        st.session_state.data_version = 0
                    st.session_state.data_version += 1

                    st.sidebar.success(f"ğŸ‰ åŠ è½½æˆåŠŸï¼å…± {len(result['df']):,} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œ{result['total_tokens']/1e9:.2f}B tokens")

                    # æ˜¾ç¤ºIDç»Ÿè®¡
                    if 'id' in result['df'] and not pd.isna(result['df']['id']).all():
                        unique_ids = result['df']['id'].nunique()
                        total = len(result['df'])
                        st.sidebar.info(f"ğŸ”‘ å”¯ä¸€ID: {unique_ids:,} / {total:,} ({unique_ids/total:.1%})")

            except Exception as e:
                st.sidebar.exception(f"_fatal error_: {str(e)}")
                st.stop()

# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
if 'df' in st.session_state:
    df = st.session_state.df
    total_tokens = st.session_state.total_tokens

    # ========== é…æ¯”è°ƒæ•´é…ç½® ==========
    st.sidebar.header("âš–ï¸ é…æ¯”è°ƒæ•´")

    # ç›®æ ‡æ€»é‡è¾“å…¥
    target_total_b = st.sidebar.number_input(
        "ç›®æ ‡æ€»é‡ (B tokens)", 
        min_value=0.01, 
        value=1.0, 
        step=0.1,
        help="1B = 10äº¿tokens"
    )
    target_total = int(target_total_b * 1e9)

    # å®šä¹‰ç»´åº¦
    dimensions = ['source', 'category', 'domain', 'language', 'token_bin']

    # ä¼˜å…ˆçº§æ’åºè¾“å…¥ (æ–°å¢)
    st.sidebar.subheader("ğŸ“Œ ä¼˜å…ˆçº§æ’åº")
    st.sidebar.info("è¯·æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ‹–æ‹½ç»´åº¦")

    # åˆå§‹åŒ– session_state å­˜å‚¨ä¼˜å…ˆçº§é¡ºåº
    if 'priority_order' not in st.session_state:
        st.session_state.priority_order = dimensions.copy()

    # åˆ›å»ºå¯æ‹–æ‹½çš„ä¼˜å…ˆçº§æ’åºç»„ä»¶
    # ä½¿ç”¨ selectbox æ¨¡æ‹Ÿæ’åºï¼ˆStreamlit åŸç”Ÿä¸æ”¯æŒæ‹–æ‹½ï¼‰
    priority_placeholders = {}
    temp_priority_order = st.session_state.priority_order.copy()
    for i in range(len(dimensions)):
        with st.sidebar.container():
            cols = st.sidebar.columns([1, 4])
            cols[0].markdown(f"**{i+1}.**")
            # åˆ›å»ºä¸€ä¸ªä¸‹æ‹‰åˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰æœªè¢«é€‰æ‹©çš„ç»´åº¦
            available_dims = [dim for dim in dimensions if dim not in temp_priority_order[:i]]
            selected_dim = cols[1].selectbox(
                f"é€‰æ‹©ç¬¬ {i+1} ä¼˜å…ˆçº§ç»´åº¦",
                options=available_dims,
                index=available_dims.index(temp_priority_order[i]) if temp_priority_order[i] in available_dims else 0,
                key=f"priority_{i}"
            )
            temp_priority_order[i] = selected_dim

    # æ›´æ–° session_state ä¸­çš„ä¼˜å…ˆçº§é¡ºåº
    if temp_priority_order != st.session_state.priority_order:
        st.session_state.priority_order = temp_priority_order
        st.rerun() # é‡æ–°è¿è¡Œä»¥æ›´æ–°UI

    st.sidebar.caption(f"å½“å‰ä¼˜å…ˆçº§é¡ºåº: {' > '.join(st.session_state.priority_order)}")

    # åŠ¨æ€ç”Ÿæˆå„ç»´åº¦é…æ¯”è¾“å…¥
    target_ratios = {}

    # åˆå§‹åŒ– session_state å­˜å‚¨ç›®æ ‡æ¯”ä¾‹
    if 'target_ratios' not in st.session_state:
        st.session_state.target_ratios = {}

    # è·å– token_bin é¡ºåº
    token_bin_order = [label for _, _, label in TOKEN_BINS]

    for dim in dimensions:
        st.sidebar.subheader(f"{dim.capitalize()} é…æ¯”")

        # è·å–è¯¥ç»´åº¦çš„å”¯ä¸€å€¼ï¼ˆæŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼‰
        if dim == 'token_bin':
            values = sorted(df['token_bin'].unique(), key=lambda x: token_bin_order.index(x) if x in token_bin_order else len(token_bin_order))
        else:
            values = sorted(df[dim].unique())

        # è®¡ç®—å½“å‰åˆ†å¸ƒ
        if dim == 'token_bin':
            current_dist = df.groupby('token_bin')['token_count'].sum() / total_tokens
        else:
            current_dist = df.groupby(dim)['token_count'].sum() / total_tokens

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè¾“å…¥æ¡†
        if dim not in st.session_state.target_ratios:
            st.session_state.target_ratios[dim] = {}

        target_ratios[dim] = {}
        total_ratio = 0.0

        # æ¯è¡Œæœ€å¤šæ”¾ 3 ä¸ªè¾“å…¥æ¡†
        items_per_row = 3
        for i_start in range(0, len(values), items_per_row):
            cols = st.sidebar.columns(items_per_row)
            for i_offset, val in enumerate(values[i_start:i_start + items_per_row]):
                current_ratio = current_dist.get(val, 0.0)
                with cols[i_offset]:
                    ratio = st.number_input(
                        label=f"{val}",
                        min_value=0.0,
                        max_value=1.0,
                        value=float(current_ratio),
                        step=0.01,
                        format="%.3f",  # æ˜¾ç¤º3ä½å°æ•°
                        key=f"{dim}_{val}"
                    )
                    st.session_state.target_ratios[dim][val] = ratio
                    target_ratios[dim][val] = ratio
                    total_ratio += ratio

        # æ˜¾ç¤ºç»´åº¦å†…æ¯”ä¾‹å’Œ
        st.sidebar.caption(f"å½“å‰å’Œ: {total_ratio:.2%}")
        if not (0.99 <= total_ratio <= 1.01):
            st.sidebar.warning("æ¯”ä¾‹å’Œåº”æ¥è¿‘100%")

    # åº”ç”¨é…æ¯”æŒ‰é’®
    if st.sidebar.button("ğŸ¯ åº”ç”¨é…æ¯”", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é…æ¯”æ–¹æ¡ˆ..."):
            # ä» session_state è¯»å–æœ€æ–°çš„ç›®æ ‡æ¯”ä¾‹å’Œä¼˜å…ˆçº§é¡ºåº
            target_ratios = st.session_state.target_ratios
            priority_order = st.session_state.priority_order

            # è¿è¡Œæ”¹è¿›çš„IPFæ±‚è§£å™¨
            weights, actual_dist, converged = advanced_ipf_solver(
                df, 
                target_ratios, 
                target_total,
                priority_order, # ä¼ å…¥ä¼˜å…ˆçº§é¡ºåº
                max_iter=100,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                tol=0.005      # é™ä½è¯¯å·®å®¹å¿åº¦åˆ°0.5%
            )

            if weights is not None:
                # å­˜å‚¨é‡‡æ ·ç»“æœï¼ˆæ­¤æ—¶dfä¸­ä»ä¸åŒ…å«textå­—æ®µï¼‰
                sampled_df = sample_dataset(df, weights, target_total)
                st.session_state.sampled_df = sampled_df

                # æ˜¾ç¤ºé‡‡æ ·ç»“æœ
                st.sidebar.success("é…æ¯”æ–¹æ¡ˆå·²ç”Ÿæˆï¼")
                st.sidebar.info(f"å®é™…æ€»é‡: {sampled_df['token_count'].sum()/1e9:.2f}B tokens")

                # æ˜¾ç¤ºæ”¶æ•›çŠ¶æ€
                if converged:
                    st.sidebar.success("âœ… æ‰€æœ‰ç»´åº¦é…æ¯”å‡å·²æ»¡è¶³ï¼")
                else:
                    st.sidebar.warning("âš ï¸ éƒ¨åˆ†ç»´åº¦é…æ¯”æœªå®Œå…¨æ»¡è¶³ï¼Œè¯·æ£€æŸ¥è¯¯å·®æŠ¥å‘Š")

    # ========== å¯¼å‡ºé…ç½® ==========
    st.sidebar.header("ğŸ“¤ å¯¼å‡ºè®¾ç½®")
    output_path = st.sidebar.text_input("å¯¼å‡ºè·¯å¾„", value="./balanced_datasets")
    shard_size = st.sidebar.number_input("åˆ†ç‰‡å¤§å° (GB)", min_value=0.1, value=1.0, step=0.1)

    if st.sidebar.button("ğŸ’¾ å¯¼å‡ºé…æ¯”æ•°æ®é›†", type="primary"):
        if 'sampled_df' not in st.session_state:
            st.sidebar.error("è¯·å…ˆåº”ç”¨é…æ¯”æ–¹æ¡ˆ")
        else:
            with st.spinner("æ­£åœ¨å¯¼å‡ºåˆ†ç‰‡..."):
                # è°ƒç”¨å¸¦éªŒè¯çš„å¯¼å‡ºå‡½æ•°
                export_shards_verified(st.session_state.sampled_df, output_path, shard_size)

    # ========== å³ä¾§å›¾è¡¨å±•ç¤º ==========

    st.header("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ")

    # ä¸ºå½“å‰æ•°æ®ç”Ÿæˆç¼“å­˜é”®
    cache_key = get_dataframe_hash(df)
    # è·å–æ•°æ®ç‰ˆæœ¬å·
    data_version = st.session_state.get('data_version', 0) # æ–°å¢ï¼šè·å–ç‰ˆæœ¬å·

    # åˆ›å»ºå›¾è¡¨å¸ƒå±€
    col1, col2 = st.columns(2)
    col3, col4 = st.columns(2)
    col5, col6 = st.columns(2)

    # 1. Source é…æ¯”å›¾
    with col1:
        st.subheader("æ•°æ®æ¥æº (Source) åˆ†å¸ƒ")
        source_dist = get_cached_pie_chart_data(df, 'source', cache_key, data_version)
        if not source_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(source_dist, labels=source_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— sourceåˆ†å¸ƒæ•°æ®")

    # 2. Category é…æ¯”å›¾
    with col2:
        st.subheader("æ•°æ®ç±»åˆ« (Category) åˆ†å¸ƒ")
        category_dist = get_cached_pie_chart_data(df, 'category', cache_key, data_version)
        if not category_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(category_dist, labels=category_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— categoryåˆ†å¸ƒæ•°æ®")

    # 3. Domain é…æ¯”å›¾
    with col3:
        st.subheader("æ•°æ®é¢†åŸŸ (Domain) åˆ†å¸ƒ")
        domain_dist = get_cached_pie_chart_data(df, 'domain', cache_key, data_version)
        if not domain_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(domain_dist, labels=domain_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— domainåˆ†å¸ƒæ•°æ®")

    # 4. Language é…æ¯”å›¾
    with col4:
        st.subheader("è¯­è¨€ (Language) åˆ†å¸ƒ")
        lang_dist = get_cached_pie_chart_data(df, 'language', cache_key, data_version)
        if not lang_dist.empty:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(lang_dist, labels=lang_dist.index, autopct='%1.1f%%', startangle=90)
            ax.axis('equal')
            st.pyplot(fig)
        else:
            st.info("æ— languageåˆ†å¸ƒæ•°æ®")

    # 5. Token Count é…æ¯”å›¾
    with col5:
        st.subheader("Tokené•¿åº¦åˆ†å¸ƒ")
        token_dist = get_cached_bar_chart_data(df, 'token_bin', cache_key, data_version)
        # ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½å­˜åœ¨å¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—
        ordered_labels = [label for _, _, label in TOKEN_BINS]
        for label in ordered_labels:
            if label not in token_dist:
                token_dist[label] = 0.0
        token_dist = token_dist.reindex(ordered_labels)

        if not token_dist.empty:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.bar(token_dist.index, token_dist.values)
            ax.set_ylabel('Ratio')
            ax.set_title('Token length distribution')
            for i, v in enumerate(token_dist.values):
                ax.text(i, v + 0.01, f'{v:.1%}', ha='center')
            st.pyplot(fig)
        else:
            st.info("æ— token countåˆ†å¸ƒæ•°æ®")

    # 6. å­ç±»åˆ†å¸ƒå›¾
    with col6:
        st.subheader("å­ç±»ç»„åˆåˆ†å¸ƒ (Top 10)")
        top10 = get_cached_subclass_data(df, cache_key, data_version)
        if not top10.empty:
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.barh(top10.index, top10.values)
            ax.set_xlabel('æ¯”ä¾‹')
            ax.set_title('Top 10 distribution of subclass combinations')
            # æ·»åŠ æ¯”ä¾‹æ ‡ç­¾
            for i, v in enumerate(top10.values):
                ax.text(v + 0.005, i, f'{v:.1%}', va='center')
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.info("æ— å­ç±»ç»„åˆåˆ†å¸ƒæ•°æ®")

    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    st.divider()
    st.subheader("ğŸ” æ•°æ®æ‘˜è¦")
    st.write(f"**æ€»æ ·æœ¬æ•°**: {len(df):,}")
    st.write(f"**æ€»Tokenæ•°**: {total_tokens/1e9:.2f} B (10äº¿)")
    st.write(f"**å¹³å‡Tokené•¿åº¦**: {total_tokens/len(df):.0f}")

    # å¦‚æœæœ‰é‡‡æ ·æ•°æ®ï¼Œæ˜¾ç¤ºé‡‡æ ·è´¨é‡
    if 'sampled_df' in st.session_state:
        st.subheader("ğŸ¯ é‡‡æ ·è´¨é‡æŠ¥å‘Š")
        sampled_df = st.session_state.sampled_df
        sampled_tokens = sampled_df['token_count'].sum()

        st.write(f"**é‡‡æ ·æ€»é‡**: {sampled_tokens/1e9:.2f} B tokens")
        st.write(f"**é‡‡æ ·æ¯”ä¾‹**: {len(sampled_df)/len(df):.1%}")

        # æ¯”è¾ƒå…³é”®ç»´åº¦
        st.subheader("ğŸ“ˆ åŸå§‹é…æ¯”ä¸ç›®æ ‡é…æ¯”åç¦»åˆ†æ")
        comparison_cols = st.columns(len(['language', 'domain', 'category', 'token_bin']))
        for i, dim in enumerate(['language', 'domain', 'category', 'token_bin']):
            with comparison_cols[i]:
                orig_dist = get_cached_pie_chart_data(df, dim, cache_key, data_version)
                sampled_dist = get_cached_pie_chart_data(sampled_df, dim, get_dataframe_hash(sampled_df))
                # è®¡ç®—æœ€å¤§è¯¯å·®
                max_error = 0
                for cat in orig_dist.index:
                    orig = orig_dist.get(cat, 0)
                    sampled = sampled_dist.get(cat, 0)
                    error = abs(orig - sampled)
                    max_error = max(max_error, error)
                st.metric(f"{dim.capitalize()}", f"{max_error:.1%}", "æœ€å¤§åç¦»")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ•°æ®é›†è·¯å¾„å¹¶ç‚¹å‡»'åŠ è½½æ•°æ®é›†'")
    st.image("https://docs.streamlit.io/images/brand/streamlit-mark-color.png  ", width=300)
